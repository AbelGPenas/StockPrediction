{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_engineering_and_modelling.ipynb",
      "provenance": [],
      "mount_file_id": "1O3x2tDDfkvZkVlROxjDPImvuDrwDV4B1",
      "authorship_tag": "ABX9TyP/ogQ1LmvxWck9i2y88lJv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A2XJBhHE3ar"
      },
      "source": [
        "# 1- Environment set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQQTeOwRPoIv",
        "outputId": "87b8945b-a725-4fea-f604-ad492bcacefc"
      },
      "source": [
        "%cd /content/drive/MyDrive/Projects/ML_Labs/"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Projects/ML_Labs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3KH08viQYt6"
      },
      "source": [
        "Update the path to import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ01hiYtQcv7",
        "outputId": "4ebb8495-963a-43db-fce0-c2c27c9ab7af"
      },
      "source": [
        "!pip install yfinance\n",
        "!pip install -U scikit-learn==0.24.2 #update to version 0.24.2\n",
        "!pip install ta"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.59)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.6.3)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already up-to-date: scikit-learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.4.1)\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ta) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ta) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELrkt8-6QqaQ",
        "outputId": "c9599eaa-e09a-42a6-f1cc-fde618df70a1"
      },
      "source": [
        "!cp ./get_data_yf.py .\n",
        "!cp ./backtest.py ."
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: './get_data_yf.py' and './get_data_yf.py' are the same file\n",
            "cp: './backtest.py' and './backtest.py' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boVCXDuZTsog"
      },
      "source": [
        "Import modules and the get_data_yf file. **Not that the references to the backtest module and the Strategy class have been removed for compatibility.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBLdnZN-LHxq"
      },
      "source": [
        "import get_data_yf\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.feature_selection as fs\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import plotly.express as px\n",
        "import ta"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdGOJK8a8Zjc"
      },
      "source": [
        "We import the market pricing data for the last four years, so we can compute indicators like the moving average for an extensive dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtFQFwHoPlmA",
        "outputId": "a3452c75-351f-411e-fd77-03297ca642f4"
      },
      "source": [
        "entity = '^GSPC'    # S&P500's symbol\n",
        "#t1 = 5              # FROM t1 years ago (start date)\n",
        "m1 = 48              # FROM m1 months ago (start date)\n",
        "#t2 = 1             # TO t2 years ago (end date)\n",
        "start = (date.today()+relativedelta(months=-m1)).strftime(\"%Y-%m-%d\")    # 'yyyy-mm-dd' format\n",
        "end = time.strftime(\"%Y-%m-%d\")     # today\n",
        "#end = (date.today()+relativedelta(years=-t2)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "get_it = get_data_yf.Get_data(entity, start, end)\n",
        "candles = get_it.get_raw_prices()\n",
        "candles.to_csv('S&P500.csv') #Save the data as csv for later use with backtrader\n",
        "candles.reset_index(inplace=True)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "i-BzWpCs6mfe",
        "outputId": "01a42c8e-8ed6-4e7d-80cf-c267477c942c"
      },
      "source": [
        "candles.tail()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>2021-06-08</td>\n",
              "      <td>4233.810059</td>\n",
              "      <td>4236.740234</td>\n",
              "      <td>4208.410156</td>\n",
              "      <td>4227.259766</td>\n",
              "      <td>4227.259766</td>\n",
              "      <td>3943870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>2021-06-09</td>\n",
              "      <td>4232.990234</td>\n",
              "      <td>4237.089844</td>\n",
              "      <td>4218.740234</td>\n",
              "      <td>4219.549805</td>\n",
              "      <td>4219.549805</td>\n",
              "      <td>3902870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>2021-06-10</td>\n",
              "      <td>4228.560059</td>\n",
              "      <td>4249.740234</td>\n",
              "      <td>4220.339844</td>\n",
              "      <td>4239.180176</td>\n",
              "      <td>4239.180176</td>\n",
              "      <td>3502480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>2021-06-11</td>\n",
              "      <td>4242.899902</td>\n",
              "      <td>4248.379883</td>\n",
              "      <td>4232.250000</td>\n",
              "      <td>4247.439941</td>\n",
              "      <td>4247.439941</td>\n",
              "      <td>3204280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>2021-06-14</td>\n",
              "      <td>4248.310059</td>\n",
              "      <td>4255.589844</td>\n",
              "      <td>4234.069824</td>\n",
              "      <td>4255.149902</td>\n",
              "      <td>4255.149902</td>\n",
              "      <td>3612050000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date         Open         High  ...        Close    Adj Close      Volume\n",
              "1001 2021-06-08  4233.810059  4236.740234  ...  4227.259766  4227.259766  3943870000\n",
              "1002 2021-06-09  4232.990234  4237.089844  ...  4219.549805  4219.549805  3902870000\n",
              "1003 2021-06-10  4228.560059  4249.740234  ...  4239.180176  4239.180176  3502480000\n",
              "1004 2021-06-11  4242.899902  4248.379883  ...  4247.439941  4247.439941  3204280000\n",
              "1005 2021-06-14  4248.310059  4255.589844  ...  4255.149902  4255.149902  3612050000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF2oYTGKznf5"
      },
      "source": [
        "# 2- Developing and selecting technical indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awFfKC1NBmR5"
      },
      "source": [
        "We create a dataframe of indicators the engineer the features for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-4NhCEE47jl"
      },
      "source": [
        "indicators = pd.DataFrame()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I01lM4XYV1WZ"
      },
      "source": [
        "## 2.1- Target variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGY7g6J5Cj-4"
      },
      "source": [
        "The target of the model is to predict the direction of the future movement of the market. One straight way of codifying this behaviour is to assignate a value to whether the product price has increased (1), decreased (-1) or keep relatively constant (0) after 1 day and 1 week.\n",
        "\n",
        "**The threshold for wheter to understand the price significantly moving or staying neutral will be placed at a $\\pm$0.1% of the value of the initial day.** \n",
        "\n",
        "This is a relevant choice, that could be tunned in the feature (for example, demanding the price variation to be of higher magnitude to be understood as non-neutral, hence training the model to detect only the most significant trends).\n",
        "\n",
        "On top of the direction of the evolution we generate as well the magnitude of this variation by difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urV8Hn1kE4de"
      },
      "source": [
        "def price_evolution(prices:pd.Series)->int:\n",
        "  #Since we apply this function with a rolling window, the input is a pnadas Serie\n",
        "  initial_value = prices.iloc[0]\n",
        "  final_value = prices.iloc[-1]\n",
        "  #\n",
        "  if final_value > initial_value*1.001:\n",
        "    return 1\n",
        "  elif final_value < initial_value*0.999:\n",
        "    return -1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riUSTiWLPZ3n"
      },
      "source": [
        "def price_difference(prices:pd.Series)->int:\n",
        "  #Since we apply this function with a rolling window, the input is a pnadas Serie\n",
        "  initial_value = prices.iloc[0]\n",
        "  final_value = prices.iloc[-1]\n",
        "  return final_value-initial_value"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHk6PblcBedk"
      },
      "source": [
        "Applying the price evolution function with different windows we can generate some target variables:\n",
        "* Price direction after one day\n",
        "* Price direction after one week\n",
        "* Price difference after one day\n",
        "* Price difference after one week"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK89OOJNHm7a"
      },
      "source": [
        "#Creating forward windows is not straight-forward in pandas, and in the DOc is done by applying the function below:\n",
        "week_window = pd.api.indexers.FixedForwardWindowIndexer(window_size=6)\n",
        "day_window = pd.api.indexers.FixedForwardWindowIndexer(window_size=2)\n",
        "\n",
        "indicators['Date']=candles['Date']\n",
        "indicators['Week_evolution'] = candles['Close'].rolling(window=week_window).apply(price_evolution)\n",
        "indicators['Day_evolution'] = candles['Close'].rolling(window=day_window).apply(price_evolution)\n",
        "indicators['Week_difference'] = candles['Close'].rolling(window=week_window).apply(price_difference)\n",
        "indicators['Day_difference'] = candles['Close'].rolling(window=day_window).apply(price_difference)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPp68zF3FUCk"
      },
      "source": [
        "indicators['Close'] = candles['Close']"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao9bDeRrQNJ6"
      },
      "source": [
        "Now we have a pair of columns for the direction of the price variation as well as the magnitude by difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ns59xDR-LS2F",
        "outputId": "72069ce7-c5f2-43a9-f27b-ff6c228fe430"
      },
      "source": [
        "indicators.head(5)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Week_evolution</th>\n",
              "      <th>Day_evolution</th>\n",
              "      <th>Week_difference</th>\n",
              "      <th>Day_difference</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-06-15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.040039</td>\n",
              "      <td>0.689941</td>\n",
              "      <td>2432.459961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-06-16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.150146</td>\n",
              "      <td>20.310059</td>\n",
              "      <td>2433.149902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-14.389893</td>\n",
              "      <td>-16.429932</td>\n",
              "      <td>2453.459961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-06-20</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-17.650146</td>\n",
              "      <td>-1.419922</td>\n",
              "      <td>2437.030029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-06-21</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.079834</td>\n",
              "      <td>-1.110107</td>\n",
              "      <td>2435.610107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  Week_evolution  ...  Day_difference        Close\n",
              "0 2017-06-15             0.0  ...        0.689941  2432.459961\n",
              "1 2017-06-16             1.0  ...       20.310059  2433.149902\n",
              "2 2017-06-19            -1.0  ...      -16.429932  2453.459961\n",
              "3 2017-06-20            -1.0  ...       -1.419922  2437.030029\n",
              "4 2017-06-21             1.0  ...       -1.110107  2435.610107\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNYtKDoYB9nV"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "ifBvQJaHBJIk",
        "outputId": "48a0da33-4ccf-4294-b46a-31312bc013f6"
      },
      "source": [
        "sns.histplot(data=indicators, x='Week_evolution')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbdf573f350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fha5MatWA68"
      },
      "source": [
        "## 2.2- Signal indicators (features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4WYgRrqWGRM"
      },
      "source": [
        "### 2.2.1- Feature engineering process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiGr8O4wWLL7"
      },
      "source": [
        "In order to tune the different technical indicators proposed, a gridsearch will be deployed. Every of the studied technical tools have a widespread criteria to identify signals for the market direction going upwards/downwards (crossing of the moving averages, null value of the MACD cummulative distribution, RSI of 30/70 etc).\n",
        "\n",
        "This criteria is evaluated to generate a vector of -1/0/1 values predicting the weekly market direction. The accuracy (proportion of matching values) and precision (proportion of correct -1/1 signals) is presented for each combination of parameters. \n",
        "\n",
        "The best performing parameters will be chosen to generate the features to be later fed to the deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2YY_o6mzzIa"
      },
      "source": [
        "### 2.2.2- Moving averages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2jgbR2oB5uw"
      },
      "source": [
        "Moving averages is one of the most common and straight-forward indicators in technical analysis. We can compute them easily with no need of specialised libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQTQquxfz4D2"
      },
      "source": [
        "The moving averages can be used to signal when the market is likely to go up or down by studying when different SMA cross or when the SMA and the price cross (the least smoothed signal crossing upwards a more smoothed signal indicates a potential upwards change of tendency in the price, and viceversa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHm4DEjLahxd"
      },
      "source": [
        "A custom function is created to generate the signal depending on the crossing events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEJYEJYaFM2x"
      },
      "source": [
        "def signal_crossing(signal_1:pd.Series, signal_2:pd.Series)->pd.Series:\n",
        "  # This function compares two time series and returns a 0 for every time point without crossing events,\n",
        "  # 1 when signal_1 crosses signal_2 upwards and -1 when signal_1 crosses downwards signal_2\n",
        "  new_series = pd.Series(dtype='float64')\n",
        "  for idx in range(1, len(signal_1)):\n",
        "    if signal_1.iloc[idx] > signal_2.iloc[idx] and signal_1.iloc[idx-1] < signal_2.iloc[idx-1]:\n",
        "      new_series.loc[idx]=1\n",
        "    elif signal_1.iloc[idx] < signal_2.iloc[idx] and signal_1.iloc[idx-1] > signal_2.iloc[idx-1]:\n",
        "      new_series.loc[idx]=-1\n",
        "    else:\n",
        "      new_series.loc[idx]=0\n",
        "  return new_series"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCnlnS2xaoyx"
      },
      "source": [
        "A new dataframe is created to populate it while optimising the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebZtSZf-TqAG"
      },
      "source": [
        "ma_df = indicators[['Date', 'Close', 'Week_evolution']]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9EiilPvTj_q",
        "outputId": "84f68a80-1b40-4568-ce23-08faf5e0bd01"
      },
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "MA_grid = {  \n",
        "    'window_slow': [4, 6, 8, 10, 12, 14, 18, 20, 22],\n",
        "    'window_fast': [1, 2, 4, 6, 8, 10, 12, 14],\n",
        "}\n",
        "# Generate all combinations of parameters\n",
        "all_MA_params = [dict(zip(MA_grid.keys(), v)) for v in itertools.product(*MA_grid.values())]\n",
        "#Leave only the param sets where the fast MA period is lower than the slow MA period\n",
        "all_MA_params = [dict for dict in all_MA_params if (dict['window_fast'] < dict['window_slow'])]\n",
        "# Loop over all the sets of parameters\n",
        "for param_set in all_MA_params:\n",
        "  ma_df['ma_fast'] = candles['Close'].transform(lambda x: x.rolling(window = param_set['window_fast']).mean())\n",
        "  ma_df['ma_slow'] = candles['Close'].transform(lambda x: x.rolling(window = param_set['window_slow']).mean())\n",
        "  ma_df['MA_signal'] = signal_crossing(ma_df['ma_fast'], ma_df['ma_slow'])\n",
        "  ma_df.fillna(0, inplace=True)\n",
        "  # Extract only the rows where the predicted direction is non-neutral\n",
        "  ma_signals = ma_df.loc[ma_df['MA_signal'] != 0]\n",
        "  # Days for which the signal can be evaluated (discarding the first days for \n",
        "  # which the ma is not available and the last days for which the direction for the following week is unknown)\n",
        "  available_days = len(ma_df)-param_set['window_slow']-5\n",
        "  # Evaluate the accuracy and the precision\n",
        "  accuracy = (ma_df['MA_signal'] == ma_df['Week_evolution']).sum() / available_days *100\n",
        "  precision = (ma_signals['MA_signal'] == ma_signals['Week_evolution']).sum() / available_days *100\n",
        "  # Capture the parameters since f-string gives an error when unpacking the dict\n",
        "  fast_period = param_set['window_fast']\n",
        "  slow_period = param_set['window_slow']\n",
        "  print(f'MA periods of {fast_period}-{slow_period}: Accuracy = {accuracy:.2f}% Precision = {precision:.2f}% #Signals {len(ma_signals)}')\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MA periods of 1-4: Accuracy = 17.05% Precision = 13.44% #Signals 289\n",
            "MA periods of 2-4: Accuracy = 15.45% Precision = 11.94% #Signals 250\n",
            "MA periods of 1-6: Accuracy = 14.07% Precision = 9.95% #Signals 215\n",
            "MA periods of 2-6: Accuracy = 12.96% Precision = 8.54% #Signals 165\n",
            "MA periods of 4-6: Accuracy = 12.76% Precision = 8.54% #Signals 170\n",
            "MA periods of 1-8: Accuracy = 12.39% Precision = 8.46% #Signals 174\n",
            "MA periods of 2-8: Accuracy = 11.58% Precision = 7.25% #Signals 148\n",
            "MA periods of 4-8: Accuracy = 10.37% Precision = 6.24% #Signals 125\n",
            "MA periods of 6-8: Accuracy = 11.68% Precision = 7.45% #Signals 144\n",
            "MA periods of 1-10: Accuracy = 11.71% Precision = 7.57% #Signals 146\n",
            "MA periods of 2-10: Accuracy = 10.70% Precision = 6.05% #Signals 111\n",
            "MA periods of 4-10: Accuracy = 9.49% Precision = 4.94% #Signals 95\n",
            "MA periods of 6-10: Accuracy = 10.09% Precision = 5.55% #Signals 97\n",
            "MA periods of 8-10: Accuracy = 11.81% Precision = 7.16% #Signals 116\n",
            "MA periods of 1-12: Accuracy = 11.53% Precision = 7.08% #Signals 131\n",
            "MA periods of 2-12: Accuracy = 10.01% Precision = 5.46% #Signals 91\n",
            "MA periods of 4-12: Accuracy = 9.30% Precision = 4.55% #Signals 75\n",
            "MA periods of 6-12: Accuracy = 9.30% Precision = 4.75% #Signals 73\n",
            "MA periods of 8-12: Accuracy = 9.10% Precision = 4.55% #Signals 77\n",
            "MA periods of 10-12: Accuracy = 9.81% Precision = 5.46% #Signals 102\n",
            "MA periods of 1-14: Accuracy = 10.94% Precision = 6.28% #Signals 115\n",
            "MA periods of 2-14: Accuracy = 9.42% Precision = 4.96% #Signals 85\n",
            "MA periods of 4-14: Accuracy = 8.92% Precision = 4.15% #Signals 65\n",
            "MA periods of 6-14: Accuracy = 8.00% Precision = 3.55% #Signals 61\n",
            "MA periods of 8-14: Accuracy = 8.11% Precision = 3.65% #Signals 67\n",
            "MA periods of 10-14: Accuracy = 8.00% Precision = 3.55% #Signals 69\n",
            "MA periods of 12-14: Accuracy = 8.71% Precision = 4.05% #Signals 88\n",
            "MA periods of 1-18: Accuracy = 10.17% Precision = 5.39% #Signals 89\n",
            "MA periods of 2-18: Accuracy = 9.56% Precision = 4.88% #Signals 75\n",
            "MA periods of 4-18: Accuracy = 8.24% Precision = 3.66% #Signals 57\n",
            "MA periods of 6-18: Accuracy = 7.63% Precision = 2.95% #Signals 55\n",
            "MA periods of 8-18: Accuracy = 7.83% Precision = 3.05% #Signals 49\n",
            "MA periods of 10-18: Accuracy = 7.32% Precision = 2.54% #Signals 45\n",
            "MA periods of 12-18: Accuracy = 6.51% Precision = 2.03% #Signals 47\n",
            "MA periods of 14-18: Accuracy = 6.92% Precision = 2.24% #Signals 49\n",
            "MA periods of 1-20: Accuracy = 9.48% Precision = 4.79% #Signals 78\n",
            "MA periods of 2-20: Accuracy = 8.46% Precision = 3.77% #Signals 58\n",
            "MA periods of 4-20: Accuracy = 7.75% Precision = 3.06% #Signals 52\n",
            "MA periods of 6-20: Accuracy = 7.14% Precision = 2.45% #Signals 43\n",
            "MA periods of 8-20: Accuracy = 7.44% Precision = 2.65% #Signals 43\n",
            "MA periods of 10-20: Accuracy = 7.14% Precision = 2.34% #Signals 41\n",
            "MA periods of 12-20: Accuracy = 6.52% Precision = 1.73% #Signals 37\n",
            "MA periods of 14-20: Accuracy = 6.83% Precision = 2.04% #Signals 41\n",
            "MA periods of 1-22: Accuracy = 9.60% Precision = 4.80% #Signals 82\n",
            "MA periods of 2-22: Accuracy = 8.68% Precision = 3.88% #Signals 58\n",
            "MA periods of 4-22: Accuracy = 7.46% Precision = 2.76% #Signals 46\n",
            "MA periods of 6-22: Accuracy = 7.25% Precision = 2.45% #Signals 40\n",
            "MA periods of 8-22: Accuracy = 7.05% Precision = 2.25% #Signals 38\n",
            "MA periods of 10-22: Accuracy = 6.23% Precision = 1.53% #Signals 36\n",
            "MA periods of 12-22: Accuracy = 6.64% Precision = 1.84% #Signals 37\n",
            "MA periods of 14-22: Accuracy = 6.44% Precision = 1.74% #Signals 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYWbno8sd0Jv"
      },
      "source": [
        "The performance of the different sets of parameters increases as the period decreases. It can be hypothesized that wide periods miss many buy and sell signals, hence scoring low in accuracy and precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPgUbbYBPHeh"
      },
      "source": [
        "ma_df['ma_fast'] = candles['Close'].transform(lambda x: x.rolling(window = 1).mean())\n",
        "ma_df['ma_slow'] = candles['Close'].transform(lambda x: x.rolling(window = 4).mean())\n",
        "indicators['ma_optimum'] = signal_crossing(ma_df['ma_fast'], ma_df['ma_slow'])"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX9G7_rCaYEM"
      },
      "source": [
        "### 2.2.3-MACD difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCDCvM_Ii_gx"
      },
      "source": [
        "The MACD is a similar indicator tho the MA crossing, with the difference of using exponential moving averages instead of simple ones. \n",
        "\n",
        "The same optimisation process is followed for the MACD indicator, which signals to buy or sell when the two EMA cross."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxnOk-ackVlC"
      },
      "source": [
        "def MACD_evolution(prices:pd.Series)->int:\n",
        "  #Since we apply this function with a rolling window, the input is a pnadas Serie\n",
        "  initial_value = prices.iloc[0]\n",
        "  final_value = prices.iloc[-1]\n",
        "  #\n",
        "  if ((final_value > 0) and (initial_value < 0)):\n",
        "    return 1\n",
        "  elif ((final_value < 0) and (initial_value > 0)):\n",
        "    return -1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sobGoeHis3ze",
        "outputId": "912171f1-73c3-409d-f449-1a207a2c8a52"
      },
      "source": [
        "MACD_grid = {  \n",
        "    'window_slow': [4, 6, 8, 10, 12, 14, 18, 22],\n",
        "    'window_fast': [1, 2, 4, 6, 8, 10],\n",
        "    'window_sign': [3, 6, 9],\n",
        "    'fillna': [False]\n",
        "}\n",
        "# Generate all combinations of parameters\n",
        "all_MACD_params = [dict(zip(MACD_grid.keys(), v)) for v in itertools.product(*MACD_grid.values())]\n",
        "#Leave only the param sets where the fast MA period is lower than the slow MA period\n",
        "all_MACD_params = [dict for dict in all_MACD_params if (dict['window_fast'] < dict['window_slow'])]\n",
        "# Iterate over all the sets of parameters\n",
        "for param_set in all_MACD_params:\n",
        "  indicators['MACD_diff']=ta.trend.MACD(candles['Close'], **param_set).macd_diff()\n",
        "  indicators['MACD_signal'] = indicators['MACD_diff'].rolling(window=2).apply(MACD_evolution)\n",
        "  indicators.fillna(0, inplace=True)\n",
        "  MACD_signals = indicators.loc[indicators['MACD_signal'] != 0]\n",
        "  available_days = len(indicators)-param_set['window_slow']-5\n",
        "  accuracy = (indicators['MACD_signal'] == indicators['Week_evolution']).sum() / available_days *100\n",
        "  precision = (MACD_signals['MACD_signal'] == MACD_signals['Week_evolution']).sum() / available_days *100\n",
        "  fast_period = param_set['window_fast']\n",
        "  slow_period = param_set['window_slow']\n",
        "  signal_period = param_set['window_sign']\n",
        "  print(f'MACD period of {fast_period}-{slow_period}-{signal_period}: Accuracy = {accuracy:.2f}% Precision = {precision:.2f}% #Signals {len(MACD_signals)}')\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MACD period of 1-4-3: Accuracy = 22.97% Precision = 20.46% #Signals 446\n",
            "MACD period of 1-4-6: Accuracy = 20.66% Precision = 17.65% #Signals 379\n",
            "MACD period of 1-4-9: Accuracy = 19.86% Precision = 16.75% #Signals 361\n",
            "MACD period of 2-4-3: Accuracy = 18.56% Precision = 15.75% #Signals 349\n",
            "MACD period of 2-4-6: Accuracy = 16.65% Precision = 13.24% #Signals 291\n",
            "MACD period of 2-4-9: Accuracy = 15.95% Precision = 12.34% #Signals 271\n",
            "MACD period of 1-6-3: Accuracy = 21.81% Precision = 19.10% #Signals 412\n",
            "MACD period of 1-6-6: Accuracy = 19.40% Precision = 16.18% #Signals 347\n",
            "MACD period of 1-6-9: Accuracy = 18.19% Precision = 14.67% #Signals 316\n",
            "MACD period of 2-6-3: Accuracy = 17.19% Precision = 14.17% #Signals 320\n",
            "MACD period of 2-6-6: Accuracy = 15.68% Precision = 11.96% #Signals 257\n",
            "MACD period of 2-6-9: Accuracy = 15.18% Precision = 11.36% #Signals 236\n",
            "MACD period of 4-6-3: Accuracy = 15.38% Precision = 11.66% #Signals 248\n",
            "MACD period of 4-6-6: Accuracy = 12.96% Precision = 8.94% #Signals 189\n",
            "MACD period of 4-6-9: Accuracy = 11.76% Precision = 8.04% #Signals 174\n",
            "MACD period of 1-8-3: Accuracy = 20.85% Precision = 18.13% #Signals 390\n",
            "MACD period of 1-8-6: Accuracy = 18.43% Precision = 14.90% #Signals 324\n",
            "MACD period of 1-8-9: Accuracy = 17.82% Precision = 14.30% #Signals 296\n",
            "MACD period of 2-8-3: Accuracy = 17.22% Precision = 13.70% #Signals 298\n",
            "MACD period of 2-8-6: Accuracy = 15.71% Precision = 11.78% #Signals 242\n",
            "MACD period of 2-8-9: Accuracy = 12.89% Precision = 9.16% #Signals 200\n",
            "MACD period of 4-8-3: Accuracy = 15.01% Precision = 11.18% #Signals 234\n",
            "MACD period of 4-8-6: Accuracy = 11.88% Precision = 8.16% #Signals 177\n",
            "MACD period of 4-8-9: Accuracy = 11.78% Precision = 7.85% #Signals 165\n",
            "MACD period of 6-8-3: Accuracy = 13.09% Precision = 9.16% #Signals 196\n",
            "MACD period of 6-8-6: Accuracy = 11.58% Precision = 7.75% #Signals 163\n",
            "MACD period of 6-8-9: Accuracy = 10.88% Precision = 6.85% #Signals 141\n",
            "MACD period of 1-10-3: Accuracy = 20.69% Precision = 17.76% #Signals 381\n",
            "MACD period of 1-10-6: Accuracy = 17.96% Precision = 14.53% #Signals 309\n",
            "MACD period of 1-10-9: Accuracy = 16.35% Precision = 13.02% #Signals 282\n",
            "MACD period of 2-10-3: Accuracy = 16.95% Precision = 13.52% #Signals 299\n",
            "MACD period of 2-10-6: Accuracy = 14.73% Precision = 10.90% #Signals 225\n",
            "MACD period of 2-10-9: Accuracy = 11.91% Precision = 8.17% #Signals 182\n",
            "MACD period of 4-10-3: Accuracy = 14.83% Precision = 11.00% #Signals 229\n",
            "MACD period of 4-10-6: Accuracy = 11.60% Precision = 7.77% #Signals 165\n",
            "MACD period of 4-10-9: Accuracy = 11.50% Precision = 7.37% #Signals 152\n",
            "MACD period of 6-10-3: Accuracy = 12.21% Precision = 8.48% #Signals 185\n",
            "MACD period of 6-10-6: Accuracy = 11.10% Precision = 7.06% #Signals 145\n",
            "MACD period of 6-10-9: Accuracy = 10.39% Precision = 6.36% #Signals 128\n",
            "MACD period of 8-10-3: Accuracy = 12.11% Precision = 8.17% #Signals 171\n",
            "MACD period of 8-10-6: Accuracy = 10.90% Precision = 6.76% #Signals 135\n",
            "MACD period of 8-10-9: Accuracy = 9.28% Precision = 5.45% #Signals 112\n",
            "MACD period of 1-12-3: Accuracy = 20.22% Precision = 17.29% #Signals 372\n",
            "MACD period of 1-12-6: Accuracy = 18.10% Precision = 14.76% #Signals 308\n",
            "MACD period of 1-12-9: Accuracy = 16.08% Precision = 12.64% #Signals 272\n",
            "MACD period of 2-12-3: Accuracy = 16.89% Precision = 13.45% #Signals 292\n",
            "MACD period of 2-12-6: Accuracy = 13.14% Precision = 9.50% #Signals 206\n",
            "MACD period of 2-12-9: Accuracy = 12.23% Precision = 8.49% #Signals 188\n",
            "MACD period of 4-12-3: Accuracy = 13.85% Precision = 10.01% #Signals 212\n",
            "MACD period of 4-12-6: Accuracy = 11.93% Precision = 7.99% #Signals 168\n",
            "MACD period of 4-12-9: Accuracy = 11.12% Precision = 6.98% #Signals 146\n",
            "MACD period of 6-12-3: Accuracy = 12.03% Precision = 8.39% #Signals 184\n",
            "MACD period of 6-12-6: Accuracy = 10.92% Precision = 6.88% #Signals 140\n",
            "MACD period of 6-12-9: Accuracy = 9.71% Precision = 5.86% #Signals 118\n",
            "MACD period of 8-12-3: Accuracy = 12.03% Precision = 8.19% #Signals 174\n",
            "MACD period of 8-12-6: Accuracy = 9.91% Precision = 6.07% #Signals 124\n",
            "MACD period of 8-12-9: Accuracy = 9.71% Precision = 5.56% #Signals 110\n",
            "MACD period of 10-12-3: Accuracy = 11.83% Precision = 7.79% #Signals 164\n",
            "MACD period of 10-12-6: Accuracy = 9.91% Precision = 5.97% #Signals 118\n",
            "MACD period of 10-12-9: Accuracy = 9.10% Precision = 4.85% #Signals 96\n",
            "MACD period of 1-14-3: Accuracy = 20.06% Precision = 17.12% #Signals 366\n",
            "MACD period of 1-14-6: Accuracy = 17.12% Precision = 13.88% #Signals 294\n",
            "MACD period of 1-14-9: Accuracy = 15.81% Precision = 12.26% #Signals 260\n",
            "MACD period of 2-14-3: Accuracy = 16.41% Precision = 12.97% #Signals 286\n",
            "MACD period of 2-14-6: Accuracy = 13.27% Precision = 9.63% #Signals 210\n",
            "MACD period of 2-14-9: Accuracy = 12.77% Precision = 8.81% #Signals 188\n",
            "MACD period of 4-14-3: Accuracy = 12.87% Precision = 9.22% #Signals 199\n",
            "MACD period of 4-14-6: Accuracy = 11.75% Precision = 7.70% #Signals 158\n",
            "MACD period of 4-14-9: Accuracy = 11.14% Precision = 6.99% #Signals 138\n",
            "MACD period of 6-14-3: Accuracy = 12.06% Precision = 8.31% #Signals 175\n",
            "MACD period of 6-14-6: Accuracy = 10.84% Precision = 6.79% #Signals 136\n",
            "MACD period of 6-14-9: Accuracy = 10.03% Precision = 5.98% #Signals 118\n",
            "MACD period of 8-14-3: Accuracy = 12.77% Precision = 8.61% #Signals 173\n",
            "MACD period of 8-14-6: Accuracy = 10.03% Precision = 6.18% #Signals 124\n",
            "MACD period of 8-14-9: Accuracy = 9.42% Precision = 5.07% #Signals 100\n",
            "MACD period of 10-14-3: Accuracy = 11.75% Precision = 7.60% #Signals 155\n",
            "MACD period of 10-14-6: Accuracy = 10.03% Precision = 5.78% #Signals 110\n",
            "MACD period of 10-14-9: Accuracy = 9.22% Precision = 4.86% #Signals 96\n",
            "MACD period of 1-18-3: Accuracy = 19.74% Precision = 16.99% #Signals 366\n",
            "MACD period of 1-18-6: Accuracy = 16.68% Precision = 13.33% #Signals 286\n",
            "MACD period of 1-18-9: Accuracy = 15.06% Precision = 11.29% #Signals 238\n",
            "MACD period of 2-18-3: Accuracy = 15.87% Precision = 12.21% #Signals 270\n",
            "MACD period of 2-18-6: Accuracy = 12.61% Precision = 8.95% #Signals 198\n",
            "MACD period of 2-18-9: Accuracy = 12.00% Precision = 8.14% #Signals 174\n",
            "MACD period of 4-18-3: Accuracy = 12.41% Precision = 8.55% #Signals 188\n",
            "MACD period of 4-18-6: Accuracy = 11.70% Precision = 7.53% #Signals 152\n",
            "MACD period of 4-18-9: Accuracy = 11.09% Precision = 6.92% #Signals 132\n",
            "MACD period of 6-18-3: Accuracy = 13.02% Precision = 8.85% #Signals 182\n",
            "MACD period of 6-18-6: Accuracy = 10.68% Precision = 6.61% #Signals 130\n",
            "MACD period of 6-18-9: Accuracy = 10.07% Precision = 5.80% #Signals 108\n",
            "MACD period of 8-18-3: Accuracy = 11.50% Precision = 7.43% #Signals 154\n",
            "MACD period of 8-18-6: Accuracy = 10.17% Precision = 5.90% #Signals 110\n",
            "MACD period of 8-18-9: Accuracy = 9.56% Precision = 5.19% #Signals 100\n",
            "MACD period of 10-18-3: Accuracy = 11.19% Precision = 7.02% #Signals 148\n",
            "MACD period of 10-18-6: Accuracy = 9.87% Precision = 5.49% #Signals 104\n",
            "MACD period of 10-18-9: Accuracy = 9.05% Precision = 4.58% #Signals 88\n",
            "MACD period of 1-22-3: Accuracy = 19.41% Precision = 16.65% #Signals 362\n",
            "MACD period of 1-22-6: Accuracy = 16.45% Precision = 12.87% #Signals 273\n",
            "MACD period of 1-22-9: Accuracy = 14.91% Precision = 10.93% #Signals 229\n",
            "MACD period of 2-22-3: Accuracy = 15.73% Precision = 12.16% #Signals 268\n",
            "MACD period of 2-22-6: Accuracy = 12.97% Precision = 8.89% #Signals 197\n",
            "MACD period of 2-22-9: Accuracy = 11.95% Precision = 7.87% #Signals 165\n",
            "MACD period of 4-22-3: Accuracy = 12.26% Precision = 8.48% #Signals 190\n",
            "MACD period of 4-22-6: Accuracy = 11.64% Precision = 7.56% #Signals 154\n",
            "MACD period of 4-22-9: Accuracy = 10.42% Precision = 6.23% #Signals 119\n",
            "MACD period of 6-22-3: Accuracy = 12.26% Precision = 8.17% #Signals 170\n",
            "MACD period of 6-22-6: Accuracy = 10.62% Precision = 6.44% #Signals 126\n",
            "MACD period of 6-22-9: Accuracy = 9.91% Precision = 5.52% #Signals 106\n",
            "MACD period of 8-22-3: Accuracy = 11.44% Precision = 7.25% #Signals 150\n",
            "MACD period of 8-22-6: Accuracy = 10.11% Precision = 5.72% #Signals 106\n",
            "MACD period of 8-22-9: Accuracy = 9.60% Precision = 5.11% #Signals 96\n",
            "MACD period of 10-22-3: Accuracy = 11.03% Precision = 6.74% #Signals 134\n",
            "MACD period of 10-22-6: Accuracy = 9.70% Precision = 5.21% #Signals 102\n",
            "MACD period of 10-22-9: Accuracy = 9.09% Precision = 4.60% #Signals 88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbpaS5gikdRW"
      },
      "source": [
        "Similarly to the simple moving averages case, the narrower the periods the more accurate and precise is the signal produced. \n",
        "\n",
        "**Very interestingly when having less signals with wider periods, the precision of these few signals is not any better of the larger set of signals obtained for narrower MA periods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aljMUwJfTJcH"
      },
      "source": [
        "indicators['MACD_optimum']=ta.trend.MACD(candles['Close'], window_fast=1, window_slow=4, window_sign=3, fillna=True).macd_diff()"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7vDnknfGodD"
      },
      "source": [
        "### 2.2.4-Relative Strength Index (RSI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHwtRpc0lb6X"
      },
      "source": [
        "The relative strength index (RSI) in short indicates when the market can be considered to be overbought or oversold. This is identified by the commonly used threshold values of 30 and 70. Without playing around with these two threshold values, we can optimise the RSI window length following a similar method as the one performed before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N42Awrh916ff",
        "outputId": "d4ced3e3-28b0-4b5e-afb4-9bf576e8e831"
      },
      "source": [
        "for window_len in range(1, 22, 1):\n",
        "  indicators['RSI_value']=ta.momentum.RSIIndicator(close=candles['Close'], window=window_len, fillna=False).rsi()\n",
        "  indicators['RSI_signal'] = [1 if rsi>70 else -1 if rsi<30 else 0 for rsi in indicators['RSI_value']]\n",
        "  indicators.fillna(0, inplace=True)\n",
        "  RSI_signals = indicators.loc[indicators['RSI_signal'] != 0]\n",
        "  available_days = len(indicators)-window_len-5\n",
        "  accuracy = (indicators['RSI_signal'] == indicators['Week_evolution']).sum() / available_days *100\n",
        "  precision = (RSI_signals['RSI_signal'] == RSI_signals['Week_evolution']).sum() / available_days *100\n",
        "\n",
        "  print(f'RSI period of {window_len}: Accuracy = {accuracy:.2f}% Precision = {precision:.2f}% #Signals {len(RSI_signals)}')\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RSI period of 1: Accuracy = 49.30% Precision = 49.30% #Signals 1006\n",
            "RSI period of 2: Accuracy = 38.54% Precision = 37.24% #Signals 704\n",
            "RSI period of 3: Accuracy = 32.06% Precision = 30.16% #Signals 558\n",
            "RSI period of 4: Accuracy = 27.98% Precision = 25.98% #Signals 488\n",
            "RSI period of 5: Accuracy = 24.00% Precision = 21.89% #Signals 424\n",
            "RSI period of 6: Accuracy = 21.11% Precision = 18.79% #Signals 377\n",
            "RSI period of 7: Accuracy = 18.41% Precision = 15.90% #Signals 319\n",
            "RSI period of 8: Accuracy = 16.21% Precision = 13.49% #Signals 269\n",
            "RSI period of 9: Accuracy = 15.32% Precision = 12.30% #Signals 240\n",
            "RSI period of 10: Accuracy = 15.04% Precision = 11.60% #Signals 218\n",
            "RSI period of 11: Accuracy = 13.94% Precision = 10.40% #Signals 193\n",
            "RSI period of 12: Accuracy = 13.14% Precision = 9.20% #Signals 172\n",
            "RSI period of 13: Accuracy = 12.25% Precision = 8.10% #Signals 149\n",
            "RSI period of 14: Accuracy = 10.94% Precision = 6.79% #Signals 130\n",
            "RSI period of 15: Accuracy = 10.45% Precision = 6.19% #Signals 117\n",
            "RSI period of 16: Accuracy = 10.25% Precision = 5.99% #Signals 110\n",
            "RSI period of 17: Accuracy = 9.65% Precision = 5.39% #Signals 98\n",
            "RSI period of 18: Accuracy = 9.05% Precision = 4.78% #Signals 89\n",
            "RSI period of 19: Accuracy = 8.15% Precision = 3.87% #Signals 77\n",
            "RSI period of 20: Accuracy = 7.75% Precision = 3.47% #Signals 70\n",
            "RSI period of 21: Accuracy = 7.65% Precision = 3.27% #Signals 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d3XioCwad3F"
      },
      "source": [
        "indicators['RSI_optimum']=ta.momentum.RSIIndicator(close=candles['Close'], window=2, fillna=False).rsi()"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOD3gDItEZiz"
      },
      "source": [
        "### 2.2.5- Prophet signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiO5hi41EeOc"
      },
      "source": [
        "The time series forecasting algorithm Prophet is another useful tool to get a prediction in the market direction for any given day. Fitting the model to all the previous days we can get a forecast with any desired period length, as well as the confidence bands for the forecast. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ByGEvAE8iX"
      },
      "source": [
        "from fbprophet import Prophet\n",
        "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
        "import itertools"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VsycOvot55f"
      },
      "source": [
        "The prophet model use as input a pandas datafame with column name 'ds' for the datestamp and 'y' for the timeseries values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY4dEDlzuATl"
      },
      "source": [
        "# Create a new dataframe with the relevant columns and modify the column names\n",
        "data_prophet = indicators[['Date', 'Close']]\n",
        "data_prophet.columns = ['ds', 'y']"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4UidGmLsuZvL",
        "outputId": "6dc3d454-c1de-48a6-d6df-064d5d13785d"
      },
      "source": [
        "data_prophet.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ds</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-06-14</td>\n",
              "      <td>2437.919922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-06-15</td>\n",
              "      <td>2432.459961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-06-16</td>\n",
              "      <td>2433.149902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2453.459961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-06-20</td>\n",
              "      <td>2437.030029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          ds            y\n",
              "0 2017-06-14  2437.919922\n",
              "1 2017-06-15  2432.459961\n",
              "2 2017-06-16  2433.149902\n",
              "3 2017-06-19  2453.459961\n",
              "4 2017-06-20  2437.030029"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l88IH_UJvocl"
      },
      "source": [
        "#### Prophet hyperparameter tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vBNJa5fnO9s"
      },
      "source": [
        "According to the Prophet docs, the two parameters recommended to be tuned are the 'changepoint_prior_scale' and the 'seasonality_prior_scale'. For this we use the built-in functions cross_validation and performance_metrics from the fbprophet library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiAB_xGCvuZK"
      },
      "source": [
        "param_grid = {  \n",
        "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 1],\n",
        "    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10],\n",
        "}\n",
        "# Generate all combinations of parameters\n",
        "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
        "prophet_rmses = []  # Store the RMSEs for each params here"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQMChj4hwpPL"
      },
      "source": [
        "# Use cross validation to evaluate all parameters\n",
        "for params in all_params:\n",
        "    m = Prophet(**params).fit(data_prophet)  # Fit model with given params\n",
        "    df_cv = cross_validation(m, horizon='30 days', period='365 days', parallel=\"threads\")\n",
        "    df_p = performance_metrics(df_cv, rolling_window=1)\n",
        "    prophet_rmses.append(df_p['rmse'].values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kED2IBwF_5Qr",
        "outputId": "463193bd-e7a1-4948-e8b5-1f821a87efad"
      },
      "source": [
        "# Find the best parameters\n",
        "tuning_results = pd.DataFrame(all_params)\n",
        "tuning_results['rmse'] = prophet_rmses\n",
        "print(tuning_results)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    changepoint_prior_scale  seasonality_prior_scale        rmse\n",
            "0                     0.001                     0.01  381.575242\n",
            "1                     0.001                     0.10  380.300553\n",
            "2                     0.001                     1.00  382.442460\n",
            "3                     0.001                    10.00  382.052637\n",
            "4                     0.010                     0.01   86.264220\n",
            "5                     0.010                     0.10   89.801803\n",
            "6                     0.010                     1.00   82.359267\n",
            "7                     0.010                    10.00   84.790880\n",
            "8                     0.100                     0.01  187.003189\n",
            "9                     0.100                     0.10  171.335086\n",
            "10                    0.100                     1.00  174.925633\n",
            "11                    0.100                    10.00  178.105420\n",
            "12                    1.000                     0.01  120.763275\n",
            "13                    1.000                     0.10   76.794246\n",
            "14                    1.000                     1.00   79.240100\n",
            "15                    1.000                    10.00   76.432742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2avxAy-0nCj9"
      },
      "source": [
        "A Changepoint_prior_scale value of 1.0 and a seasonality_prior_scale value of 10.0 will be kept to be used for the further optimisation of the fitting window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mhf9OGh_eI"
      },
      "source": [
        "#### Optimize the training window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK0Tsp5zlMMR"
      },
      "source": [
        "We define a function to retrieve the model parameters and ease the warm start of the model fitting when only adding the new day values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqnJdMQHlKj5"
      },
      "source": [
        "def stan_init(m):\n",
        "    \"\"\"Retrieve parameters from a trained model.\n",
        "    \n",
        "    Retrieve parameters from a trained model in the format\n",
        "    used to initialize a new Stan model.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    m: A trained model of the Prophet class.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    A Dictionary containing retrieved parameters of m.\n",
        "    \n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for pname in ['k', 'm', 'sigma_obs']:\n",
        "        res[pname] = m.params[pname][0][0]\n",
        "    for pname in ['delta', 'beta']:\n",
        "        res[pname] = m.params[pname][0]\n",
        "    return res"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBHve5zpOi6"
      },
      "source": [
        "We optimise the window of past datapoints used to train the model for any give present day by predicting the next week price for every day using the 3 considered windows (60, 120 and 240 weekdays) and converting it into price variation (next week value - current value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU4ZFuaWiYmg"
      },
      "source": [
        "prophet_direction = data_prophet\n",
        "passing_issues=0\n",
        "modelled_params = {}\n",
        "for window_len in [60, 120, 240]:\n",
        "  #Create a new column for the new window length\n",
        "  prophet_direction.loc[:, str(window_len)] = np.nan\n",
        "\n",
        "  for day in range(window_len, len(data_prophet)-5, 1):\n",
        "    print(f'Modelling day {day} with window {window_len}')\n",
        "    prophet_train = data_prophet.iloc[:day, :]\n",
        "    # The model can only be fitted once after instantiating it, so we do it inside the loop\n",
        "    prophet_model = Prophet(changepoint_prior_scale=1, seasonality_prior_scale=0.01)\n",
        "    if not modelled_params: # We are fitting for the first time\n",
        "      prophet_model.fit(prophet_train)\n",
        "    else:\n",
        "      new_data = data_prophet.iloc[window_len, :]\n",
        "      # We use the parameters from the previous model to warm start the new one\n",
        "      try :\n",
        "        prophet_model.fit(new_data, init=modelled_params)\n",
        "      except:\n",
        "        prophet_model = Prophet(changepoint_prior_scale=1, seasonality_prior_scale=0.01)\n",
        "        prophet_model.fit(prophet_train)\n",
        "    # we store the parameters for the next model\n",
        "    if prophet_model.params:\n",
        "      modelled_params = stan_init(prophet_model)\n",
        "    else:\n",
        "      modelled_params = {}\n",
        "    future = prophet_model.make_future_dataframe(periods=8, freq = 'd')\n",
        "    #We remove the weekends\n",
        "    future['day'] = future['ds'].dt.weekday\n",
        "    future = future.loc[future['day']<=4, :]\n",
        "    #We perfomr the one week forecast\n",
        "    forecast = prophet_model.predict(future)\n",
        "    variation = forecast.loc[forecast.index[-1], 'yhat'] - data_prophet.loc[day, 'y']\n",
        "    prophet_direction.loc[day, [str(window_len)]] = variation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d8HrbukrlNU"
      },
      "source": [
        "These two lines of codes are only here to be able to export/import the results of the previous timeconsuming optimisation process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCskXtnXvA9"
      },
      "source": [
        "prophet_direction.to_csv('prophet_optimisation_wsignals.csv')"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gujU8_5Mre7w"
      },
      "source": [
        "prophet_direction = pd.read_csv('prophet_optimisation_wsignals.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2w5Mkf0ps3a"
      },
      "source": [
        "The price variation vectors are converted into price direction vectors in order to calculate the accuracies. Again this can be done applying a lambda with a custom function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JqkTo5yUhRC"
      },
      "source": [
        "def price_direction(element)->int:\n",
        "  stock_value = 4200\n",
        "  if element > stock_value*0.001:\n",
        "    return 1\n",
        "  elif element < -stock_value*0.001:\n",
        "    return -1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz2iLKvFVzvc"
      },
      "source": [
        "prophet_direction['60_direction'] = prophet_direction['60'].apply(lambda x: price_direction(x))\n",
        "prophet_direction['120_direction'] = prophet_direction['120'].apply(lambda x: price_direction(x))\n",
        "prophet_direction['240_direction'] = prophet_direction['240'].apply(lambda x: price_direction(x))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahBUQABCrEVe"
      },
      "source": [
        "We create a new dataframe with the indicators dataframe in order to have the target and the signal columns in the same dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUbgiWhUxkkg"
      },
      "source": [
        "indicators['Date']=indicators['Date'].astype('datetime64')\n",
        "prophet_direction['ds'] = prophet_direction['ds'].astype('datetime64')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjzTtd-szj6z"
      },
      "source": [
        "prophet_labelled = indicators.merge(prophet_direction, how='inner', left_on='Date', right_on='ds')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "04WUddCk0PjI",
        "outputId": "69ea3c36-384b-4862-bcd4-6f513b3b41ee"
      },
      "source": [
        "prophet_labelled.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Week_evolution</th>\n",
              "      <th>Day_evolution</th>\n",
              "      <th>Week_difference</th>\n",
              "      <th>Day_difference</th>\n",
              "      <th>Close</th>\n",
              "      <th>ma_optimum</th>\n",
              "      <th>MACD_diff</th>\n",
              "      <th>MACD_signal</th>\n",
              "      <th>MACD_optimum</th>\n",
              "      <th>RSI_value</th>\n",
              "      <th>RSI_signal</th>\n",
              "      <th>RSI_optimum</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ds</th>\n",
              "      <th>y</th>\n",
              "      <th>60</th>\n",
              "      <th>120</th>\n",
              "      <th>240</th>\n",
              "      <th>60_direction</th>\n",
              "      <th>120_direction</th>\n",
              "      <th>240_direction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-06-15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.040039</td>\n",
              "      <td>0.689941</td>\n",
              "      <td>2432.459961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2017-06-15</td>\n",
              "      <td>2432.459961</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-06-16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.150146</td>\n",
              "      <td>20.310059</td>\n",
              "      <td>2433.149902</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.206982</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>2017-06-16</td>\n",
              "      <td>2433.149902</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-14.389893</td>\n",
              "      <td>-16.429932</td>\n",
              "      <td>2453.459961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.113716</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2453.459961</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-06-20</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-17.650146</td>\n",
              "      <td>-1.419922</td>\n",
              "      <td>2437.030029</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.359004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>38.596787</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-06-20</td>\n",
              "      <td>2437.030029</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-06-21</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.079834</td>\n",
              "      <td>-1.110107</td>\n",
              "      <td>2435.610107</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.126017</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.893447</td>\n",
              "      <td>6</td>\n",
              "      <td>2017-06-21</td>\n",
              "      <td>2435.610107</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  Week_evolution  ...  120_direction  240_direction\n",
              "0 2017-06-15             0.0  ...              0              0\n",
              "1 2017-06-16             1.0  ...              0              0\n",
              "2 2017-06-19            -1.0  ...              0              0\n",
              "3 2017-06-20            -1.0  ...              0              0\n",
              "4 2017-06-21             1.0  ...              0              0\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyT7vQAorOM2"
      },
      "source": [
        "Now we can calculate the accuracy and the precision of both the buying and selling signals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS-0GbK6YBlE",
        "outputId": "45796ded-9be9-4399-b441-b60f12f6bdef"
      },
      "source": [
        "for name, compensation in zip(['60_direction', '120_direction', '240_direction'], [60, 120, 240]):\n",
        "  available_days = len(prophet_labelled) - compensation - 5\n",
        "  accuracy = (prophet_labelled['Week_evolution'] == prophet_labelled['60_direction']).sum() / available_days *100 \n",
        "  print(f'The accuracy using a fitting window of {compensation} days is {accuracy} %')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy using a fitting window of 60 days is 48.08510638297872 %\n",
            "The accuracy using a fitting window of 120 days is 51.36363636363637 %\n",
            "The accuracy using a fitting window of 240 days is 59.473684210526315 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZmCCE7lapOD",
        "outputId": "abc85e27-3336-46db-bf69-c3d8b2a71068"
      },
      "source": [
        "for name, compensation in zip(['60_direction', '120_direction', '240_direction'], [60, 120, 240]):\n",
        "  df_signals = prophet_labelled.loc[prophet_labelled[name] == 1, :]\n",
        "  available_days = len(df_signals)\n",
        "  precision = (df_signals['Week_evolution'] == df_signals['60_direction']).sum() / available_days *100 \n",
        "  print(f'The precision of the buy signal using a fitting window of {compensation} days is {precision} %')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision of the buy signal using a fitting window of 60 days is 62.32179226069247 %\n",
            "The precision of the buy signal using a fitting window of 120 days is 61.16071428571429 %\n",
            "The precision of the buy signal using a fitting window of 240 days is 60.25 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS9hHce9mZ5y",
        "outputId": "9abd9ae2-6877-4def-c7c1-66105332e1ec"
      },
      "source": [
        "for name, compensation in zip(['60_direction', '120_direction', '240_direction'], [60, 120, 240]):\n",
        "  df_signals = prophet_labelled.loc[prophet_labelled[name] == -1, :]\n",
        "  available_days = len(df_signals)\n",
        "  precision = (df_signals['Week_evolution'] == df_signals['60_direction']).sum() / available_days *100 \n",
        "  print(f'The precision of the buy signal using a fitting window of {compensation} days is {precision} %')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision of the buy signal using a fitting window of 60 days is 33.582089552238806 %\n",
            "The precision of the buy signal using a fitting window of 120 days is 34.096692111959285 %\n",
            "The precision of the buy signal using a fitting window of 240 days is 32.52279635258359 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-l-r5CVZGeo"
      },
      "source": [
        "indicators['prophet_optimum'] = df_signals['240_direction']"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XurPNAW_48mx"
      },
      "source": [
        "### 2.2.6- Sentiment signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNScyB15FU-"
      },
      "source": [
        "The dataframe with the sentiment data from finantial news scraped and processed with the fine-tunned finBERT model is imported and cleaned to select only the days with more than 20 news (thus dropping weekends)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "nzr0j4vS5BWe",
        "outputId": "09e2012e-893a-4183-ba70-150b7964fc26"
      },
      "source": [
        "sent_df = pd.read_csv('news_sentiment_data.csv')\n",
        "sent_df = sent_df.loc[sent_df['Head_Pos_y']>10, :]\n",
        "sent_df.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Head_Pos_x</th>\n",
              "      <th>Head_neutral_x</th>\n",
              "      <th>Head_Neg_x</th>\n",
              "      <th>Body_Pos_x</th>\n",
              "      <th>Body_neutral_x</th>\n",
              "      <th>Body_Neg_x</th>\n",
              "      <th>Summ_Head</th>\n",
              "      <th>Summ_Body</th>\n",
              "      <th>Head_Pos_y</th>\n",
              "      <th>Head_neutral_y</th>\n",
              "      <th>Head_Neg_y</th>\n",
              "      <th>Body_Pos_y</th>\n",
              "      <th>Body_neutral_y</th>\n",
              "      <th>Body_Neg_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2021-03-12</td>\n",
              "      <td>0.211117</td>\n",
              "      <td>0.120124</td>\n",
              "      <td>0.668759</td>\n",
              "      <td>0.173189</td>\n",
              "      <td>0.186587</td>\n",
              "      <td>0.640224</td>\n",
              "      <td>-0.457642</td>\n",
              "      <td>-0.467035</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2021-03-15</td>\n",
              "      <td>0.322917</td>\n",
              "      <td>0.264581</td>\n",
              "      <td>0.412502</td>\n",
              "      <td>0.456975</td>\n",
              "      <td>0.388529</td>\n",
              "      <td>0.154496</td>\n",
              "      <td>-0.089585</td>\n",
              "      <td>0.302479</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2021-03-16</td>\n",
              "      <td>0.375604</td>\n",
              "      <td>0.300247</td>\n",
              "      <td>0.324149</td>\n",
              "      <td>0.326206</td>\n",
              "      <td>0.477734</td>\n",
              "      <td>0.196060</td>\n",
              "      <td>0.051456</td>\n",
              "      <td>0.130146</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2021-03-17</td>\n",
              "      <td>0.297586</td>\n",
              "      <td>0.328815</td>\n",
              "      <td>0.373599</td>\n",
              "      <td>0.360851</td>\n",
              "      <td>0.436148</td>\n",
              "      <td>0.203001</td>\n",
              "      <td>-0.076013</td>\n",
              "      <td>0.157849</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2021-03-18</td>\n",
              "      <td>0.328113</td>\n",
              "      <td>0.386985</td>\n",
              "      <td>0.284902</td>\n",
              "      <td>0.348881</td>\n",
              "      <td>0.511252</td>\n",
              "      <td>0.139867</td>\n",
              "      <td>0.043211</td>\n",
              "      <td>0.209014</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          date  Head_Pos_x  ...  Body_neutral_y  Body_Neg_y\n",
              "13  2021-03-12    0.211117  ...              11          11\n",
              "16  2021-03-15    0.322917  ...              37          37\n",
              "17  2021-03-16    0.375604  ...              59          59\n",
              "18  2021-03-17    0.297586  ...              60          60\n",
              "19  2021-03-18    0.328113  ...              66          66\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyo0wEuo7ptb"
      },
      "source": [
        "sent_df['date'] = sent_df['date'].astype('datetime64')"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxrafehM9nka"
      },
      "source": [
        "indicators_final = indicators.merge(right=sent_df.loc[:, ['date', 'Summ_Head', 'Summ_Body']], how='outer', left_on='Date', right_on='date')"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbLTiy97-Gs0",
        "outputId": "7dc392a8-8fee-48b2-db4c-6a7423b2d046"
      },
      "source": [
        "indicators_final.info()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1008 entries, 0 to 1007\n",
            "Data columns (total 17 columns):\n",
            " #   Column           Non-Null Count  Dtype         \n",
            "---  ------           --------------  -----         \n",
            " 0   Date             1006 non-null   datetime64[ns]\n",
            " 1   Week_evolution   1006 non-null   float64       \n",
            " 2   Day_evolution    1006 non-null   float64       \n",
            " 3   Week_difference  1006 non-null   float64       \n",
            " 4   Day_difference   1006 non-null   float64       \n",
            " 5   Close            1006 non-null   float64       \n",
            " 6   ma_optimum       1006 non-null   float64       \n",
            " 7   MACD_diff        1006 non-null   float64       \n",
            " 8   MACD_signal      1006 non-null   float64       \n",
            " 9   MACD_optimum     1006 non-null   float64       \n",
            " 10  RSI_value        1006 non-null   float64       \n",
            " 11  RSI_signal       1006 non-null   float64       \n",
            " 12  RSI_optimum      1005 non-null   float64       \n",
            " 13  prophet_optimum  329 non-null    float64       \n",
            " 14  date             45 non-null     datetime64[ns]\n",
            " 15  Summ_Head        45 non-null     float64       \n",
            " 16  Summ_Body        45 non-null     float64       \n",
            "dtypes: datetime64[ns](2), float64(15)\n",
            "memory usage: 141.8 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDtcukDJGxpH"
      },
      "source": [
        "## 2.3- Cleaning the indicators df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yo_QzNkjwAf"
      },
      "source": [
        "The elements close to the margin have NaN values, so we impute them with 0 (no signal for the indicators like SMA and no evolution for the targets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpB1cm56giR0"
      },
      "source": [
        "indicators_final.fillna(0, inplace=True)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "488ssFvm-M0X",
        "outputId": "a3f9fc21-0c7e-496f-e056-87cc721f7d07"
      },
      "source": [
        "indicators_final.info()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1008 entries, 0 to 1007\n",
            "Data columns (total 17 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Date             1008 non-null   object \n",
            " 1   Week_evolution   1008 non-null   float64\n",
            " 2   Day_evolution    1008 non-null   float64\n",
            " 3   Week_difference  1008 non-null   float64\n",
            " 4   Day_difference   1008 non-null   float64\n",
            " 5   Close            1008 non-null   float64\n",
            " 6   ma_optimum       1008 non-null   float64\n",
            " 7   MACD_diff        1008 non-null   float64\n",
            " 8   MACD_signal      1008 non-null   float64\n",
            " 9   MACD_optimum     1008 non-null   float64\n",
            " 10  RSI_value        1008 non-null   float64\n",
            " 11  RSI_signal       1008 non-null   float64\n",
            " 12  RSI_optimum      1008 non-null   float64\n",
            " 13  prophet_optimum  1008 non-null   float64\n",
            " 14  date             1008 non-null   object \n",
            " 15  Summ_Head        1008 non-null   float64\n",
            " 16  Summ_Body        1008 non-null   float64\n",
            "dtypes: float64(15), object(2)\n",
            "memory usage: 141.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYb4KZjpTpw8"
      },
      "source": [
        "# 3- Deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiB7pyacUlm_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tpssvv2CPEb"
      },
      "source": [
        "# 3.1- Dataloader instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMI4qSwlDh6d"
      },
      "source": [
        "We use the primitives dataloader and dataset to create an iterable with the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3DwEwbRANmb"
      },
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, x_train, y_train):\n",
        "        self.x=torch.from_numpy(x_train)\n",
        "        self.y=torch.from_numpy(y_train)\n",
        "        self.len=self.x.shape[0]\n",
        "    def __getitem__(self,index):      \n",
        "        return self.x[index], self.y[index]\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-JgAZCpCIs5"
      },
      "source": [
        "# 3.2- Test and training sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dX7Y3fmmLY5"
      },
      "source": [
        "This next chunk is only to create the train and test datasets without considering the sentiment data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtC0DS6ePT0R"
      },
      "source": [
        "features = indicators_final.loc[:,['Week_evolution', 'RSI_optimum', 'MACD_optimum', 'ma_optimum', 'prophet_optimum']]\n",
        "target = features['Week_evolution'].values.astype('float')\n",
        "features.drop('Week_evolution', axis=1, inplace=True)\n",
        "\n",
        "features = features.values.astype('float')\n",
        "\n",
        "#Label encoding since Pytorch CrossEntropy loss function does not deal with negative values\n",
        "lab_enc = LabelEncoder()\n",
        "target = lab_enc.fit_transform(target)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(features,target, test_size=0.4, random_state=22)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYHtlNYamYzq"
      },
      "source": [
        "The next two code cells prepare the input and target data for the nn with stratifying the train and test split, so both sets have the same proportion of non-null values for the sentiment rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp0yEsXPHH9H"
      },
      "source": [
        "def train_test_strat_sent(features_df:pd.DataFrame, test_size:float, random_state:int, sent_col_name:str='Summ_Head'): \n",
        "  # This funtion gets a df, test set proportion and random seed and returns an\n",
        "  # stratified split of the input data, using the sent_col_name column name to \n",
        "  # locate the column which will dictate the stratification. This stratification\n",
        "  # is performed separating the rows with null and non-null values for this column.\n",
        "\n",
        "  features_with_sent = features.loc[features[sent_col_name] != 0, :]\n",
        "  features_no_sent = features.loc[features[sent_col_name] == 0, :]\n",
        "\n",
        "  target_nosent = features_no_sent['Week_evolution'].values.astype('float')\n",
        "  features_no_sent.drop('Week_evolution', axis=1, inplace=True)\n",
        "  target_with_sent = features_with_sent['Week_evolution'].values.astype('float')\n",
        "  features_with_sent.drop('Week_evolution', axis=1, inplace=True)\n",
        "\n",
        "  features_with_sent = features_with_sent.values.astype('float')\n",
        "  features_no_sent = features_no_sent.values.astype('float')\n",
        "  #Label encoding since Pytorch CrossEntropy loss function does not deal with negative values\n",
        "  lab_enc = LabelEncoder()\n",
        "  target_nosent = lab_enc.fit_transform(target_nosent)\n",
        "  target_with_sent = lab_enc.transform(target_with_sent)\n",
        "\n",
        "  X_train_nosent,X_test_nosent,y_train_nosent,y_test_nosent = train_test_split(features_no_sent,target_nosent, test_size=test_size, random_state=random_state)\n",
        "  X_train_withsent,X_test_withsent,y_train_withsent,y_test_withsent = train_test_split(features_with_sent,target_with_sent,test_size=test_size, random_state=random_state)\n",
        "\n",
        "  X_train = np.concatenate((X_train_nosent, X_train_withsent), axis=0)\n",
        "  X_test = np.concatenate((X_test_nosent, X_test_withsent), axis=0)\n",
        "  y_train = np.concatenate((y_train_nosent, y_train_withsent), axis=0)\n",
        "  y_test = np.concatenate((y_test_nosent, y_test_withsent), axis=0)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1F4lVvJpZI6"
      },
      "source": [
        "features = indicators_final.loc[:,['Week_evolution', 'RSI_optimum', 'MACD_optimum', 'ma_optimum', 'prophet_optimum', 'Summ_Head', 'Summ_Body']]\n",
        "X_train, X_test, y_train, y_test = train_test_strat_sent(features_df=features, random_state=22, test_size=0.5)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBtAgWhsDx9X"
      },
      "source": [
        "data_set=Data(x_train=X_train, y_train=y_train)\n",
        "trainloader=DataLoader(dataset=data_set,batch_size=int(len(X_train)/1))"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qxTw3-CXPU"
      },
      "source": [
        "# 3.3- Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CrwM92QZwK0"
      },
      "source": [
        "Model parameters are defined as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-WqxbiG77I3"
      },
      "source": [
        "input_dim=6     # how many Variables are in the dataset\n",
        "hidden_dim = 25 # hidden layers\n",
        "output_dim=3    # number of classes"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz0Jol1iZ1YO"
      },
      "source": [
        "class DNN_Model(nn.Module):\n",
        "  def __init__(self, D_in, H1, H2, H3, D_out):\n",
        "    super(DNN_Model, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H1)\n",
        "    self.linear2 = nn.Linear(H1, H2)\n",
        "    self.linear3 = nn.Linear(H2, H3)\n",
        "    self.linear4 = nn.Linear(H3, D_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.sigmoid(self.linear1(x))\n",
        "    x = torch.sigmoid(self.linear2(x))\n",
        "    x = torch.sigmoid(self.linear3(x))\n",
        "    x = self.linear4(x)\n",
        "    return x"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3YoXLj_HRL"
      },
      "source": [
        "Instantiate a first deep model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGulJotS_J_a"
      },
      "source": [
        "# Instantiate model\n",
        "dnn_model=DNN_Model(input_dim, hidden_dim, hidden_dim, hidden_dim, output_dim)\n",
        "loss_dnn = nn.CrossEntropyLoss(reduction='mean')\n",
        "learning_rate_dnn = 1e-2\n",
        "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate_dnn)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr8mbmFNN6Kt",
        "outputId": "eb8fcc12-8cec-41c5-ae86-ef74f74e45b6"
      },
      "source": [
        "len(target_tensor)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA-FziE7Ye2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4329f487-6819-4134-9ef1-8d83ca5d6053"
      },
      "source": [
        "n_epochs=10000\n",
        "loss_list=[]\n",
        "target_tensor = torch.Tensor(y_test)\n",
        "#n_epochs\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in trainloader:\n",
        "\n",
        "        x = x.float()\n",
        "        y = y.long()\n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z=dnn_model(x)\n",
        "        # calculate loss\n",
        "        loss=loss_dnn(z,y)\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.data)\n",
        "        \n",
        "        correct = (torch.max(z.data,1).indices == y).sum()\n",
        "        training_accuracy = correct/len(z)*100\n",
        "        tensor_test = torch.Tensor(X_test)\n",
        "        dnn_pred = dnn_model(tensor_test)\n",
        "        correct_pred = (torch.max(dnn_pred.data,1).indices == target_tensor).sum()\n",
        "        test_accuracy = correct_pred/len(target_tensor)*100\n",
        "        print(f'epoch {epoch}, loss {loss.item():.5f}, training accuracy {training_accuracy:.2f}%, test accuracy {test_accuracy:.2f}%')"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a t tronqu et ne contient que les 5000dernires lignes.\u001b[0m\n",
            "epoch 5000, loss 0.48420, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 5001, loss 0.48514, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5002, loss 0.48706, training accuracy 77.73%, test accuracy 51.09%\n",
            "epoch 5003, loss 0.48773, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5004, loss 0.48992, training accuracy 77.53%, test accuracy 50.50%\n",
            "epoch 5005, loss 0.48993, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5006, loss 0.49174, training accuracy 77.53%, test accuracy 50.10%\n",
            "epoch 5007, loss 0.49038, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5008, loss 0.49019, training accuracy 77.53%, test accuracy 50.50%\n",
            "epoch 5009, loss 0.48749, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5010, loss 0.48578, training accuracy 77.93%, test accuracy 51.49%\n",
            "epoch 5011, loss 0.48316, training accuracy 77.53%, test accuracy 51.49%\n",
            "epoch 5012, loss 0.48113, training accuracy 78.53%, test accuracy 50.89%\n",
            "epoch 5013, loss 0.48002, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5014, loss 0.47928, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5015, loss 0.47930, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 5016, loss 0.47955, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5017, loss 0.48004, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5018, loss 0.48078, training accuracy 77.93%, test accuracy 51.68%\n",
            "epoch 5019, loss 0.48170, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 5020, loss 0.48265, training accuracy 78.13%, test accuracy 52.08%\n",
            "epoch 5021, loss 0.48378, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 5022, loss 0.48455, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5023, loss 0.48597, training accuracy 77.93%, test accuracy 50.89%\n",
            "epoch 5024, loss 0.48636, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5025, loss 0.48748, training accuracy 77.53%, test accuracy 50.69%\n",
            "epoch 5026, loss 0.48705, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5027, loss 0.48764, training accuracy 77.53%, test accuracy 50.69%\n",
            "epoch 5028, loss 0.48642, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5029, loss 0.48592, training accuracy 77.93%, test accuracy 51.68%\n",
            "epoch 5030, loss 0.48425, training accuracy 78.13%, test accuracy 52.08%\n",
            "epoch 5031, loss 0.48321, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 5032, loss 0.48181, training accuracy 77.34%, test accuracy 51.49%\n",
            "epoch 5033, loss 0.48079, training accuracy 78.33%, test accuracy 51.09%\n",
            "epoch 5034, loss 0.47995, training accuracy 77.73%, test accuracy 51.09%\n",
            "epoch 5035, loss 0.47935, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 5036, loss 0.47901, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5037, loss 0.47878, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5038, loss 0.47872, training accuracy 78.93%, test accuracy 50.50%\n",
            "epoch 5039, loss 0.47872, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5040, loss 0.47880, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 5041, loss 0.47894, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5042, loss 0.47912, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 5043, loss 0.47937, training accuracy 77.93%, test accuracy 51.49%\n",
            "epoch 5044, loss 0.47975, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5045, loss 0.48021, training accuracy 77.93%, test accuracy 51.49%\n",
            "epoch 5046, loss 0.48096, training accuracy 78.93%, test accuracy 51.88%\n",
            "epoch 5047, loss 0.48183, training accuracy 77.93%, test accuracy 52.08%\n",
            "epoch 5048, loss 0.48350, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5049, loss 0.48514, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5050, loss 0.48858, training accuracy 77.53%, test accuracy 50.30%\n",
            "epoch 5051, loss 0.49115, training accuracy 78.33%, test accuracy 51.88%\n",
            "epoch 5052, loss 0.49767, training accuracy 77.73%, test accuracy 49.11%\n",
            "epoch 5053, loss 0.50031, training accuracy 78.13%, test accuracy 51.29%\n",
            "epoch 5054, loss 0.50968, training accuracy 77.14%, test accuracy 48.71%\n",
            "epoch 5055, loss 0.51174, training accuracy 77.93%, test accuracy 51.88%\n",
            "epoch 5056, loss 0.52726, training accuracy 76.94%, test accuracy 48.12%\n",
            "epoch 5057, loss 0.52926, training accuracy 75.94%, test accuracy 52.87%\n",
            "epoch 5058, loss 0.53620, training accuracy 76.94%, test accuracy 48.91%\n",
            "epoch 5059, loss 0.51224, training accuracy 77.14%, test accuracy 52.28%\n",
            "epoch 5060, loss 0.49548, training accuracy 77.73%, test accuracy 51.49%\n",
            "epoch 5061, loss 0.48256, training accuracy 77.93%, test accuracy 50.89%\n",
            "epoch 5062, loss 0.49015, training accuracy 77.93%, test accuracy 51.88%\n",
            "epoch 5063, loss 0.51155, training accuracy 77.53%, test accuracy 49.50%\n",
            "epoch 5064, loss 0.50556, training accuracy 77.34%, test accuracy 52.28%\n",
            "epoch 5065, loss 0.50863, training accuracy 77.53%, test accuracy 51.49%\n",
            "epoch 5066, loss 0.51688, training accuracy 77.53%, test accuracy 52.87%\n",
            "epoch 5067, loss 0.49574, training accuracy 77.53%, test accuracy 51.49%\n",
            "epoch 5068, loss 0.51780, training accuracy 76.34%, test accuracy 51.68%\n",
            "epoch 5069, loss 0.53179, training accuracy 77.34%, test accuracy 53.47%\n",
            "epoch 5070, loss 0.55214, training accuracy 75.75%, test accuracy 48.51%\n",
            "epoch 5071, loss 0.52007, training accuracy 73.16%, test accuracy 53.07%\n",
            "epoch 5072, loss 0.53086, training accuracy 76.34%, test accuracy 53.07%\n",
            "epoch 5073, loss 0.50020, training accuracy 77.34%, test accuracy 49.11%\n",
            "epoch 5074, loss 0.52721, training accuracy 76.34%, test accuracy 53.27%\n",
            "epoch 5075, loss 0.51836, training accuracy 76.34%, test accuracy 51.09%\n",
            "epoch 5076, loss 0.52228, training accuracy 76.94%, test accuracy 53.07%\n",
            "epoch 5077, loss 0.51494, training accuracy 76.74%, test accuracy 51.49%\n",
            "epoch 5078, loss 0.50572, training accuracy 78.53%, test accuracy 52.48%\n",
            "epoch 5079, loss 0.49906, training accuracy 77.53%, test accuracy 50.69%\n",
            "epoch 5080, loss 0.49480, training accuracy 78.33%, test accuracy 50.69%\n",
            "epoch 5081, loss 0.50149, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 5082, loss 0.51493, training accuracy 77.53%, test accuracy 51.88%\n",
            "epoch 5083, loss 0.49770, training accuracy 78.13%, test accuracy 50.89%\n",
            "epoch 5084, loss 0.50256, training accuracy 76.94%, test accuracy 51.88%\n",
            "epoch 5085, loss 0.49161, training accuracy 77.53%, test accuracy 52.08%\n",
            "epoch 5086, loss 0.49512, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 5087, loss 0.49094, training accuracy 77.73%, test accuracy 51.09%\n",
            "epoch 5088, loss 0.49242, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 5089, loss 0.48994, training accuracy 77.73%, test accuracy 51.88%\n",
            "epoch 5090, loss 0.49527, training accuracy 78.33%, test accuracy 50.69%\n",
            "epoch 5091, loss 0.48911, training accuracy 78.13%, test accuracy 52.08%\n",
            "epoch 5092, loss 0.49426, training accuracy 77.93%, test accuracy 52.08%\n",
            "epoch 5093, loss 0.48493, training accuracy 77.53%, test accuracy 51.49%\n",
            "epoch 5094, loss 0.49197, training accuracy 77.34%, test accuracy 52.28%\n",
            "epoch 5095, loss 0.48984, training accuracy 78.33%, test accuracy 50.30%\n",
            "epoch 5096, loss 0.49334, training accuracy 77.73%, test accuracy 52.67%\n",
            "epoch 5097, loss 0.48764, training accuracy 77.53%, test accuracy 51.68%\n",
            "epoch 5098, loss 0.48920, training accuracy 77.34%, test accuracy 51.09%\n",
            "epoch 5099, loss 0.48547, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 5100, loss 0.49106, training accuracy 78.33%, test accuracy 51.68%\n",
            "epoch 5101, loss 0.48487, training accuracy 78.53%, test accuracy 50.69%\n",
            "epoch 5102, loss 0.48621, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5103, loss 0.48096, training accuracy 78.13%, test accuracy 50.89%\n",
            "epoch 5104, loss 0.48389, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5105, loss 0.48264, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5106, loss 0.48254, training accuracy 77.93%, test accuracy 51.68%\n",
            "epoch 5107, loss 0.48064, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5108, loss 0.48125, training accuracy 78.53%, test accuracy 51.09%\n",
            "epoch 5109, loss 0.48085, training accuracy 77.73%, test accuracy 51.49%\n",
            "epoch 5110, loss 0.48102, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5111, loss 0.48089, training accuracy 78.13%, test accuracy 51.29%\n",
            "epoch 5112, loss 0.47978, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5113, loss 0.47990, training accuracy 77.93%, test accuracy 51.09%\n",
            "epoch 5114, loss 0.48029, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 5115, loss 0.48028, training accuracy 77.73%, test accuracy 51.29%\n",
            "epoch 5116, loss 0.48021, training accuracy 77.53%, test accuracy 50.89%\n",
            "epoch 5117, loss 0.47937, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5118, loss 0.47871, training accuracy 78.93%, test accuracy 50.89%\n",
            "epoch 5119, loss 0.47962, training accuracy 77.73%, test accuracy 51.09%\n",
            "epoch 5120, loss 0.47901, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5121, loss 0.47932, training accuracy 77.93%, test accuracy 51.49%\n",
            "epoch 5122, loss 0.47834, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5123, loss 0.47847, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5124, loss 0.47808, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5125, loss 0.47851, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5126, loss 0.47815, training accuracy 78.13%, test accuracy 51.29%\n",
            "epoch 5127, loss 0.47839, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5128, loss 0.47777, training accuracy 78.53%, test accuracy 50.89%\n",
            "epoch 5129, loss 0.47784, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5130, loss 0.47778, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5131, loss 0.47800, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 5132, loss 0.47780, training accuracy 78.93%, test accuracy 50.89%\n",
            "epoch 5133, loss 0.47779, training accuracy 78.53%, test accuracy 51.09%\n",
            "epoch 5134, loss 0.47750, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5135, loss 0.47755, training accuracy 78.33%, test accuracy 50.89%\n",
            "epoch 5136, loss 0.47747, training accuracy 78.53%, test accuracy 51.09%\n",
            "epoch 5137, loss 0.47748, training accuracy 78.53%, test accuracy 50.89%\n",
            "epoch 5138, loss 0.47749, training accuracy 78.33%, test accuracy 51.49%\n",
            "epoch 5139, loss 0.47739, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5140, loss 0.47731, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5141, loss 0.47720, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5142, loss 0.47721, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5143, loss 0.47713, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5144, loss 0.47718, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5145, loss 0.47709, training accuracy 78.33%, test accuracy 51.09%\n",
            "epoch 5146, loss 0.47707, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5147, loss 0.47697, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5148, loss 0.47694, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5149, loss 0.47688, training accuracy 78.53%, test accuracy 51.09%\n",
            "epoch 5150, loss 0.47688, training accuracy 78.13%, test accuracy 51.29%\n",
            "epoch 5151, loss 0.47683, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5152, loss 0.47681, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5153, loss 0.47677, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5154, loss 0.47671, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5155, loss 0.47668, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5156, loss 0.47661, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5157, loss 0.47659, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5158, loss 0.47655, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5159, loss 0.47653, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5160, loss 0.47649, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5161, loss 0.47646, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5162, loss 0.47642, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5163, loss 0.47638, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5164, loss 0.47634, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5165, loss 0.47630, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5166, loss 0.47626, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5167, loss 0.47622, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5168, loss 0.47619, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5169, loss 0.47615, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5170, loss 0.47612, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5171, loss 0.47609, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5172, loss 0.47606, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5173, loss 0.47602, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5174, loss 0.47599, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5175, loss 0.47596, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5176, loss 0.47592, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5177, loss 0.47589, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5178, loss 0.47586, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5179, loss 0.47582, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5180, loss 0.47579, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5181, loss 0.47576, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5182, loss 0.47572, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5183, loss 0.47569, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5184, loss 0.47566, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5185, loss 0.47563, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5186, loss 0.47560, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5187, loss 0.47557, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5188, loss 0.47555, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5189, loss 0.47552, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5190, loss 0.47550, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5191, loss 0.47548, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5192, loss 0.47546, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5193, loss 0.47546, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5194, loss 0.47548, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5195, loss 0.47552, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5196, loss 0.47561, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5197, loss 0.47579, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5198, loss 0.47608, training accuracy 78.13%, test accuracy 52.08%\n",
            "epoch 5199, loss 0.47668, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 5200, loss 0.47757, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5201, loss 0.47944, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 5202, loss 0.48183, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5203, loss 0.48722, training accuracy 77.93%, test accuracy 49.11%\n",
            "epoch 5204, loss 0.49177, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5205, loss 0.50311, training accuracy 77.73%, test accuracy 49.11%\n",
            "epoch 5206, loss 0.50440, training accuracy 77.93%, test accuracy 52.08%\n",
            "epoch 5207, loss 0.51445, training accuracy 77.34%, test accuracy 49.31%\n",
            "epoch 5208, loss 0.50411, training accuracy 77.93%, test accuracy 51.88%\n",
            "epoch 5209, loss 0.49853, training accuracy 77.73%, test accuracy 51.09%\n",
            "epoch 5210, loss 0.48457, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5211, loss 0.47718, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5212, loss 0.47554, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 5213, loss 0.47942, training accuracy 78.13%, test accuracy 52.48%\n",
            "epoch 5214, loss 0.48694, training accuracy 78.13%, test accuracy 49.90%\n",
            "epoch 5215, loss 0.49052, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5216, loss 0.49474, training accuracy 77.93%, test accuracy 50.30%\n",
            "epoch 5217, loss 0.48936, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5218, loss 0.48444, training accuracy 78.13%, test accuracy 51.88%\n",
            "epoch 5219, loss 0.47814, training accuracy 77.93%, test accuracy 51.68%\n",
            "epoch 5220, loss 0.47525, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 5221, loss 0.47581, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 5222, loss 0.47868, training accuracy 77.93%, test accuracy 52.67%\n",
            "epoch 5223, loss 0.48209, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5224, loss 0.48243, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 5225, loss 0.48190, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5226, loss 0.47889, training accuracy 78.33%, test accuracy 52.48%\n",
            "epoch 5227, loss 0.47654, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5228, loss 0.47503, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5229, loss 0.47492, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5230, loss 0.47604, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 5231, loss 0.47720, training accuracy 78.33%, test accuracy 52.48%\n",
            "epoch 5232, loss 0.47838, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5233, loss 0.47835, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5234, loss 0.47782, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5235, loss 0.47665, training accuracy 77.93%, test accuracy 52.48%\n",
            "epoch 5236, loss 0.47559, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5237, loss 0.47481, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5238, loss 0.47452, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5239, loss 0.47475, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5240, loss 0.47513, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5241, loss 0.47565, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 5242, loss 0.47589, training accuracy 78.13%, test accuracy 52.28%\n",
            "epoch 5243, loss 0.47599, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 5244, loss 0.47582, training accuracy 77.93%, test accuracy 52.28%\n",
            "epoch 5245, loss 0.47552, training accuracy 79.92%, test accuracy 51.49%\n",
            "epoch 5246, loss 0.47518, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5247, loss 0.47477, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5248, loss 0.47450, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5249, loss 0.47428, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5250, loss 0.47421, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5251, loss 0.47420, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5252, loss 0.47424, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5253, loss 0.47435, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5254, loss 0.47444, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5255, loss 0.47457, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5256, loss 0.47467, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5257, loss 0.47477, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5258, loss 0.47489, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5259, loss 0.47496, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 5260, loss 0.47509, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 5261, loss 0.47515, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 5262, loss 0.47532, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 5263, loss 0.47542, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5264, loss 0.47570, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 5265, loss 0.47587, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 5266, loss 0.47631, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5267, loss 0.47663, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5268, loss 0.47739, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5269, loss 0.47787, training accuracy 78.53%, test accuracy 52.67%\n",
            "epoch 5270, loss 0.47905, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5271, loss 0.47966, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5272, loss 0.48139, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5273, loss 0.48187, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5274, loss 0.48377, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5275, loss 0.48339, training accuracy 78.33%, test accuracy 52.48%\n",
            "epoch 5276, loss 0.48460, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 5277, loss 0.48294, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 5278, loss 0.48242, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5279, loss 0.48000, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 5280, loss 0.47857, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5281, loss 0.47664, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5282, loss 0.47531, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 5283, loss 0.47440, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 5284, loss 0.47379, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5285, loss 0.47357, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5286, loss 0.47345, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5287, loss 0.47356, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5288, loss 0.47371, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5289, loss 0.47400, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5290, loss 0.47434, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 5291, loss 0.47479, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5292, loss 0.47533, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 5293, loss 0.47615, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 5294, loss 0.47693, training accuracy 79.13%, test accuracy 52.67%\n",
            "epoch 5295, loss 0.47825, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5296, loss 0.47924, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5297, loss 0.48139, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5298, loss 0.48235, training accuracy 79.13%, test accuracy 52.67%\n",
            "epoch 5299, loss 0.48483, training accuracy 78.33%, test accuracy 51.09%\n",
            "epoch 5300, loss 0.48479, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 5301, loss 0.48668, training accuracy 78.33%, test accuracy 51.09%\n",
            "epoch 5302, loss 0.48475, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5303, loss 0.48397, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5304, loss 0.48077, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5305, loss 0.47861, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 5306, loss 0.47621, training accuracy 78.13%, test accuracy 52.08%\n",
            "epoch 5307, loss 0.47434, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5308, loss 0.47362, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5309, loss 0.47305, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5310, loss 0.47331, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5311, loss 0.47361, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5312, loss 0.47419, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 5313, loss 0.47492, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5314, loss 0.47585, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 5315, loss 0.47675, training accuracy 79.13%, test accuracy 52.67%\n",
            "epoch 5316, loss 0.47774, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 5317, loss 0.47838, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5318, loss 0.47949, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5319, loss 0.47959, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5320, loss 0.48014, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5321, loss 0.47930, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5322, loss 0.47917, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5323, loss 0.47792, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5324, loss 0.47725, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5325, loss 0.47599, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5326, loss 0.47523, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5327, loss 0.47436, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5328, loss 0.47382, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5329, loss 0.47335, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5330, loss 0.47304, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5331, loss 0.47284, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5332, loss 0.47269, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5333, loss 0.47262, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5334, loss 0.47256, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5335, loss 0.47255, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5336, loss 0.47255, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5337, loss 0.47256, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5338, loss 0.47260, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5339, loss 0.47265, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5340, loss 0.47275, training accuracy 78.93%, test accuracy 51.88%\n",
            "epoch 5341, loss 0.47289, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5342, loss 0.47311, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5343, loss 0.47349, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 5344, loss 0.47401, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5345, loss 0.47500, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 5346, loss 0.47623, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 5347, loss 0.47871, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5348, loss 0.48125, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 5349, loss 0.48684, training accuracy 78.53%, test accuracy 49.50%\n",
            "epoch 5350, loss 0.49038, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5351, loss 0.49983, training accuracy 77.53%, test accuracy 49.31%\n",
            "epoch 5352, loss 0.50072, training accuracy 77.93%, test accuracy 52.08%\n",
            "epoch 5353, loss 0.51017, training accuracy 77.14%, test accuracy 48.71%\n",
            "epoch 5354, loss 0.50547, training accuracy 77.73%, test accuracy 52.28%\n",
            "epoch 5355, loss 0.50791, training accuracy 77.14%, test accuracy 49.70%\n",
            "epoch 5356, loss 0.49543, training accuracy 78.33%, test accuracy 52.67%\n",
            "epoch 5357, loss 0.48489, training accuracy 78.33%, test accuracy 51.68%\n",
            "epoch 5358, loss 0.47510, training accuracy 77.53%, test accuracy 51.29%\n",
            "epoch 5359, loss 0.47346, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 5360, loss 0.47937, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5361, loss 0.48582, training accuracy 78.13%, test accuracy 52.28%\n",
            "epoch 5362, loss 0.49213, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 5363, loss 0.48828, training accuracy 78.33%, test accuracy 52.48%\n",
            "epoch 5364, loss 0.48551, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5365, loss 0.48222, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 5366, loss 0.47443, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5367, loss 0.47564, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5368, loss 0.47979, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 5369, loss 0.47778, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5370, loss 0.48585, training accuracy 77.93%, test accuracy 52.67%\n",
            "epoch 5371, loss 0.48857, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5372, loss 0.47693, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5373, loss 0.48144, training accuracy 78.13%, test accuracy 52.48%\n",
            "epoch 5374, loss 0.48362, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5375, loss 0.47496, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5376, loss 0.48613, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 5377, loss 0.49175, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 5378, loss 0.48331, training accuracy 78.73%, test accuracy 50.10%\n",
            "epoch 5379, loss 0.49843, training accuracy 77.93%, test accuracy 53.27%\n",
            "epoch 5380, loss 0.49049, training accuracy 78.33%, test accuracy 51.68%\n",
            "epoch 5381, loss 0.48236, training accuracy 78.33%, test accuracy 51.68%\n",
            "epoch 5382, loss 0.49124, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5383, loss 0.47781, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5384, loss 0.48443, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5385, loss 0.48103, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 5386, loss 0.47963, training accuracy 77.73%, test accuracy 52.87%\n",
            "epoch 5387, loss 0.48537, training accuracy 77.93%, test accuracy 51.68%\n",
            "epoch 5388, loss 0.47570, training accuracy 78.33%, test accuracy 51.49%\n",
            "epoch 5389, loss 0.48155, training accuracy 78.13%, test accuracy 52.28%\n",
            "epoch 5390, loss 0.48089, training accuracy 78.33%, test accuracy 51.68%\n",
            "epoch 5391, loss 0.47735, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5392, loss 0.48624, training accuracy 78.93%, test accuracy 50.89%\n",
            "epoch 5393, loss 0.47666, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 5394, loss 0.47960, training accuracy 77.93%, test accuracy 52.28%\n",
            "epoch 5395, loss 0.47802, training accuracy 77.73%, test accuracy 51.29%\n",
            "epoch 5396, loss 0.47253, training accuracy 78.53%, test accuracy 52.48%\n",
            "epoch 5397, loss 0.47780, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5398, loss 0.47275, training accuracy 78.33%, test accuracy 51.29%\n",
            "epoch 5399, loss 0.47652, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 5400, loss 0.47563, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 5401, loss 0.47365, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 5402, loss 0.47596, training accuracy 77.34%, test accuracy 51.88%\n",
            "epoch 5403, loss 0.47226, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5404, loss 0.47406, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5405, loss 0.47287, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5406, loss 0.47227, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5407, loss 0.47389, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5408, loss 0.47212, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 5409, loss 0.47343, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5410, loss 0.47288, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5411, loss 0.47221, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5412, loss 0.47293, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5413, loss 0.47161, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5414, loss 0.47221, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5415, loss 0.47192, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5416, loss 0.47161, training accuracy 78.93%, test accuracy 51.88%\n",
            "epoch 5417, loss 0.47226, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5418, loss 0.47161, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5419, loss 0.47199, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5420, loss 0.47190, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5421, loss 0.47156, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5422, loss 0.47185, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 5423, loss 0.47141, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5424, loss 0.47147, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5425, loss 0.47146, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5426, loss 0.47117, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5427, loss 0.47137, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5428, loss 0.47118, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5429, loss 0.47116, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5430, loss 0.47126, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5431, loss 0.47108, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5432, loss 0.47119, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5433, loss 0.47115, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5434, loss 0.47108, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5435, loss 0.47117, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5436, loss 0.47109, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5437, loss 0.47112, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5438, loss 0.47116, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5439, loss 0.47111, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 5440, loss 0.47122, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5441, loss 0.47124, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5442, loss 0.47134, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5443, loss 0.47152, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5444, loss 0.47172, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5445, loss 0.47204, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5446, loss 0.47264, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5447, loss 0.47323, training accuracy 78.13%, test accuracy 52.28%\n",
            "epoch 5448, loss 0.47449, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5449, loss 0.47584, training accuracy 78.93%, test accuracy 52.67%\n",
            "epoch 5450, loss 0.47855, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5451, loss 0.48058, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 5452, loss 0.48542, training accuracy 78.13%, test accuracy 51.09%\n",
            "epoch 5453, loss 0.48697, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 5454, loss 0.49233, training accuracy 77.73%, test accuracy 50.10%\n",
            "epoch 5455, loss 0.49099, training accuracy 78.13%, test accuracy 52.48%\n",
            "epoch 5456, loss 0.49322, training accuracy 77.73%, test accuracy 50.69%\n",
            "epoch 5457, loss 0.48645, training accuracy 78.33%, test accuracy 52.48%\n",
            "epoch 5458, loss 0.48179, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 5459, loss 0.47517, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5460, loss 0.47158, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 5461, loss 0.47063, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 5462, loss 0.47194, training accuracy 78.13%, test accuracy 52.28%\n",
            "epoch 5463, loss 0.47481, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5464, loss 0.47760, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 5465, loss 0.48106, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5466, loss 0.48119, training accuracy 78.93%, test accuracy 52.67%\n",
            "epoch 5467, loss 0.48161, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5468, loss 0.47821, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 5469, loss 0.47535, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 5470, loss 0.47227, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 5471, loss 0.47068, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5472, loss 0.47053, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 5473, loss 0.47148, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 5474, loss 0.47306, training accuracy 79.92%, test accuracy 51.68%\n",
            "epoch 5475, loss 0.47435, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 5476, loss 0.47570, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5477, loss 0.47569, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 5478, loss 0.47571, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5479, loss 0.47438, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 5480, loss 0.47325, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5481, loss 0.47185, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 5482, loss 0.47088, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5483, loss 0.47031, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 5484, loss 0.47018, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5485, loss 0.47038, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 5486, loss 0.47078, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 5487, loss 0.47129, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5488, loss 0.47169, training accuracy 78.13%, test accuracy 52.48%\n",
            "epoch 5489, loss 0.47208, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 5490, loss 0.47216, training accuracy 78.33%, test accuracy 52.67%\n",
            "epoch 5491, loss 0.47226, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 5492, loss 0.47206, training accuracy 78.33%, test accuracy 52.67%\n",
            "epoch 5493, loss 0.47195, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 5494, loss 0.47162, training accuracy 78.53%, test accuracy 52.67%\n",
            "epoch 5495, loss 0.47139, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5496, loss 0.47106, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5497, loss 0.47081, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5498, loss 0.47055, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5499, loss 0.47037, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 5500, loss 0.47020, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5501, loss 0.47007, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5502, loss 0.46998, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5503, loss 0.46991, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5504, loss 0.46986, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5505, loss 0.46982, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5506, loss 0.46979, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5507, loss 0.46976, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5508, loss 0.46974, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5509, loss 0.46972, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 5510, loss 0.46971, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5511, loss 0.46970, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 5512, loss 0.46970, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5513, loss 0.46971, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 5514, loss 0.46974, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5515, loss 0.46980, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 5516, loss 0.46991, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 5517, loss 0.47012, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 5518, loss 0.47044, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5519, loss 0.47107, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 5520, loss 0.47199, training accuracy 78.53%, test accuracy 52.48%\n",
            "epoch 5521, loss 0.47383, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 5522, loss 0.47624, training accuracy 78.93%, test accuracy 52.87%\n",
            "epoch 5523, loss 0.48144, training accuracy 78.73%, test accuracy 50.50%\n",
            "epoch 5524, loss 0.48648, training accuracy 78.53%, test accuracy 52.87%\n",
            "epoch 5525, loss 0.49891, training accuracy 77.73%, test accuracy 48.71%\n",
            "epoch 5526, loss 0.50553, training accuracy 77.93%, test accuracy 53.27%\n",
            "epoch 5527, loss 0.53535, training accuracy 76.34%, test accuracy 46.73%\n",
            "epoch 5528, loss 0.54729, training accuracy 73.76%, test accuracy 53.86%\n",
            "epoch 5529, loss 0.61605, training accuracy 74.95%, test accuracy 46.93%\n",
            "epoch 5530, loss 0.54127, training accuracy 72.96%, test accuracy 53.27%\n",
            "epoch 5531, loss 0.49653, training accuracy 76.94%, test accuracy 52.67%\n",
            "epoch 5532, loss 0.50018, training accuracy 78.93%, test accuracy 46.93%\n",
            "epoch 5533, loss 0.54006, training accuracy 71.37%, test accuracy 55.25%\n",
            "epoch 5534, loss 0.60213, training accuracy 75.35%, test accuracy 50.89%\n",
            "epoch 5535, loss 0.50525, training accuracy 77.93%, test accuracy 46.93%\n",
            "epoch 5536, loss 0.53787, training accuracy 72.56%, test accuracy 54.85%\n",
            "epoch 5537, loss 0.58773, training accuracy 75.55%, test accuracy 51.68%\n",
            "epoch 5538, loss 0.56875, training accuracy 75.94%, test accuracy 51.49%\n",
            "epoch 5539, loss 0.50654, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 5540, loss 0.54507, training accuracy 76.54%, test accuracy 50.30%\n",
            "epoch 5541, loss 0.62867, training accuracy 75.15%, test accuracy 53.86%\n",
            "epoch 5542, loss 0.64261, training accuracy 74.75%, test accuracy 51.68%\n",
            "epoch 5543, loss 0.54772, training accuracy 77.34%, test accuracy 46.73%\n",
            "epoch 5544, loss 0.54188, training accuracy 73.76%, test accuracy 53.66%\n",
            "epoch 5545, loss 0.61887, training accuracy 75.15%, test accuracy 53.07%\n",
            "epoch 5546, loss 0.53714, training accuracy 77.53%, test accuracy 43.37%\n",
            "epoch 5547, loss 0.65141, training accuracy 67.00%, test accuracy 54.26%\n",
            "epoch 5548, loss 0.63407, training accuracy 73.96%, test accuracy 51.88%\n",
            "epoch 5549, loss 0.56322, training accuracy 76.74%, test accuracy 47.92%\n",
            "epoch 5550, loss 0.57406, training accuracy 72.96%, test accuracy 53.66%\n",
            "epoch 5551, loss 0.59240, training accuracy 74.95%, test accuracy 52.08%\n",
            "epoch 5552, loss 0.56572, training accuracy 74.75%, test accuracy 49.90%\n",
            "epoch 5553, loss 0.54638, training accuracy 75.15%, test accuracy 51.88%\n",
            "epoch 5554, loss 0.55076, training accuracy 76.74%, test accuracy 52.08%\n",
            "epoch 5555, loss 0.53350, training accuracy 77.34%, test accuracy 47.92%\n",
            "epoch 5556, loss 0.54423, training accuracy 74.16%, test accuracy 52.48%\n",
            "epoch 5557, loss 0.54519, training accuracy 77.53%, test accuracy 54.26%\n",
            "epoch 5558, loss 0.51024, training accuracy 77.34%, test accuracy 48.32%\n",
            "epoch 5559, loss 0.55231, training accuracy 74.35%, test accuracy 52.08%\n",
            "epoch 5560, loss 0.50104, training accuracy 78.13%, test accuracy 53.47%\n",
            "epoch 5561, loss 0.51778, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5562, loss 0.52228, training accuracy 76.74%, test accuracy 52.48%\n",
            "epoch 5563, loss 0.48994, training accuracy 78.33%, test accuracy 51.29%\n",
            "epoch 5564, loss 0.51183, training accuracy 77.93%, test accuracy 50.50%\n",
            "epoch 5565, loss 0.49685, training accuracy 78.13%, test accuracy 52.08%\n",
            "epoch 5566, loss 0.50390, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 5567, loss 0.48927, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5568, loss 0.49412, training accuracy 77.53%, test accuracy 52.87%\n",
            "epoch 5569, loss 0.48913, training accuracy 78.53%, test accuracy 52.48%\n",
            "epoch 5570, loss 0.48500, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5571, loss 0.49256, training accuracy 77.93%, test accuracy 50.69%\n",
            "epoch 5572, loss 0.48269, training accuracy 77.34%, test accuracy 51.29%\n",
            "epoch 5573, loss 0.48793, training accuracy 77.73%, test accuracy 50.69%\n",
            "epoch 5574, loss 0.48402, training accuracy 78.33%, test accuracy 52.48%\n",
            "epoch 5575, loss 0.48665, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 5576, loss 0.48021, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5577, loss 0.48504, training accuracy 77.34%, test accuracy 51.49%\n",
            "epoch 5578, loss 0.47947, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5579, loss 0.48637, training accuracy 77.93%, test accuracy 51.09%\n",
            "epoch 5580, loss 0.47971, training accuracy 77.93%, test accuracy 50.89%\n",
            "epoch 5581, loss 0.48104, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 5582, loss 0.47999, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5583, loss 0.47977, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 5584, loss 0.47749, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 5585, loss 0.47807, training accuracy 78.33%, test accuracy 50.69%\n",
            "epoch 5586, loss 0.47755, training accuracy 78.13%, test accuracy 50.69%\n",
            "epoch 5587, loss 0.47473, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5588, loss 0.47666, training accuracy 78.93%, test accuracy 50.89%\n",
            "epoch 5589, loss 0.47431, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5590, loss 0.47554, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5591, loss 0.47373, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5592, loss 0.47453, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5593, loss 0.47349, training accuracy 78.53%, test accuracy 51.49%\n",
            "epoch 5594, loss 0.47317, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 5595, loss 0.47332, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5596, loss 0.47257, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 5597, loss 0.47300, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5598, loss 0.47166, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5599, loss 0.47265, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 5600, loss 0.47124, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5601, loss 0.47213, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5602, loss 0.47129, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 5603, loss 0.47171, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5604, loss 0.47094, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5605, loss 0.47133, training accuracy 78.53%, test accuracy 51.09%\n",
            "epoch 5606, loss 0.47099, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5607, loss 0.47106, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5608, loss 0.47065, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5609, loss 0.47083, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5610, loss 0.47055, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 5611, loss 0.47071, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5612, loss 0.47030, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5613, loss 0.47062, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5614, loss 0.47023, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5615, loss 0.47040, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5616, loss 0.47015, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5617, loss 0.47030, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5618, loss 0.47002, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5619, loss 0.47016, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5620, loss 0.46999, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5621, loss 0.46999, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5622, loss 0.46990, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5623, loss 0.46993, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5624, loss 0.46983, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5625, loss 0.46980, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5626, loss 0.46978, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5627, loss 0.46973, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5628, loss 0.46969, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5629, loss 0.46966, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5630, loss 0.46963, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5631, loss 0.46958, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5632, loss 0.46956, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5633, loss 0.46953, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5634, loss 0.46949, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5635, loss 0.46946, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5636, loss 0.46944, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5637, loss 0.46940, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5638, loss 0.46937, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5639, loss 0.46934, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5640, loss 0.46932, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5641, loss 0.46928, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5642, loss 0.46927, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5643, loss 0.46923, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5644, loss 0.46921, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5645, loss 0.46918, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5646, loss 0.46916, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5647, loss 0.46913, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5648, loss 0.46911, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5649, loss 0.46908, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5650, loss 0.46906, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5651, loss 0.46903, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5652, loss 0.46901, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5653, loss 0.46898, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5654, loss 0.46896, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5655, loss 0.46894, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5656, loss 0.46892, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5657, loss 0.46889, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5658, loss 0.46887, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5659, loss 0.46885, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5660, loss 0.46882, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5661, loss 0.46880, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5662, loss 0.46878, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5663, loss 0.46876, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5664, loss 0.46874, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5665, loss 0.46872, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5666, loss 0.46869, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5667, loss 0.46867, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5668, loss 0.46865, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5669, loss 0.46863, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5670, loss 0.46861, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5671, loss 0.46859, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5672, loss 0.46857, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5673, loss 0.46855, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5674, loss 0.46853, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5675, loss 0.46851, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5676, loss 0.46848, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5677, loss 0.46846, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5678, loss 0.46844, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5679, loss 0.46842, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5680, loss 0.46840, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5681, loss 0.46838, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5682, loss 0.46836, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5683, loss 0.46834, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5684, loss 0.46832, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5685, loss 0.46830, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5686, loss 0.46829, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5687, loss 0.46827, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5688, loss 0.46825, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5689, loss 0.46823, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5690, loss 0.46821, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5691, loss 0.46819, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5692, loss 0.46817, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5693, loss 0.46815, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5694, loss 0.46813, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5695, loss 0.46811, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5696, loss 0.46809, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5697, loss 0.46807, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5698, loss 0.46806, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5699, loss 0.46804, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5700, loss 0.46802, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5701, loss 0.46800, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5702, loss 0.46798, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5703, loss 0.46796, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5704, loss 0.46795, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5705, loss 0.46793, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5706, loss 0.46791, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5707, loss 0.46789, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5708, loss 0.46787, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5709, loss 0.46785, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5710, loss 0.46784, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5711, loss 0.46782, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5712, loss 0.46780, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5713, loss 0.46778, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5714, loss 0.46776, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5715, loss 0.46775, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5716, loss 0.46773, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5717, loss 0.46771, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5718, loss 0.46769, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5719, loss 0.46767, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5720, loss 0.46766, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5721, loss 0.46764, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5722, loss 0.46762, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5723, loss 0.46760, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5724, loss 0.46759, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5725, loss 0.46757, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5726, loss 0.46755, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5727, loss 0.46753, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5728, loss 0.46752, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5729, loss 0.46750, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5730, loss 0.46748, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5731, loss 0.46746, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5732, loss 0.46745, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5733, loss 0.46743, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5734, loss 0.46741, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5735, loss 0.46740, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5736, loss 0.46738, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5737, loss 0.46736, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5738, loss 0.46734, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5739, loss 0.46733, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5740, loss 0.46731, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5741, loss 0.46729, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5742, loss 0.46728, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5743, loss 0.46726, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5744, loss 0.46724, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5745, loss 0.46722, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5746, loss 0.46721, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5747, loss 0.46719, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5748, loss 0.46717, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5749, loss 0.46716, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5750, loss 0.46714, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5751, loss 0.46712, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5752, loss 0.46711, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5753, loss 0.46709, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5754, loss 0.46707, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5755, loss 0.46706, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5756, loss 0.46704, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5757, loss 0.46702, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5758, loss 0.46701, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5759, loss 0.46699, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5760, loss 0.46697, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5761, loss 0.46696, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5762, loss 0.46694, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5763, loss 0.46692, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5764, loss 0.46691, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5765, loss 0.46689, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5766, loss 0.46687, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5767, loss 0.46686, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5768, loss 0.46684, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5769, loss 0.46682, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5770, loss 0.46681, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5771, loss 0.46679, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5772, loss 0.46677, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5773, loss 0.46676, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5774, loss 0.46674, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5775, loss 0.46672, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5776, loss 0.46671, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5777, loss 0.46669, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5778, loss 0.46668, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5779, loss 0.46666, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5780, loss 0.46664, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5781, loss 0.46663, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5782, loss 0.46661, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5783, loss 0.46659, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5784, loss 0.46658, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5785, loss 0.46656, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5786, loss 0.46654, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5787, loss 0.46653, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5788, loss 0.46651, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5789, loss 0.46649, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5790, loss 0.46648, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5791, loss 0.46646, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5792, loss 0.46645, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5793, loss 0.46643, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5794, loss 0.46641, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5795, loss 0.46640, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5796, loss 0.46638, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5797, loss 0.46636, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5798, loss 0.46635, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5799, loss 0.46633, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5800, loss 0.46632, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5801, loss 0.46630, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5802, loss 0.46628, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5803, loss 0.46627, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5804, loss 0.46625, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5805, loss 0.46623, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5806, loss 0.46622, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5807, loss 0.46620, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5808, loss 0.46619, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5809, loss 0.46617, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5810, loss 0.46615, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5811, loss 0.46614, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5812, loss 0.46612, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5813, loss 0.46610, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5814, loss 0.46609, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5815, loss 0.46607, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5816, loss 0.46606, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5817, loss 0.46604, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5818, loss 0.46602, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5819, loss 0.46601, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5820, loss 0.46599, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5821, loss 0.46598, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5822, loss 0.46596, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5823, loss 0.46594, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5824, loss 0.46593, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5825, loss 0.46591, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5826, loss 0.46589, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5827, loss 0.46588, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5828, loss 0.46586, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5829, loss 0.46585, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5830, loss 0.46583, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5831, loss 0.46581, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5832, loss 0.46580, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5833, loss 0.46578, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5834, loss 0.46577, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5835, loss 0.46575, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5836, loss 0.46573, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5837, loss 0.46572, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5838, loss 0.46570, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5839, loss 0.46568, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5840, loss 0.46567, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5841, loss 0.46565, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5842, loss 0.46564, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5843, loss 0.46562, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5844, loss 0.46560, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5845, loss 0.46559, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5846, loss 0.46557, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5847, loss 0.46556, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5848, loss 0.46554, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5849, loss 0.46552, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5850, loss 0.46551, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5851, loss 0.46549, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5852, loss 0.46547, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5853, loss 0.46546, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5854, loss 0.46544, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5855, loss 0.46543, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5856, loss 0.46541, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5857, loss 0.46539, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5858, loss 0.46538, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5859, loss 0.46536, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5860, loss 0.46535, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5861, loss 0.46533, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5862, loss 0.46531, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5863, loss 0.46530, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5864, loss 0.46528, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5865, loss 0.46527, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5866, loss 0.46525, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5867, loss 0.46523, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5868, loss 0.46522, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5869, loss 0.46520, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5870, loss 0.46519, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5871, loss 0.46517, training accuracy 79.32%, test accuracy 50.50%\n",
            "epoch 5872, loss 0.46515, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5873, loss 0.46514, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5874, loss 0.46513, training accuracy 79.52%, test accuracy 50.50%\n",
            "epoch 5875, loss 0.46511, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5876, loss 0.46511, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5877, loss 0.46511, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 5878, loss 0.46513, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5879, loss 0.46517, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5880, loss 0.46528, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 5881, loss 0.46549, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5882, loss 0.46595, training accuracy 79.92%, test accuracy 51.49%\n",
            "epoch 5883, loss 0.46678, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5884, loss 0.46864, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 5885, loss 0.47161, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 5886, loss 0.47859, training accuracy 78.53%, test accuracy 49.90%\n",
            "epoch 5887, loss 0.48621, training accuracy 78.73%, test accuracy 52.87%\n",
            "epoch 5888, loss 0.50530, training accuracy 77.53%, test accuracy 47.72%\n",
            "epoch 5889, loss 0.50876, training accuracy 76.94%, test accuracy 53.27%\n",
            "epoch 5890, loss 0.53160, training accuracy 77.14%, test accuracy 47.52%\n",
            "epoch 5891, loss 0.51749, training accuracy 75.15%, test accuracy 52.67%\n",
            "epoch 5892, loss 0.51658, training accuracy 76.94%, test accuracy 50.10%\n",
            "epoch 5893, loss 0.48975, training accuracy 77.53%, test accuracy 51.88%\n",
            "epoch 5894, loss 0.47325, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5895, loss 0.46773, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5896, loss 0.47758, training accuracy 78.53%, test accuracy 53.27%\n",
            "epoch 5897, loss 0.49693, training accuracy 77.93%, test accuracy 49.90%\n",
            "epoch 5898, loss 0.49073, training accuracy 77.73%, test accuracy 52.48%\n",
            "epoch 5899, loss 0.48544, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5900, loss 0.47533, training accuracy 78.93%, test accuracy 51.09%\n",
            "epoch 5901, loss 0.46670, training accuracy 78.73%, test accuracy 50.89%\n",
            "epoch 5902, loss 0.46980, training accuracy 78.33%, test accuracy 51.29%\n",
            "epoch 5903, loss 0.47517, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 5904, loss 0.47891, training accuracy 78.53%, test accuracy 50.69%\n",
            "epoch 5905, loss 0.47875, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 5906, loss 0.47105, training accuracy 80.12%, test accuracy 51.09%\n",
            "epoch 5907, loss 0.46751, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 5908, loss 0.46728, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 5909, loss 0.47132, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5910, loss 0.47431, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 5911, loss 0.47257, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5912, loss 0.46974, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5913, loss 0.46609, training accuracy 79.92%, test accuracy 50.89%\n",
            "epoch 5914, loss 0.46603, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5915, loss 0.46809, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 5916, loss 0.47007, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 5917, loss 0.46979, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5918, loss 0.46789, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5919, loss 0.46575, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 5920, loss 0.46541, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5921, loss 0.46558, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5922, loss 0.46679, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5923, loss 0.46739, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5924, loss 0.46646, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5925, loss 0.46574, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5926, loss 0.46471, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5927, loss 0.46509, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5928, loss 0.46534, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5929, loss 0.46578, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 5930, loss 0.46606, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 5931, loss 0.46540, training accuracy 78.73%, test accuracy 50.69%\n",
            "epoch 5932, loss 0.46510, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5933, loss 0.46458, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5934, loss 0.46463, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5935, loss 0.46480, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5936, loss 0.46495, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 5937, loss 0.46514, training accuracy 79.92%, test accuracy 51.29%\n",
            "epoch 5938, loss 0.46496, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5939, loss 0.46477, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5940, loss 0.46455, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 5941, loss 0.46438, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5942, loss 0.46440, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5943, loss 0.46444, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5944, loss 0.46453, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5945, loss 0.46460, training accuracy 79.13%, test accuracy 50.89%\n",
            "epoch 5946, loss 0.46454, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5947, loss 0.46450, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5948, loss 0.46437, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5949, loss 0.46427, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5950, loss 0.46423, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5951, loss 0.46417, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 5952, loss 0.46420, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5953, loss 0.46420, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5954, loss 0.46422, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5955, loss 0.46423, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5956, loss 0.46421, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5957, loss 0.46419, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5958, loss 0.46414, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5959, loss 0.46410, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 5960, loss 0.46406, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 5961, loss 0.46402, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 5962, loss 0.46399, training accuracy 79.52%, test accuracy 50.69%\n",
            "epoch 5963, loss 0.46396, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 5964, loss 0.46394, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5965, loss 0.46393, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 5966, loss 0.46392, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 5967, loss 0.46391, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 5968, loss 0.46391, training accuracy 79.92%, test accuracy 51.09%\n",
            "epoch 5969, loss 0.46389, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 5970, loss 0.46389, training accuracy 79.92%, test accuracy 51.09%\n",
            "epoch 5971, loss 0.46388, training accuracy 79.32%, test accuracy 50.69%\n",
            "epoch 5972, loss 0.46387, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 5973, loss 0.46386, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5974, loss 0.46386, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5975, loss 0.46386, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5976, loss 0.46386, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 5977, loss 0.46386, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5978, loss 0.46388, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 5979, loss 0.46390, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 5980, loss 0.46395, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 5981, loss 0.46402, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 5982, loss 0.46414, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 5983, loss 0.46429, training accuracy 79.13%, test accuracy 51.09%\n",
            "epoch 5984, loss 0.46457, training accuracy 79.92%, test accuracy 51.49%\n",
            "epoch 5985, loss 0.46491, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 5986, loss 0.46554, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 5987, loss 0.46628, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 5988, loss 0.46769, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 5989, loss 0.46914, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 5990, loss 0.47210, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 5991, loss 0.47428, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 5992, loss 0.47920, training accuracy 78.53%, test accuracy 50.30%\n",
            "epoch 5993, loss 0.48038, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 5994, loss 0.48492, training accuracy 78.13%, test accuracy 49.90%\n",
            "epoch 5995, loss 0.48188, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 5996, loss 0.48113, training accuracy 78.33%, test accuracy 51.29%\n",
            "epoch 5997, loss 0.47444, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 5998, loss 0.46979, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 5999, loss 0.46550, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 6000, loss 0.46364, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6001, loss 0.46378, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6002, loss 0.46534, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6003, loss 0.46798, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6004, loss 0.47039, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6005, loss 0.47357, training accuracy 79.32%, test accuracy 50.89%\n",
            "epoch 6006, loss 0.47393, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 6007, loss 0.47475, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6008, loss 0.47182, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6009, loss 0.46928, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6010, loss 0.46600, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6011, loss 0.46403, training accuracy 79.52%, test accuracy 50.89%\n",
            "epoch 6012, loss 0.46334, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6013, loss 0.46381, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6014, loss 0.46506, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6015, loss 0.46642, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6016, loss 0.46797, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6017, loss 0.46854, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6018, loss 0.46922, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6019, loss 0.46844, training accuracy 80.32%, test accuracy 51.68%\n",
            "epoch 6020, loss 0.46776, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6021, loss 0.46624, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6022, loss 0.46504, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6023, loss 0.46395, training accuracy 78.93%, test accuracy 50.89%\n",
            "epoch 6024, loss 0.46332, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 6025, loss 0.46310, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6026, loss 0.46323, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 6027, loss 0.46359, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6028, loss 0.46404, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 6029, loss 0.46458, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6030, loss 0.46495, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6031, loss 0.46539, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6032, loss 0.46552, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6033, loss 0.46576, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6034, loss 0.46563, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6035, loss 0.46566, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6036, loss 0.46534, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6037, loss 0.46519, training accuracy 80.12%, test accuracy 51.49%\n",
            "epoch 6038, loss 0.46482, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6039, loss 0.46458, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6040, loss 0.46423, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 6041, loss 0.46401, training accuracy 79.92%, test accuracy 51.49%\n",
            "epoch 6042, loss 0.46375, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 6043, loss 0.46358, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6044, loss 0.46342, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6045, loss 0.46334, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 6046, loss 0.46326, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6047, loss 0.46323, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 6048, loss 0.46322, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6049, loss 0.46325, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 6050, loss 0.46330, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6051, loss 0.46343, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6052, loss 0.46358, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 6053, loss 0.46386, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6054, loss 0.46420, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 6055, loss 0.46482, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6056, loss 0.46552, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6057, loss 0.46688, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6058, loss 0.46822, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6059, loss 0.47098, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 6060, loss 0.47309, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6061, loss 0.47781, training accuracy 79.13%, test accuracy 49.90%\n",
            "epoch 6062, loss 0.47953, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 6063, loss 0.48479, training accuracy 78.13%, test accuracy 49.70%\n",
            "epoch 6064, loss 0.48311, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 6065, loss 0.48408, training accuracy 78.13%, test accuracy 50.69%\n",
            "epoch 6066, loss 0.47749, training accuracy 78.53%, test accuracy 52.28%\n",
            "epoch 6067, loss 0.47248, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6068, loss 0.46633, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 6069, loss 0.46315, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6070, loss 0.46267, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6071, loss 0.46426, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6072, loss 0.46731, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6073, loss 0.47036, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6074, loss 0.47440, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 6075, loss 0.47551, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 6076, loss 0.47717, training accuracy 78.93%, test accuracy 50.89%\n",
            "epoch 6077, loss 0.47374, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 6078, loss 0.47040, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6079, loss 0.46565, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6080, loss 0.46300, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6081, loss 0.46261, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6082, loss 0.46397, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 6083, loss 0.46643, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6084, loss 0.46835, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6085, loss 0.47028, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6086, loss 0.47004, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 6087, loss 0.46977, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 6088, loss 0.46746, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6089, loss 0.46546, training accuracy 79.92%, test accuracy 51.49%\n",
            "epoch 6090, loss 0.46349, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 6091, loss 0.46242, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6092, loss 0.46226, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6093, loss 0.46282, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6094, loss 0.46378, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6095, loss 0.46459, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6096, loss 0.46538, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6097, loss 0.46551, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6098, loss 0.46549, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6099, loss 0.46483, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6100, loss 0.46420, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6101, loss 0.46337, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 6102, loss 0.46274, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6103, loss 0.46227, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6104, loss 0.46203, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6105, loss 0.46198, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6106, loss 0.46207, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6107, loss 0.46226, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6108, loss 0.46249, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 6109, loss 0.46277, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6110, loss 0.46299, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 6111, loss 0.46325, training accuracy 79.92%, test accuracy 51.68%\n",
            "epoch 6112, loss 0.46340, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6113, loss 0.46364, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6114, loss 0.46374, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 6115, loss 0.46396, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6116, loss 0.46402, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6117, loss 0.46426, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6118, loss 0.46432, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6119, loss 0.46461, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6120, loss 0.46467, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6121, loss 0.46504, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6122, loss 0.46514, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6123, loss 0.46563, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6124, loss 0.46579, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6125, loss 0.46647, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6126, loss 0.46667, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6127, loss 0.46753, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6128, loss 0.46771, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6129, loss 0.46864, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6130, loss 0.46854, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 6131, loss 0.46924, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6132, loss 0.46869, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6133, loss 0.46890, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6134, loss 0.46792, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6135, loss 0.46758, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6136, loss 0.46637, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6137, loss 0.46569, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6138, loss 0.46463, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6139, loss 0.46395, training accuracy 79.92%, test accuracy 51.68%\n",
            "epoch 6140, loss 0.46321, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6141, loss 0.46274, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 6142, loss 0.46231, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 6143, loss 0.46203, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6144, loss 0.46182, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 6145, loss 0.46167, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6146, loss 0.46157, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6147, loss 0.46150, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6148, loss 0.46145, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6149, loss 0.46141, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6150, loss 0.46139, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6151, loss 0.46137, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6152, loss 0.46136, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6153, loss 0.46137, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6154, loss 0.46138, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 6155, loss 0.46143, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6156, loss 0.46150, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6157, loss 0.46164, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6158, loss 0.46186, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6159, loss 0.46226, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 6160, loss 0.46286, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 6161, loss 0.46400, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6162, loss 0.46557, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6163, loss 0.46873, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6164, loss 0.47243, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 6165, loss 0.48051, training accuracy 78.53%, test accuracy 50.10%\n",
            "epoch 6166, loss 0.48733, training accuracy 78.13%, test accuracy 53.66%\n",
            "epoch 6167, loss 0.50604, training accuracy 77.14%, test accuracy 48.12%\n",
            "epoch 6168, loss 0.52363, training accuracy 76.14%, test accuracy 54.06%\n",
            "epoch 6169, loss 0.57805, training accuracy 76.14%, test accuracy 46.53%\n",
            "epoch 6170, loss 0.54297, training accuracy 72.17%, test accuracy 53.47%\n",
            "epoch 6171, loss 0.52047, training accuracy 77.53%, test accuracy 50.89%\n",
            "epoch 6172, loss 0.46782, training accuracy 79.13%, test accuracy 49.90%\n",
            "epoch 6173, loss 0.48678, training accuracy 77.53%, test accuracy 53.86%\n",
            "epoch 6174, loss 0.54623, training accuracy 76.74%, test accuracy 49.70%\n",
            "epoch 6175, loss 0.54125, training accuracy 75.35%, test accuracy 53.27%\n",
            "epoch 6176, loss 0.55767, training accuracy 76.94%, test accuracy 49.90%\n",
            "epoch 6177, loss 0.52541, training accuracy 77.53%, test accuracy 53.66%\n",
            "epoch 6178, loss 0.51241, training accuracy 77.73%, test accuracy 53.07%\n",
            "epoch 6179, loss 0.53801, training accuracy 76.34%, test accuracy 48.12%\n",
            "epoch 6180, loss 0.56185, training accuracy 72.96%, test accuracy 54.65%\n",
            "epoch 6181, loss 0.60870, training accuracy 75.75%, test accuracy 51.09%\n",
            "epoch 6182, loss 0.53662, training accuracy 77.53%, test accuracy 51.68%\n",
            "epoch 6183, loss 0.50909, training accuracy 77.14%, test accuracy 52.67%\n",
            "epoch 6184, loss 0.54391, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 6185, loss 0.55984, training accuracy 77.53%, test accuracy 54.26%\n",
            "epoch 6186, loss 0.55862, training accuracy 75.15%, test accuracy 51.88%\n",
            "epoch 6187, loss 0.51797, training accuracy 78.33%, test accuracy 50.10%\n",
            "epoch 6188, loss 0.52084, training accuracy 77.14%, test accuracy 53.47%\n",
            "epoch 6189, loss 0.53396, training accuracy 77.14%, test accuracy 52.67%\n",
            "epoch 6190, loss 0.50659, training accuracy 76.94%, test accuracy 51.88%\n",
            "epoch 6191, loss 0.51110, training accuracy 77.53%, test accuracy 53.27%\n",
            "epoch 6192, loss 0.51107, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 6193, loss 0.50758, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 6194, loss 0.47735, training accuracy 78.93%, test accuracy 52.87%\n",
            "epoch 6195, loss 0.50793, training accuracy 78.73%, test accuracy 48.91%\n",
            "epoch 6196, loss 0.49698, training accuracy 77.73%, test accuracy 51.68%\n",
            "epoch 6197, loss 0.49346, training accuracy 77.93%, test accuracy 52.28%\n",
            "epoch 6198, loss 0.49910, training accuracy 78.33%, test accuracy 52.28%\n",
            "epoch 6199, loss 0.50523, training accuracy 78.73%, test accuracy 52.87%\n",
            "epoch 6200, loss 0.47705, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 6201, loss 0.50424, training accuracy 77.73%, test accuracy 51.88%\n",
            "epoch 6202, loss 0.47483, training accuracy 79.52%, test accuracy 53.47%\n",
            "epoch 6203, loss 0.50263, training accuracy 78.33%, test accuracy 51.49%\n",
            "epoch 6204, loss 0.47631, training accuracy 79.32%, test accuracy 53.47%\n",
            "epoch 6205, loss 0.49504, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6206, loss 0.47034, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6207, loss 0.48899, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6208, loss 0.47364, training accuracy 80.52%, test accuracy 50.89%\n",
            "epoch 6209, loss 0.47916, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 6210, loss 0.46928, training accuracy 78.73%, test accuracy 52.67%\n",
            "epoch 6211, loss 0.47661, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6212, loss 0.46881, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6213, loss 0.47506, training accuracy 79.13%, test accuracy 52.67%\n",
            "epoch 6214, loss 0.46575, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6215, loss 0.47400, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6216, loss 0.46694, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6217, loss 0.46907, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6218, loss 0.46593, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6219, loss 0.47062, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 6220, loss 0.46374, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6221, loss 0.46955, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6222, loss 0.46445, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6223, loss 0.46765, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6224, loss 0.46366, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6225, loss 0.46708, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6226, loss 0.46313, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6227, loss 0.46501, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 6228, loss 0.46397, training accuracy 80.32%, test accuracy 51.68%\n",
            "epoch 6229, loss 0.46447, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6230, loss 0.46323, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 6231, loss 0.46380, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6232, loss 0.46396, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6233, loss 0.46316, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6234, loss 0.46343, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6235, loss 0.46292, training accuracy 78.93%, test accuracy 51.88%\n",
            "epoch 6236, loss 0.46371, training accuracy 80.12%, test accuracy 51.49%\n",
            "epoch 6237, loss 0.46227, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6238, loss 0.46338, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6239, loss 0.46223, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6240, loss 0.46347, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 6241, loss 0.46200, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6242, loss 0.46330, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6243, loss 0.46214, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6244, loss 0.46334, training accuracy 80.12%, test accuracy 51.68%\n",
            "epoch 6245, loss 0.46181, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6246, loss 0.46299, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6247, loss 0.46196, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6248, loss 0.46292, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6249, loss 0.46175, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6250, loss 0.46264, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6251, loss 0.46184, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6252, loss 0.46256, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6253, loss 0.46152, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6254, loss 0.46238, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 6255, loss 0.46143, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 6256, loss 0.46226, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 6257, loss 0.46123, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 6258, loss 0.46207, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6259, loss 0.46112, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 6260, loss 0.46192, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6261, loss 0.46102, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6262, loss 0.46169, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6263, loss 0.46094, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 6264, loss 0.46152, training accuracy 78.93%, test accuracy 51.88%\n",
            "epoch 6265, loss 0.46090, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6266, loss 0.46134, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6267, loss 0.46085, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6268, loss 0.46118, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6269, loss 0.46083, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 6270, loss 0.46105, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6271, loss 0.46078, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6272, loss 0.46093, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6273, loss 0.46075, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6274, loss 0.46084, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6275, loss 0.46070, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6276, loss 0.46075, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6277, loss 0.46065, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 6278, loss 0.46069, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6279, loss 0.46060, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6280, loss 0.46063, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6281, loss 0.46054, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6282, loss 0.46058, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6283, loss 0.46050, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6284, loss 0.46053, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6285, loss 0.46045, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6286, loss 0.46048, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6287, loss 0.46041, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6288, loss 0.46043, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6289, loss 0.46037, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6290, loss 0.46038, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6291, loss 0.46033, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6292, loss 0.46033, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6293, loss 0.46030, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6294, loss 0.46029, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6295, loss 0.46026, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6296, loss 0.46025, training accuracy 79.32%, test accuracy 51.68%\n",
            "epoch 6297, loss 0.46022, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6298, loss 0.46021, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6299, loss 0.46018, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6300, loss 0.46017, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6301, loss 0.46014, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6302, loss 0.46013, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6303, loss 0.46011, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6304, loss 0.46010, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6305, loss 0.46007, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6306, loss 0.46006, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6307, loss 0.46004, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6308, loss 0.46002, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6309, loss 0.46000, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6310, loss 0.45999, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6311, loss 0.45997, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6312, loss 0.45995, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6313, loss 0.45993, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6314, loss 0.45992, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6315, loss 0.45990, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6316, loss 0.45988, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6317, loss 0.45986, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6318, loss 0.45985, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6319, loss 0.45983, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6320, loss 0.45981, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6321, loss 0.45980, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6322, loss 0.45978, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6323, loss 0.45976, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6324, loss 0.45975, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6325, loss 0.45973, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6326, loss 0.45971, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6327, loss 0.45970, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6328, loss 0.45968, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6329, loss 0.45967, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6330, loss 0.45965, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6331, loss 0.45963, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6332, loss 0.45962, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6333, loss 0.45960, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6334, loss 0.45959, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6335, loss 0.45957, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6336, loss 0.45955, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6337, loss 0.45954, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6338, loss 0.45952, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6339, loss 0.45951, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6340, loss 0.45949, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6341, loss 0.45947, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6342, loss 0.45946, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6343, loss 0.45944, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6344, loss 0.45943, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6345, loss 0.45941, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6346, loss 0.45940, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6347, loss 0.45938, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6348, loss 0.45937, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6349, loss 0.45935, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6350, loss 0.45933, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6351, loss 0.45932, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6352, loss 0.45930, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6353, loss 0.45929, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6354, loss 0.45927, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6355, loss 0.45926, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6356, loss 0.45924, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6357, loss 0.45923, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6358, loss 0.45921, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6359, loss 0.45920, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6360, loss 0.45918, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6361, loss 0.45917, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6362, loss 0.45915, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6363, loss 0.45914, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6364, loss 0.45912, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6365, loss 0.45911, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 6366, loss 0.45909, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6367, loss 0.45908, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6368, loss 0.45906, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6369, loss 0.45905, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6370, loss 0.45903, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6371, loss 0.45902, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6372, loss 0.45900, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6373, loss 0.45899, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6374, loss 0.45897, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6375, loss 0.45896, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6376, loss 0.45894, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6377, loss 0.45893, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6378, loss 0.45891, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6379, loss 0.45890, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6380, loss 0.45888, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6381, loss 0.45887, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6382, loss 0.45885, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6383, loss 0.45884, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6384, loss 0.45882, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6385, loss 0.45881, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6386, loss 0.45879, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6387, loss 0.45878, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6388, loss 0.45876, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6389, loss 0.45875, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6390, loss 0.45873, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6391, loss 0.45872, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6392, loss 0.45870, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6393, loss 0.45869, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6394, loss 0.45868, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6395, loss 0.45866, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6396, loss 0.45865, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6397, loss 0.45863, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6398, loss 0.45862, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6399, loss 0.45860, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6400, loss 0.45859, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6401, loss 0.45857, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6402, loss 0.45856, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6403, loss 0.45854, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6404, loss 0.45853, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6405, loss 0.45851, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6406, loss 0.45850, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6407, loss 0.45849, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6408, loss 0.45847, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6409, loss 0.45846, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6410, loss 0.45844, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6411, loss 0.45843, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6412, loss 0.45841, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6413, loss 0.45840, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6414, loss 0.45839, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6415, loss 0.45838, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6416, loss 0.45837, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6417, loss 0.45838, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6418, loss 0.45839, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6419, loss 0.45845, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6420, loss 0.45856, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6421, loss 0.45882, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6422, loss 0.45933, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6423, loss 0.46045, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 6424, loss 0.46246, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 6425, loss 0.46719, training accuracy 79.52%, test accuracy 50.30%\n",
            "epoch 6426, loss 0.47391, training accuracy 78.73%, test accuracy 53.27%\n",
            "epoch 6427, loss 0.49057, training accuracy 77.73%, test accuracy 48.71%\n",
            "epoch 6428, loss 0.50118, training accuracy 77.34%, test accuracy 53.47%\n",
            "epoch 6429, loss 0.53549, training accuracy 77.34%, test accuracy 46.93%\n",
            "epoch 6430, loss 0.53416, training accuracy 73.56%, test accuracy 53.66%\n",
            "epoch 6431, loss 0.57126, training accuracy 75.94%, test accuracy 47.13%\n",
            "epoch 6432, loss 0.51871, training accuracy 73.76%, test accuracy 53.07%\n",
            "epoch 6433, loss 0.48900, training accuracy 76.94%, test accuracy 52.08%\n",
            "epoch 6434, loss 0.47000, training accuracy 78.33%, test accuracy 47.72%\n",
            "epoch 6435, loss 0.50879, training accuracy 74.35%, test accuracy 54.65%\n",
            "epoch 6436, loss 0.57447, training accuracy 76.34%, test accuracy 51.49%\n",
            "epoch 6437, loss 0.51244, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 6438, loss 0.48075, training accuracy 78.73%, test accuracy 54.06%\n",
            "epoch 6439, loss 0.49447, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 6440, loss 0.51809, training accuracy 77.73%, test accuracy 54.26%\n",
            "epoch 6441, loss 0.52176, training accuracy 77.53%, test accuracy 52.67%\n",
            "epoch 6442, loss 0.50639, training accuracy 77.93%, test accuracy 53.27%\n",
            "epoch 6443, loss 0.49275, training accuracy 78.93%, test accuracy 53.66%\n",
            "epoch 6444, loss 0.48237, training accuracy 79.13%, test accuracy 49.90%\n",
            "epoch 6445, loss 0.51128, training accuracy 76.54%, test accuracy 53.86%\n",
            "epoch 6446, loss 0.49619, training accuracy 79.13%, test accuracy 53.27%\n",
            "epoch 6447, loss 0.51982, training accuracy 78.73%, test accuracy 52.48%\n",
            "epoch 6448, loss 0.47517, training accuracy 79.92%, test accuracy 53.47%\n",
            "epoch 6449, loss 0.52059, training accuracy 78.53%, test accuracy 53.07%\n",
            "epoch 6450, loss 0.47389, training accuracy 78.73%, test accuracy 53.27%\n",
            "epoch 6451, loss 0.49794, training accuracy 78.13%, test accuracy 54.46%\n",
            "epoch 6452, loss 0.48809, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 6453, loss 0.47823, training accuracy 78.73%, test accuracy 52.08%\n",
            "epoch 6454, loss 0.48682, training accuracy 79.32%, test accuracy 53.27%\n",
            "epoch 6455, loss 0.47545, training accuracy 78.73%, test accuracy 53.27%\n",
            "epoch 6456, loss 0.48781, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 6457, loss 0.46755, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6458, loss 0.49378, training accuracy 77.53%, test accuracy 53.07%\n",
            "epoch 6459, loss 0.47621, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6460, loss 0.49575, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 6461, loss 0.46544, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6462, loss 0.49541, training accuracy 77.93%, test accuracy 52.28%\n",
            "epoch 6463, loss 0.46873, training accuracy 79.52%, test accuracy 53.27%\n",
            "epoch 6464, loss 0.48483, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 6465, loss 0.46653, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6466, loss 0.47850, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 6467, loss 0.46311, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6468, loss 0.47188, training accuracy 78.93%, test accuracy 52.67%\n",
            "epoch 6469, loss 0.46679, training accuracy 78.73%, test accuracy 52.28%\n",
            "epoch 6470, loss 0.46508, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6471, loss 0.46846, training accuracy 78.33%, test accuracy 52.87%\n",
            "epoch 6472, loss 0.46336, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 6473, loss 0.46838, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6474, loss 0.46182, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6475, loss 0.47178, training accuracy 78.53%, test accuracy 53.07%\n",
            "epoch 6476, loss 0.47551, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 6477, loss 0.47891, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6478, loss 0.46364, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6479, loss 0.47951, training accuracy 78.53%, test accuracy 52.67%\n",
            "epoch 6480, loss 0.46493, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6481, loss 0.47917, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6482, loss 0.46074, training accuracy 80.12%, test accuracy 51.49%\n",
            "epoch 6483, loss 0.47982, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6484, loss 0.46287, training accuracy 80.12%, test accuracy 53.47%\n",
            "epoch 6485, loss 0.47830, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6486, loss 0.46344, training accuracy 79.13%, test accuracy 51.88%\n",
            "epoch 6487, loss 0.46944, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 6488, loss 0.46150, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 6489, loss 0.46404, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 6490, loss 0.46246, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6491, loss 0.46109, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6492, loss 0.46316, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6493, loss 0.46097, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6494, loss 0.46441, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6495, loss 0.45990, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6496, loss 0.46586, training accuracy 78.73%, test accuracy 52.67%\n",
            "epoch 6497, loss 0.46315, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6498, loss 0.46509, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6499, loss 0.46107, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 6500, loss 0.46688, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 6501, loss 0.46284, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6502, loss 0.46809, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6503, loss 0.46016, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 6504, loss 0.47197, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 6505, loss 0.46264, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6506, loss 0.47305, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6507, loss 0.45968, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 6508, loss 0.47322, training accuracy 78.53%, test accuracy 52.67%\n",
            "epoch 6509, loss 0.46034, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6510, loss 0.47298, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6511, loss 0.45928, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6512, loss 0.46991, training accuracy 78.93%, test accuracy 52.08%\n",
            "epoch 6513, loss 0.45916, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6514, loss 0.46792, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6515, loss 0.45887, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6516, loss 0.46587, training accuracy 78.93%, test accuracy 52.87%\n",
            "epoch 6517, loss 0.45854, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 6518, loss 0.46410, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 6519, loss 0.45855, training accuracy 79.52%, test accuracy 51.49%\n",
            "epoch 6520, loss 0.46230, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6521, loss 0.45851, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6522, loss 0.46046, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6523, loss 0.45926, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6524, loss 0.45907, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6525, loss 0.45975, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6526, loss 0.45849, training accuracy 79.13%, test accuracy 52.08%\n",
            "epoch 6527, loss 0.45995, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6528, loss 0.45807, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6529, loss 0.45968, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6530, loss 0.45810, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6531, loss 0.45900, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6532, loss 0.45835, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 6533, loss 0.45841, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6534, loss 0.45847, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6535, loss 0.45805, training accuracy 79.32%, test accuracy 53.07%\n",
            "epoch 6536, loss 0.45854, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6537, loss 0.45786, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6538, loss 0.45840, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6539, loss 0.45785, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 6540, loss 0.45815, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6541, loss 0.45790, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6542, loss 0.45793, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6543, loss 0.45792, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6544, loss 0.45777, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6545, loss 0.45791, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6546, loss 0.45766, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6547, loss 0.45784, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6548, loss 0.45762, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6549, loss 0.45773, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6550, loss 0.45760, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6551, loss 0.45764, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6552, loss 0.45758, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6553, loss 0.45755, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6554, loss 0.45755, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6555, loss 0.45748, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6556, loss 0.45750, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6557, loss 0.45743, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6558, loss 0.45746, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6559, loss 0.45739, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6560, loss 0.45741, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6561, loss 0.45735, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6562, loss 0.45736, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6563, loss 0.45731, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6564, loss 0.45731, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6565, loss 0.45728, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6566, loss 0.45726, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6567, loss 0.45725, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6568, loss 0.45722, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6569, loss 0.45721, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6570, loss 0.45717, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6571, loss 0.45717, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6572, loss 0.45714, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6573, loss 0.45714, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6574, loss 0.45710, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6575, loss 0.45710, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6576, loss 0.45707, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6577, loss 0.45706, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6578, loss 0.45704, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6579, loss 0.45702, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6580, loss 0.45701, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6581, loss 0.45699, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6582, loss 0.45697, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6583, loss 0.45696, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6584, loss 0.45694, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6585, loss 0.45692, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6586, loss 0.45691, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6587, loss 0.45689, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6588, loss 0.45688, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6589, loss 0.45686, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6590, loss 0.45684, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6591, loss 0.45683, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6592, loss 0.45681, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6593, loss 0.45679, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6594, loss 0.45678, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6595, loss 0.45676, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6596, loss 0.45675, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6597, loss 0.45673, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6598, loss 0.45672, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6599, loss 0.45670, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6600, loss 0.45669, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6601, loss 0.45667, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6602, loss 0.45666, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6603, loss 0.45664, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6604, loss 0.45663, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6605, loss 0.45661, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6606, loss 0.45660, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6607, loss 0.45658, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6608, loss 0.45657, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6609, loss 0.45656, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6610, loss 0.45655, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6611, loss 0.45655, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6612, loss 0.45655, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6613, loss 0.45658, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6614, loss 0.45662, training accuracy 80.12%, test accuracy 51.68%\n",
            "epoch 6615, loss 0.45673, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6616, loss 0.45692, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6617, loss 0.45731, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6618, loss 0.45798, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6619, loss 0.45939, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6620, loss 0.46155, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 6621, loss 0.46641, training accuracy 79.92%, test accuracy 50.50%\n",
            "epoch 6622, loss 0.47192, training accuracy 79.13%, test accuracy 53.66%\n",
            "epoch 6623, loss 0.48510, training accuracy 78.13%, test accuracy 49.11%\n",
            "epoch 6624, loss 0.49003, training accuracy 78.13%, test accuracy 54.06%\n",
            "epoch 6625, loss 0.50694, training accuracy 77.73%, test accuracy 49.50%\n",
            "epoch 6626, loss 0.49566, training accuracy 77.34%, test accuracy 53.47%\n",
            "epoch 6627, loss 0.49241, training accuracy 77.53%, test accuracy 51.09%\n",
            "epoch 6628, loss 0.47349, training accuracy 78.53%, test accuracy 53.27%\n",
            "epoch 6629, loss 0.46194, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 6630, loss 0.45696, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6631, loss 0.46053, training accuracy 79.72%, test accuracy 53.47%\n",
            "epoch 6632, loss 0.47008, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 6633, loss 0.47473, training accuracy 78.73%, test accuracy 53.86%\n",
            "epoch 6634, loss 0.47889, training accuracy 78.93%, test accuracy 51.29%\n",
            "epoch 6635, loss 0.47187, training accuracy 78.73%, test accuracy 53.47%\n",
            "epoch 6636, loss 0.46514, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6637, loss 0.45836, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6638, loss 0.45661, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 6639, loss 0.45943, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 6640, loss 0.46309, training accuracy 79.92%, test accuracy 53.47%\n",
            "epoch 6641, loss 0.46631, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6642, loss 0.46470, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6643, loss 0.46184, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6644, loss 0.45813, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6645, loss 0.45642, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6646, loss 0.45716, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6647, loss 0.45900, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6648, loss 0.46089, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 6649, loss 0.46098, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6650, loss 0.46005, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6651, loss 0.45808, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6652, loss 0.45667, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6653, loss 0.45623, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6654, loss 0.45670, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6655, loss 0.45763, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6656, loss 0.45836, training accuracy 79.32%, test accuracy 53.27%\n",
            "epoch 6657, loss 0.45863, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 6658, loss 0.45812, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6659, loss 0.45744, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6660, loss 0.45663, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6661, loss 0.45615, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6662, loss 0.45607, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6663, loss 0.45632, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6664, loss 0.45670, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6665, loss 0.45700, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6666, loss 0.45720, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 6667, loss 0.45709, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6668, loss 0.45689, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6669, loss 0.45654, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6670, loss 0.45626, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6671, loss 0.45602, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6672, loss 0.45590, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6673, loss 0.45587, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 6674, loss 0.45592, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 6675, loss 0.45602, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6676, loss 0.45611, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 6677, loss 0.45621, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6678, loss 0.45625, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6679, loss 0.45628, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6680, loss 0.45627, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6681, loss 0.45624, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6682, loss 0.45619, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6683, loss 0.45615, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6684, loss 0.45609, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6685, loss 0.45605, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6686, loss 0.45600, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6687, loss 0.45597, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6688, loss 0.45594, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6689, loss 0.45592, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6690, loss 0.45591, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6691, loss 0.45593, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6692, loss 0.45595, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 6693, loss 0.45600, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6694, loss 0.45607, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6695, loss 0.45620, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6696, loss 0.45636, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6697, loss 0.45667, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 6698, loss 0.45703, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 6699, loss 0.45771, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 6700, loss 0.45848, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 6701, loss 0.45997, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6702, loss 0.46140, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 6703, loss 0.46443, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6704, loss 0.46652, training accuracy 79.13%, test accuracy 53.66%\n",
            "epoch 6705, loss 0.47144, training accuracy 79.72%, test accuracy 50.50%\n",
            "epoch 6706, loss 0.47257, training accuracy 78.73%, test accuracy 53.47%\n",
            "epoch 6707, loss 0.47726, training accuracy 78.73%, test accuracy 50.50%\n",
            "epoch 6708, loss 0.47420, training accuracy 78.73%, test accuracy 53.66%\n",
            "epoch 6709, loss 0.47378, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6710, loss 0.46724, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 6711, loss 0.46287, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6712, loss 0.45827, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6713, loss 0.45594, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 6714, loss 0.45544, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6715, loss 0.45641, training accuracy 78.93%, test accuracy 52.67%\n",
            "epoch 6716, loss 0.45841, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6717, loss 0.46050, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 6718, loss 0.46313, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6719, loss 0.46383, training accuracy 79.52%, test accuracy 53.27%\n",
            "epoch 6720, loss 0.46495, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6721, loss 0.46322, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 6722, loss 0.46183, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6723, loss 0.45915, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6724, loss 0.45722, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 6725, loss 0.45581, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6726, loss 0.45527, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6727, loss 0.45547, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6728, loss 0.45616, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6729, loss 0.45715, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 6730, loss 0.45798, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6731, loss 0.45892, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6732, loss 0.45918, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 6733, loss 0.45959, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6734, loss 0.45915, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6735, loss 0.45889, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6736, loss 0.45802, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 6737, loss 0.45738, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 6738, loss 0.45657, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6739, loss 0.45599, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 6740, loss 0.45552, training accuracy 79.32%, test accuracy 51.88%\n",
            "epoch 6741, loss 0.45522, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6742, loss 0.45506, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6743, loss 0.45501, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6744, loss 0.45505, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6745, loss 0.45515, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 6746, loss 0.45530, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6747, loss 0.45548, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6748, loss 0.45573, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6749, loss 0.45599, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 6750, loss 0.45639, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 6751, loss 0.45678, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6752, loss 0.45745, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 6753, loss 0.45805, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 6754, loss 0.45919, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6755, loss 0.46004, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 6756, loss 0.46185, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6757, loss 0.46283, training accuracy 79.92%, test accuracy 53.66%\n",
            "epoch 6758, loss 0.46531, training accuracy 80.12%, test accuracy 51.29%\n",
            "epoch 6759, loss 0.46585, training accuracy 79.52%, test accuracy 53.66%\n",
            "epoch 6760, loss 0.46830, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 6761, loss 0.46743, training accuracy 79.13%, test accuracy 53.47%\n",
            "epoch 6762, loss 0.46836, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 6763, loss 0.46568, training accuracy 79.32%, test accuracy 53.86%\n",
            "epoch 6764, loss 0.46440, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6765, loss 0.46115, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6766, loss 0.45901, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6767, loss 0.45686, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6768, loss 0.45557, training accuracy 80.32%, test accuracy 52.08%\n",
            "epoch 6769, loss 0.45487, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6770, loss 0.45472, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6771, loss 0.45499, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6772, loss 0.45553, training accuracy 79.13%, test accuracy 52.67%\n",
            "epoch 6773, loss 0.45630, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6774, loss 0.45707, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6775, loss 0.45809, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6776, loss 0.45874, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6777, loss 0.45979, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6778, loss 0.46003, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 6779, loss 0.46077, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6780, loss 0.46041, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 6781, loss 0.46059, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6782, loss 0.45972, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 6783, loss 0.45937, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6784, loss 0.45835, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 6785, loss 0.45777, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6786, loss 0.45692, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6787, loss 0.45639, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 6788, loss 0.45583, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6789, loss 0.45547, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6790, loss 0.45516, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6791, loss 0.45495, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6792, loss 0.45479, training accuracy 79.52%, test accuracy 51.68%\n",
            "epoch 6793, loss 0.45469, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6794, loss 0.45461, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6795, loss 0.45456, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6796, loss 0.45452, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6797, loss 0.45450, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6798, loss 0.45449, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 6799, loss 0.45449, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6800, loss 0.45451, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 6801, loss 0.45455, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6802, loss 0.45462, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6803, loss 0.45475, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6804, loss 0.45494, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6805, loss 0.45530, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6806, loss 0.45581, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6807, loss 0.45678, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 6808, loss 0.45807, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 6809, loss 0.46067, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6810, loss 0.46361, training accuracy 79.92%, test accuracy 53.66%\n",
            "epoch 6811, loss 0.47002, training accuracy 79.72%, test accuracy 50.30%\n",
            "epoch 6812, loss 0.47487, training accuracy 78.53%, test accuracy 53.86%\n",
            "epoch 6813, loss 0.48708, training accuracy 78.13%, test accuracy 49.50%\n",
            "epoch 6814, loss 0.48974, training accuracy 78.13%, test accuracy 54.06%\n",
            "epoch 6815, loss 0.50451, training accuracy 77.34%, test accuracy 48.71%\n",
            "epoch 6816, loss 0.50120, training accuracy 76.74%, test accuracy 53.66%\n",
            "epoch 6817, loss 0.51544, training accuracy 77.14%, test accuracy 48.91%\n",
            "epoch 6818, loss 0.50620, training accuracy 76.34%, test accuracy 53.66%\n",
            "epoch 6819, loss 0.50518, training accuracy 77.34%, test accuracy 51.09%\n",
            "epoch 6820, loss 0.47763, training accuracy 78.73%, test accuracy 52.87%\n",
            "epoch 6821, loss 0.45890, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 6822, loss 0.45741, training accuracy 80.12%, test accuracy 51.29%\n",
            "epoch 6823, loss 0.47108, training accuracy 78.93%, test accuracy 54.26%\n",
            "epoch 6824, loss 0.49205, training accuracy 78.53%, test accuracy 50.69%\n",
            "epoch 6825, loss 0.48612, training accuracy 78.33%, test accuracy 53.27%\n",
            "epoch 6826, loss 0.47627, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6827, loss 0.46090, training accuracy 79.52%, test accuracy 52.08%\n",
            "epoch 6828, loss 0.45594, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 6829, loss 0.46091, training accuracy 80.12%, test accuracy 51.68%\n",
            "epoch 6830, loss 0.46758, training accuracy 78.53%, test accuracy 54.26%\n",
            "epoch 6831, loss 0.47180, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 6832, loss 0.46101, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 6833, loss 0.45593, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6834, loss 0.45707, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 6835, loss 0.46203, training accuracy 79.72%, test accuracy 53.47%\n",
            "epoch 6836, loss 0.46391, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6837, loss 0.45992, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6838, loss 0.45655, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 6839, loss 0.45454, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6840, loss 0.45716, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6841, loss 0.45957, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6842, loss 0.45904, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6843, loss 0.45672, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6844, loss 0.45459, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 6845, loss 0.45480, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 6846, loss 0.45610, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 6847, loss 0.45711, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 6848, loss 0.45681, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 6849, loss 0.45540, training accuracy 78.93%, test accuracy 52.48%\n",
            "epoch 6850, loss 0.45431, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 6851, loss 0.45442, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6852, loss 0.45522, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6853, loss 0.45576, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 6854, loss 0.45548, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6855, loss 0.45481, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6856, loss 0.45415, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 6857, loss 0.45409, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 6858, loss 0.45439, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 6859, loss 0.45480, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6860, loss 0.45482, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6861, loss 0.45453, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 6862, loss 0.45416, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6863, loss 0.45387, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6864, loss 0.45390, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6865, loss 0.45402, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6866, loss 0.45423, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6867, loss 0.45422, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6868, loss 0.45411, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 6869, loss 0.45391, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6870, loss 0.45376, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6871, loss 0.45371, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6872, loss 0.45373, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6873, loss 0.45382, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6874, loss 0.45386, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 6875, loss 0.45385, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 6876, loss 0.45380, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6877, loss 0.45370, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6878, loss 0.45362, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6879, loss 0.45357, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 6880, loss 0.45355, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6881, loss 0.45356, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 6882, loss 0.45358, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6883, loss 0.45360, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6884, loss 0.45358, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6885, loss 0.45356, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6886, loss 0.45352, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6887, loss 0.45348, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6888, loss 0.45344, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6889, loss 0.45340, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6890, loss 0.45338, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 6891, loss 0.45337, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6892, loss 0.45336, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6893, loss 0.45336, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6894, loss 0.45335, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6895, loss 0.45335, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6896, loss 0.45334, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6897, loss 0.45332, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6898, loss 0.45331, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6899, loss 0.45329, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6900, loss 0.45327, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6901, loss 0.45325, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6902, loss 0.45323, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6903, loss 0.45322, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6904, loss 0.45320, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6905, loss 0.45318, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6906, loss 0.45316, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6907, loss 0.45315, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6908, loss 0.45313, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6909, loss 0.45312, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6910, loss 0.45311, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6911, loss 0.45310, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6912, loss 0.45309, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6913, loss 0.45308, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6914, loss 0.45308, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6915, loss 0.45309, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6916, loss 0.45310, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6917, loss 0.45313, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6918, loss 0.45319, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6919, loss 0.45328, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 6920, loss 0.45344, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6921, loss 0.45368, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 6922, loss 0.45413, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6923, loss 0.45476, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 6924, loss 0.45599, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6925, loss 0.45759, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 6926, loss 0.46088, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 6927, loss 0.46443, training accuracy 79.32%, test accuracy 54.26%\n",
            "epoch 6928, loss 0.47237, training accuracy 79.72%, test accuracy 49.70%\n",
            "epoch 6929, loss 0.47772, training accuracy 78.53%, test accuracy 54.06%\n",
            "epoch 6930, loss 0.49212, training accuracy 77.93%, test accuracy 49.11%\n",
            "epoch 6931, loss 0.49433, training accuracy 77.34%, test accuracy 53.86%\n",
            "epoch 6932, loss 0.51278, training accuracy 77.34%, test accuracy 48.71%\n",
            "epoch 6933, loss 0.51461, training accuracy 75.35%, test accuracy 54.06%\n",
            "epoch 6934, loss 0.55148, training accuracy 76.14%, test accuracy 47.13%\n",
            "epoch 6935, loss 0.53306, training accuracy 73.76%, test accuracy 53.47%\n",
            "epoch 6936, loss 0.51458, training accuracy 77.53%, test accuracy 51.68%\n",
            "epoch 6937, loss 0.46449, training accuracy 79.92%, test accuracy 51.09%\n",
            "epoch 6938, loss 0.46275, training accuracy 80.12%, test accuracy 53.86%\n",
            "epoch 6939, loss 0.50687, training accuracy 77.93%, test accuracy 48.51%\n",
            "epoch 6940, loss 0.52544, training accuracy 75.75%, test accuracy 54.06%\n",
            "epoch 6941, loss 0.55448, training accuracy 75.94%, test accuracy 50.69%\n",
            "epoch 6942, loss 0.52078, training accuracy 77.53%, test accuracy 53.07%\n",
            "epoch 6943, loss 0.48001, training accuracy 79.32%, test accuracy 53.27%\n",
            "epoch 6944, loss 0.51964, training accuracy 77.73%, test accuracy 48.12%\n",
            "epoch 6945, loss 0.55068, training accuracy 71.97%, test accuracy 54.46%\n",
            "epoch 6946, loss 0.61215, training accuracy 75.35%, test accuracy 51.88%\n",
            "epoch 6947, loss 0.48211, training accuracy 77.14%, test accuracy 48.71%\n",
            "epoch 6948, loss 0.55275, training accuracy 74.35%, test accuracy 55.25%\n",
            "epoch 6949, loss 0.55119, training accuracy 76.34%, test accuracy 52.28%\n",
            "epoch 6950, loss 0.54177, training accuracy 78.13%, test accuracy 52.48%\n",
            "epoch 6951, loss 0.48655, training accuracy 79.52%, test accuracy 54.26%\n",
            "epoch 6952, loss 0.53215, training accuracy 78.13%, test accuracy 51.49%\n",
            "epoch 6953, loss 0.50854, training accuracy 76.54%, test accuracy 53.86%\n",
            "epoch 6954, loss 0.50305, training accuracy 78.93%, test accuracy 51.88%\n",
            "epoch 6955, loss 0.51187, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 6956, loss 0.48339, training accuracy 78.73%, test accuracy 54.06%\n",
            "epoch 6957, loss 0.51454, training accuracy 77.73%, test accuracy 52.08%\n",
            "epoch 6958, loss 0.47246, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 6959, loss 0.50144, training accuracy 78.53%, test accuracy 55.25%\n",
            "epoch 6960, loss 0.49329, training accuracy 78.53%, test accuracy 51.29%\n",
            "epoch 6961, loss 0.48048, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 6962, loss 0.46720, training accuracy 78.73%, test accuracy 54.46%\n",
            "epoch 6963, loss 0.48446, training accuracy 78.73%, test accuracy 53.07%\n",
            "epoch 6964, loss 0.47249, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 6965, loss 0.47036, training accuracy 78.93%, test accuracy 52.87%\n",
            "epoch 6966, loss 0.46259, training accuracy 79.72%, test accuracy 53.66%\n",
            "epoch 6967, loss 0.46813, training accuracy 79.72%, test accuracy 53.66%\n",
            "epoch 6968, loss 0.46132, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 6969, loss 0.46049, training accuracy 79.13%, test accuracy 53.07%\n",
            "epoch 6970, loss 0.45965, training accuracy 79.72%, test accuracy 54.06%\n",
            "epoch 6971, loss 0.46083, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 6972, loss 0.45882, training accuracy 79.13%, test accuracy 53.47%\n",
            "epoch 6973, loss 0.45785, training accuracy 79.13%, test accuracy 53.07%\n",
            "epoch 6974, loss 0.45861, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6975, loss 0.45746, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 6976, loss 0.45718, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 6977, loss 0.45616, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6978, loss 0.45718, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 6979, loss 0.45582, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 6980, loss 0.45611, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 6981, loss 0.45572, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6982, loss 0.45614, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 6983, loss 0.45502, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 6984, loss 0.45511, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 6985, loss 0.45504, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 6986, loss 0.45498, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 6987, loss 0.45448, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6988, loss 0.45445, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 6989, loss 0.45449, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 6990, loss 0.45448, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6991, loss 0.45411, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6992, loss 0.45386, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 6993, loss 0.45424, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 6994, loss 0.45406, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 6995, loss 0.45380, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 6996, loss 0.45363, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 6997, loss 0.45376, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 6998, loss 0.45374, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 6999, loss 0.45356, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7000, loss 0.45339, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 7001, loss 0.45352, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7002, loss 0.45349, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7003, loss 0.45329, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7004, loss 0.45322, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7005, loss 0.45332, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7006, loss 0.45331, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7007, loss 0.45311, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 7008, loss 0.45309, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7009, loss 0.45314, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7010, loss 0.45310, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7011, loss 0.45299, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7012, loss 0.45297, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 7013, loss 0.45297, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7014, loss 0.45296, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7015, loss 0.45288, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7016, loss 0.45286, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7017, loss 0.45285, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7018, loss 0.45283, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7019, loss 0.45279, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7020, loss 0.45276, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7021, loss 0.45273, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7022, loss 0.45272, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7023, loss 0.45270, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7024, loss 0.45266, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7025, loss 0.45263, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 7026, loss 0.45262, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7027, loss 0.45261, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 7028, loss 0.45258, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7029, loss 0.45255, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7030, loss 0.45253, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 7031, loss 0.45252, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7032, loss 0.45250, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7033, loss 0.45247, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7034, loss 0.45245, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7035, loss 0.45243, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 7036, loss 0.45242, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7037, loss 0.45240, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7038, loss 0.45238, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7039, loss 0.45236, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7040, loss 0.45234, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7041, loss 0.45232, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7042, loss 0.45230, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7043, loss 0.45228, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7044, loss 0.45227, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7045, loss 0.45225, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7046, loss 0.45223, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7047, loss 0.45221, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7048, loss 0.45220, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7049, loss 0.45218, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7050, loss 0.45216, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7051, loss 0.45214, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7052, loss 0.45213, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7053, loss 0.45211, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7054, loss 0.45209, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7055, loss 0.45208, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7056, loss 0.45206, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7057, loss 0.45204, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7058, loss 0.45203, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7059, loss 0.45201, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7060, loss 0.45199, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7061, loss 0.45198, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7062, loss 0.45196, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7063, loss 0.45195, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7064, loss 0.45193, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7065, loss 0.45191, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7066, loss 0.45190, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7067, loss 0.45188, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7068, loss 0.45187, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7069, loss 0.45185, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7070, loss 0.45184, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7071, loss 0.45182, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7072, loss 0.45180, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7073, loss 0.45179, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7074, loss 0.45177, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7075, loss 0.45176, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7076, loss 0.45174, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7077, loss 0.45173, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7078, loss 0.45171, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7079, loss 0.45170, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7080, loss 0.45168, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7081, loss 0.45167, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7082, loss 0.45165, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7083, loss 0.45164, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7084, loss 0.45162, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7085, loss 0.45161, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7086, loss 0.45159, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7087, loss 0.45158, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7088, loss 0.45156, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7089, loss 0.45155, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7090, loss 0.45153, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7091, loss 0.45152, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7092, loss 0.45150, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7093, loss 0.45149, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7094, loss 0.45148, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7095, loss 0.45146, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7096, loss 0.45145, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7097, loss 0.45143, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7098, loss 0.45142, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7099, loss 0.45140, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7100, loss 0.45139, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7101, loss 0.45137, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7102, loss 0.45136, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7103, loss 0.45135, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7104, loss 0.45133, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7105, loss 0.45132, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7106, loss 0.45130, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7107, loss 0.45129, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7108, loss 0.45128, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7109, loss 0.45126, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7110, loss 0.45125, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7111, loss 0.45123, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7112, loss 0.45122, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7113, loss 0.45121, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7114, loss 0.45119, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7115, loss 0.45118, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7116, loss 0.45116, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7117, loss 0.45115, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7118, loss 0.45114, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7119, loss 0.45112, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7120, loss 0.45111, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7121, loss 0.45109, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7122, loss 0.45108, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7123, loss 0.45107, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7124, loss 0.45105, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7125, loss 0.45104, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7126, loss 0.45103, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7127, loss 0.45101, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7128, loss 0.45100, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7129, loss 0.45098, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7130, loss 0.45097, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7131, loss 0.45096, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7132, loss 0.45094, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7133, loss 0.45093, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7134, loss 0.45092, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7135, loss 0.45090, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7136, loss 0.45089, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7137, loss 0.45088, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7138, loss 0.45086, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7139, loss 0.45085, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7140, loss 0.45084, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7141, loss 0.45082, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7142, loss 0.45081, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7143, loss 0.45079, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7144, loss 0.45078, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7145, loss 0.45077, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7146, loss 0.45075, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7147, loss 0.45074, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7148, loss 0.45073, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7149, loss 0.45071, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7150, loss 0.45070, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7151, loss 0.45069, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7152, loss 0.45067, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7153, loss 0.45066, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7154, loss 0.45065, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7155, loss 0.45064, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7156, loss 0.45062, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7157, loss 0.45061, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7158, loss 0.45060, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7159, loss 0.45059, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7160, loss 0.45058, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7161, loss 0.45058, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7162, loss 0.45058, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7163, loss 0.45059, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7164, loss 0.45063, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7165, loss 0.45072, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7166, loss 0.45089, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7167, loss 0.45120, training accuracy 79.52%, test accuracy 52.48%\n",
            "epoch 7168, loss 0.45187, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 7169, loss 0.45300, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 7170, loss 0.45549, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 7171, loss 0.45917, training accuracy 80.12%, test accuracy 54.06%\n",
            "epoch 7172, loss 0.46791, training accuracy 79.92%, test accuracy 49.70%\n",
            "epoch 7173, loss 0.47648, training accuracy 78.53%, test accuracy 53.86%\n",
            "epoch 7174, loss 0.49914, training accuracy 77.93%, test accuracy 48.32%\n",
            "epoch 7175, loss 0.50330, training accuracy 75.94%, test accuracy 53.47%\n",
            "epoch 7176, loss 0.53529, training accuracy 77.14%, test accuracy 47.52%\n",
            "epoch 7177, loss 0.52650, training accuracy 73.96%, test accuracy 53.66%\n",
            "epoch 7178, loss 0.55075, training accuracy 76.34%, test accuracy 49.11%\n",
            "epoch 7179, loss 0.50835, training accuracy 75.15%, test accuracy 53.27%\n",
            "epoch 7180, loss 0.47641, training accuracy 78.73%, test accuracy 51.49%\n",
            "epoch 7181, loss 0.45485, training accuracy 79.72%, test accuracy 50.69%\n",
            "epoch 7182, loss 0.47229, training accuracy 78.93%, test accuracy 53.27%\n",
            "epoch 7183, loss 0.51448, training accuracy 77.73%, test accuracy 49.70%\n",
            "epoch 7184, loss 0.49268, training accuracy 77.34%, test accuracy 53.07%\n",
            "epoch 7185, loss 0.47265, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 7186, loss 0.45710, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 7187, loss 0.45756, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 7188, loss 0.46584, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 7189, loss 0.47198, training accuracy 79.13%, test accuracy 54.06%\n",
            "epoch 7190, loss 0.46685, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 7191, loss 0.45842, training accuracy 79.52%, test accuracy 53.47%\n",
            "epoch 7192, loss 0.45342, training accuracy 79.52%, test accuracy 54.06%\n",
            "epoch 7193, loss 0.46491, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 7194, loss 0.46711, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 7195, loss 0.46441, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 7196, loss 0.45494, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7197, loss 0.45303, training accuracy 79.32%, test accuracy 53.27%\n",
            "epoch 7198, loss 0.45960, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 7199, loss 0.46033, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 7200, loss 0.45834, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 7201, loss 0.45216, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 7202, loss 0.45263, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7203, loss 0.45462, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7204, loss 0.45672, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7205, loss 0.45448, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7206, loss 0.45216, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7207, loss 0.45110, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 7208, loss 0.45329, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 7209, loss 0.45416, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7210, loss 0.45348, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7211, loss 0.45147, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 7212, loss 0.45084, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7213, loss 0.45172, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7214, loss 0.45239, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 7215, loss 0.45244, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7216, loss 0.45126, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 7217, loss 0.45063, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7218, loss 0.45069, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7219, loss 0.45135, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 7220, loss 0.45153, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7221, loss 0.45127, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 7222, loss 0.45054, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 7223, loss 0.45044, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7224, loss 0.45046, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7225, loss 0.45088, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7226, loss 0.45086, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 7227, loss 0.45071, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7228, loss 0.45035, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7229, loss 0.45023, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7230, loss 0.45030, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7231, loss 0.45044, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7232, loss 0.45051, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7233, loss 0.45040, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7234, loss 0.45025, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7235, loss 0.45012, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7236, loss 0.45010, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7237, loss 0.45015, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7238, loss 0.45023, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7239, loss 0.45020, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7240, loss 0.45016, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7241, loss 0.45005, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7242, loss 0.45000, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7243, loss 0.44997, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7244, loss 0.44999, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7245, loss 0.45001, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7246, loss 0.45002, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7247, loss 0.44999, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7248, loss 0.44994, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7249, loss 0.44989, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7250, loss 0.44986, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7251, loss 0.44985, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7252, loss 0.44985, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7253, loss 0.44986, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7254, loss 0.44985, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7255, loss 0.44984, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7256, loss 0.44981, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7257, loss 0.44978, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7258, loss 0.44975, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7259, loss 0.44974, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7260, loss 0.44972, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7261, loss 0.44972, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7262, loss 0.44971, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7263, loss 0.44970, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7264, loss 0.44969, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7265, loss 0.44968, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7266, loss 0.44966, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7267, loss 0.44964, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7268, loss 0.44963, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7269, loss 0.44961, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7270, loss 0.44960, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7271, loss 0.44958, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7272, loss 0.44957, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7273, loss 0.44956, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7274, loss 0.44955, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7275, loss 0.44954, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7276, loss 0.44953, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7277, loss 0.44951, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7278, loss 0.44950, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7279, loss 0.44949, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7280, loss 0.44948, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7281, loss 0.44947, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7282, loss 0.44945, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7283, loss 0.44944, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7284, loss 0.44943, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7285, loss 0.44942, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7286, loss 0.44941, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7287, loss 0.44940, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7288, loss 0.44938, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7289, loss 0.44937, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7290, loss 0.44937, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7291, loss 0.44936, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7292, loss 0.44935, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7293, loss 0.44935, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7294, loss 0.44935, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7295, loss 0.44936, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7296, loss 0.44937, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7297, loss 0.44940, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7298, loss 0.44946, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7299, loss 0.44955, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7300, loss 0.44969, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7301, loss 0.44994, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7302, loss 0.45031, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7303, loss 0.45100, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 7304, loss 0.45193, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 7305, loss 0.45381, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7306, loss 0.45602, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7307, loss 0.46084, training accuracy 80.32%, test accuracy 50.50%\n",
            "epoch 7308, loss 0.46491, training accuracy 79.52%, test accuracy 53.66%\n",
            "epoch 7309, loss 0.47493, training accuracy 78.73%, test accuracy 49.70%\n",
            "epoch 7310, loss 0.47793, training accuracy 78.33%, test accuracy 53.86%\n",
            "epoch 7311, loss 0.48995, training accuracy 77.73%, test accuracy 49.11%\n",
            "epoch 7312, loss 0.48436, training accuracy 77.34%, test accuracy 53.47%\n",
            "epoch 7313, loss 0.48561, training accuracy 77.73%, test accuracy 50.89%\n",
            "epoch 7314, loss 0.47072, training accuracy 78.73%, test accuracy 53.27%\n",
            "epoch 7315, loss 0.46016, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 7316, loss 0.45120, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7317, loss 0.44988, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 7318, loss 0.45492, training accuracy 79.52%, test accuracy 51.29%\n",
            "epoch 7319, loss 0.46073, training accuracy 79.72%, test accuracy 53.66%\n",
            "epoch 7320, loss 0.46774, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 7321, loss 0.46715, training accuracy 79.32%, test accuracy 53.66%\n",
            "epoch 7322, loss 0.46546, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 7323, loss 0.45770, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 7324, loss 0.45225, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7325, loss 0.44940, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7326, loss 0.45066, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 7327, loss 0.45414, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 7328, loss 0.45661, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7329, loss 0.45785, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7330, loss 0.45540, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7331, loss 0.45269, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 7332, loss 0.44997, training accuracy 79.13%, test accuracy 52.48%\n",
            "epoch 7333, loss 0.44917, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7334, loss 0.45011, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7335, loss 0.45168, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 7336, loss 0.45319, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7337, loss 0.45329, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7338, loss 0.45281, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 7339, loss 0.45116, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 7340, loss 0.44979, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7341, loss 0.44908, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7342, loss 0.44914, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7343, loss 0.44978, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7344, loss 0.45050, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 7345, loss 0.45113, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 7346, loss 0.45118, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 7347, loss 0.45099, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 7348, loss 0.45035, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7349, loss 0.44971, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7350, loss 0.44920, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 7351, loss 0.44893, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 7352, loss 0.44888, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7353, loss 0.44902, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7354, loss 0.44926, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7355, loss 0.44950, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7356, loss 0.44971, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7357, loss 0.44980, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 7358, loss 0.44985, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7359, loss 0.44975, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7360, loss 0.44966, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7361, loss 0.44949, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 7362, loss 0.44934, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7363, loss 0.44917, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7364, loss 0.44902, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7365, loss 0.44890, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7366, loss 0.44880, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7367, loss 0.44873, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7368, loss 0.44869, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7369, loss 0.44866, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7370, loss 0.44864, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7371, loss 0.44863, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7372, loss 0.44862, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7373, loss 0.44862, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7374, loss 0.44863, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7375, loss 0.44864, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7376, loss 0.44866, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7377, loss 0.44869, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7378, loss 0.44873, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7379, loss 0.44881, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7380, loss 0.44891, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7381, loss 0.44908, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7382, loss 0.44933, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 7383, loss 0.44976, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7384, loss 0.45034, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 7385, loss 0.45146, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7386, loss 0.45282, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7387, loss 0.45561, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 7388, loss 0.45843, training accuracy 79.92%, test accuracy 53.66%\n",
            "epoch 7389, loss 0.46485, training accuracy 79.72%, test accuracy 50.10%\n",
            "epoch 7390, loss 0.46909, training accuracy 79.32%, test accuracy 53.47%\n",
            "epoch 7391, loss 0.48067, training accuracy 78.13%, test accuracy 49.70%\n",
            "epoch 7392, loss 0.48281, training accuracy 77.73%, test accuracy 53.66%\n",
            "epoch 7393, loss 0.49634, training accuracy 77.53%, test accuracy 49.11%\n",
            "epoch 7394, loss 0.49286, training accuracy 76.94%, test accuracy 53.66%\n",
            "epoch 7395, loss 0.49983, training accuracy 77.14%, test accuracy 49.31%\n",
            "epoch 7396, loss 0.48737, training accuracy 77.73%, test accuracy 53.66%\n",
            "epoch 7397, loss 0.47715, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 7398, loss 0.45764, training accuracy 81.11%, test accuracy 52.28%\n",
            "epoch 7399, loss 0.44926, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7400, loss 0.45317, training accuracy 80.32%, test accuracy 51.29%\n",
            "epoch 7401, loss 0.46264, training accuracy 79.92%, test accuracy 54.06%\n",
            "epoch 7402, loss 0.47340, training accuracy 79.32%, test accuracy 51.29%\n",
            "epoch 7403, loss 0.46992, training accuracy 79.13%, test accuracy 53.07%\n",
            "epoch 7404, loss 0.46387, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7405, loss 0.45431, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7406, loss 0.44995, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 7407, loss 0.45013, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7408, loss 0.45426, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7409, loss 0.45899, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7410, loss 0.45636, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7411, loss 0.45314, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7412, loss 0.44959, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 7413, loss 0.44897, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7414, loss 0.45049, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7415, loss 0.45222, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7416, loss 0.45361, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7417, loss 0.45179, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7418, loss 0.45006, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7419, loss 0.44877, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7420, loss 0.44874, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7421, loss 0.44963, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7422, loss 0.45026, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7423, loss 0.45076, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 7424, loss 0.44999, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 7425, loss 0.44924, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7426, loss 0.44863, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7427, loss 0.44835, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7428, loss 0.44867, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7429, loss 0.44891, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 7430, loss 0.44928, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7431, loss 0.44921, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7432, loss 0.44899, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7433, loss 0.44873, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7434, loss 0.44836, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7435, loss 0.44829, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7436, loss 0.44822, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7437, loss 0.44835, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7438, loss 0.44847, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7439, loss 0.44855, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7440, loss 0.44863, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7441, loss 0.44854, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7442, loss 0.44847, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7443, loss 0.44834, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 7444, loss 0.44823, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7445, loss 0.44814, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7446, loss 0.44807, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7447, loss 0.44807, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7448, loss 0.44806, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7449, loss 0.44810, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7450, loss 0.44813, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7451, loss 0.44815, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7452, loss 0.44817, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7453, loss 0.44817, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7454, loss 0.44817, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7455, loss 0.44815, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7456, loss 0.44813, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7457, loss 0.44810, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7458, loss 0.44807, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7459, loss 0.44804, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7460, loss 0.44802, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7461, loss 0.44799, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7462, loss 0.44797, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7463, loss 0.44796, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7464, loss 0.44794, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7465, loss 0.44793, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7466, loss 0.44793, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7467, loss 0.44793, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7468, loss 0.44794, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7469, loss 0.44796, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7470, loss 0.44800, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7471, loss 0.44805, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7472, loss 0.44814, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7473, loss 0.44827, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7474, loss 0.44850, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 7475, loss 0.44880, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7476, loss 0.44936, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 7477, loss 0.45006, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7478, loss 0.45142, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 7479, loss 0.45303, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7480, loss 0.45639, training accuracy 80.32%, test accuracy 51.29%\n",
            "epoch 7481, loss 0.45956, training accuracy 79.92%, test accuracy 53.66%\n",
            "epoch 7482, loss 0.46697, training accuracy 79.72%, test accuracy 50.10%\n",
            "epoch 7483, loss 0.47152, training accuracy 79.32%, test accuracy 53.66%\n",
            "epoch 7484, loss 0.48485, training accuracy 77.73%, test accuracy 49.11%\n",
            "epoch 7485, loss 0.48809, training accuracy 77.34%, test accuracy 53.86%\n",
            "epoch 7486, loss 0.50719, training accuracy 77.34%, test accuracy 49.31%\n",
            "epoch 7487, loss 0.51486, training accuracy 75.75%, test accuracy 53.66%\n",
            "epoch 7488, loss 0.56036, training accuracy 76.34%, test accuracy 46.73%\n",
            "epoch 7489, loss 0.55638, training accuracy 72.17%, test accuracy 54.46%\n",
            "epoch 7490, loss 0.57904, training accuracy 76.14%, test accuracy 51.68%\n",
            "epoch 7491, loss 0.48982, training accuracy 77.93%, test accuracy 51.49%\n",
            "epoch 7492, loss 0.45393, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 7493, loss 0.50527, training accuracy 77.93%, test accuracy 46.93%\n",
            "epoch 7494, loss 0.55787, training accuracy 72.96%, test accuracy 54.46%\n",
            "epoch 7495, loss 0.58819, training accuracy 75.94%, test accuracy 51.68%\n",
            "epoch 7496, loss 0.49387, training accuracy 77.93%, test accuracy 51.29%\n",
            "epoch 7497, loss 0.48359, training accuracy 77.93%, test accuracy 54.26%\n",
            "epoch 7498, loss 0.60074, training accuracy 76.34%, test accuracy 48.91%\n",
            "epoch 7499, loss 0.49454, training accuracy 77.73%, test accuracy 51.09%\n",
            "epoch 7500, loss 0.46735, training accuracy 79.52%, test accuracy 54.46%\n",
            "epoch 7501, loss 0.50350, training accuracy 78.33%, test accuracy 48.12%\n",
            "epoch 7502, loss 0.50688, training accuracy 76.54%, test accuracy 53.66%\n",
            "epoch 7503, loss 0.47857, training accuracy 78.93%, test accuracy 53.07%\n",
            "epoch 7504, loss 0.46220, training accuracy 79.92%, test accuracy 51.68%\n",
            "epoch 7505, loss 0.47658, training accuracy 78.33%, test accuracy 53.66%\n",
            "epoch 7506, loss 0.48873, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 7507, loss 0.46725, training accuracy 78.53%, test accuracy 53.07%\n",
            "epoch 7508, loss 0.45230, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 7509, loss 0.46428, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 7510, loss 0.46709, training accuracy 79.52%, test accuracy 54.06%\n",
            "epoch 7511, loss 0.46717, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7512, loss 0.45825, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 7513, loss 0.45873, training accuracy 79.92%, test accuracy 53.66%\n",
            "epoch 7514, loss 0.46390, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 7515, loss 0.45464, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 7516, loss 0.45502, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 7517, loss 0.45795, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 7518, loss 0.46061, training accuracy 79.32%, test accuracy 53.07%\n",
            "epoch 7519, loss 0.45534, training accuracy 80.72%, test accuracy 53.86%\n",
            "epoch 7520, loss 0.45579, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 7521, loss 0.45582, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 7522, loss 0.45734, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7523, loss 0.45357, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 7524, loss 0.45240, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 7525, loss 0.45491, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7526, loss 0.45333, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 7527, loss 0.45169, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 7528, loss 0.45203, training accuracy 81.11%, test accuracy 53.07%\n",
            "epoch 7529, loss 0.45355, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 7530, loss 0.45026, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7531, loss 0.45089, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7532, loss 0.45125, training accuracy 79.52%, test accuracy 53.86%\n",
            "epoch 7533, loss 0.45166, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7534, loss 0.44949, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 7535, loss 0.44974, training accuracy 80.12%, test accuracy 53.47%\n",
            "epoch 7536, loss 0.45071, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7537, loss 0.45020, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7538, loss 0.44911, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7539, loss 0.44915, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7540, loss 0.44978, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 7541, loss 0.44930, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7542, loss 0.44867, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 7543, loss 0.44886, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 7544, loss 0.44926, training accuracy 80.91%, test accuracy 53.27%\n",
            "epoch 7545, loss 0.44855, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7546, loss 0.44836, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7547, loss 0.44865, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7548, loss 0.44860, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7549, loss 0.44847, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7550, loss 0.44816, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 7551, loss 0.44826, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7552, loss 0.44823, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7553, loss 0.44826, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7554, loss 0.44800, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7555, loss 0.44805, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7556, loss 0.44809, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7557, loss 0.44799, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7558, loss 0.44788, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7559, loss 0.44790, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7560, loss 0.44790, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7561, loss 0.44787, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7562, loss 0.44781, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 7563, loss 0.44770, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7564, loss 0.44778, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7565, loss 0.44777, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7566, loss 0.44771, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7567, loss 0.44763, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7568, loss 0.44767, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7569, loss 0.44765, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7570, loss 0.44763, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7571, loss 0.44760, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7572, loss 0.44756, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7573, loss 0.44757, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7574, loss 0.44755, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7575, loss 0.44752, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7576, loss 0.44750, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7577, loss 0.44748, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7578, loss 0.44748, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7579, loss 0.44747, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7580, loss 0.44744, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7581, loss 0.44742, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7582, loss 0.44741, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7583, loss 0.44740, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7584, loss 0.44739, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7585, loss 0.44737, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7586, loss 0.44735, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7587, loss 0.44734, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7588, loss 0.44733, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7589, loss 0.44732, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7590, loss 0.44730, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7591, loss 0.44729, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7592, loss 0.44727, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7593, loss 0.44727, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7594, loss 0.44725, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7595, loss 0.44724, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7596, loss 0.44722, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7597, loss 0.44721, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7598, loss 0.44720, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7599, loss 0.44719, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7600, loss 0.44718, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7601, loss 0.44717, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7602, loss 0.44716, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7603, loss 0.44714, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7604, loss 0.44713, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7605, loss 0.44712, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7606, loss 0.44711, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7607, loss 0.44710, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7608, loss 0.44709, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7609, loss 0.44708, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7610, loss 0.44707, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7611, loss 0.44706, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7612, loss 0.44704, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7613, loss 0.44703, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7614, loss 0.44702, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7615, loss 0.44701, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7616, loss 0.44700, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7617, loss 0.44699, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7618, loss 0.44698, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7619, loss 0.44697, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7620, loss 0.44696, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7621, loss 0.44695, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7622, loss 0.44694, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7623, loss 0.44693, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7624, loss 0.44692, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7625, loss 0.44691, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7626, loss 0.44690, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7627, loss 0.44689, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7628, loss 0.44688, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7629, loss 0.44687, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7630, loss 0.44686, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7631, loss 0.44685, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7632, loss 0.44684, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7633, loss 0.44683, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7634, loss 0.44682, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7635, loss 0.44681, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7636, loss 0.44680, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7637, loss 0.44679, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7638, loss 0.44678, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7639, loss 0.44677, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7640, loss 0.44676, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7641, loss 0.44675, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7642, loss 0.44674, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7643, loss 0.44673, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7644, loss 0.44672, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7645, loss 0.44671, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7646, loss 0.44670, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7647, loss 0.44669, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7648, loss 0.44668, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7649, loss 0.44667, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7650, loss 0.44666, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7651, loss 0.44665, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7652, loss 0.44664, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7653, loss 0.44663, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7654, loss 0.44662, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7655, loss 0.44661, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7656, loss 0.44660, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7657, loss 0.44659, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7658, loss 0.44658, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7659, loss 0.44657, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7660, loss 0.44656, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7661, loss 0.44655, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7662, loss 0.44655, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7663, loss 0.44654, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7664, loss 0.44653, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7665, loss 0.44652, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7666, loss 0.44651, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7667, loss 0.44650, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7668, loss 0.44649, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7669, loss 0.44648, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7670, loss 0.44647, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7671, loss 0.44646, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7672, loss 0.44645, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7673, loss 0.44644, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7674, loss 0.44643, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7675, loss 0.44642, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7676, loss 0.44641, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7677, loss 0.44640, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7678, loss 0.44640, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7679, loss 0.44639, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7680, loss 0.44638, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7681, loss 0.44637, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7682, loss 0.44636, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7683, loss 0.44635, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7684, loss 0.44634, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7685, loss 0.44633, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7686, loss 0.44632, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7687, loss 0.44631, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7688, loss 0.44630, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7689, loss 0.44629, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7690, loss 0.44629, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7691, loss 0.44628, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7692, loss 0.44627, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7693, loss 0.44626, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7694, loss 0.44625, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7695, loss 0.44624, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7696, loss 0.44623, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7697, loss 0.44622, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7698, loss 0.44621, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7699, loss 0.44620, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7700, loss 0.44619, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7701, loss 0.44618, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7702, loss 0.44618, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7703, loss 0.44617, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7704, loss 0.44616, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7705, loss 0.44615, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7706, loss 0.44614, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7707, loss 0.44613, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7708, loss 0.44612, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7709, loss 0.44611, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7710, loss 0.44610, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7711, loss 0.44610, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7712, loss 0.44609, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7713, loss 0.44608, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7714, loss 0.44607, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7715, loss 0.44607, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7716, loss 0.44607, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7717, loss 0.44607, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7718, loss 0.44609, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7719, loss 0.44613, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 7720, loss 0.44622, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7721, loss 0.44638, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7722, loss 0.44671, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 7723, loss 0.44730, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7724, loss 0.44857, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 7725, loss 0.45061, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7726, loss 0.45533, training accuracy 80.52%, test accuracy 50.69%\n",
            "epoch 7727, loss 0.46148, training accuracy 79.52%, test accuracy 53.66%\n",
            "epoch 7728, loss 0.47714, training accuracy 78.33%, test accuracy 48.51%\n",
            "epoch 7729, loss 0.48848, training accuracy 76.94%, test accuracy 53.47%\n",
            "epoch 7730, loss 0.52717, training accuracy 77.34%, test accuracy 47.33%\n",
            "epoch 7731, loss 0.54869, training accuracy 73.96%, test accuracy 54.26%\n",
            "epoch 7732, loss 0.63541, training accuracy 75.75%, test accuracy 47.52%\n",
            "epoch 7733, loss 0.53197, training accuracy 72.37%, test accuracy 53.27%\n",
            "epoch 7734, loss 0.49297, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 7735, loss 0.45645, training accuracy 79.92%, test accuracy 48.32%\n",
            "epoch 7736, loss 0.51344, training accuracy 74.16%, test accuracy 54.06%\n",
            "epoch 7737, loss 0.60987, training accuracy 75.55%, test accuracy 50.50%\n",
            "epoch 7738, loss 0.60515, training accuracy 76.14%, test accuracy 52.08%\n",
            "epoch 7739, loss 0.52029, training accuracy 78.73%, test accuracy 51.88%\n",
            "epoch 7740, loss 0.54951, training accuracy 76.14%, test accuracy 51.68%\n",
            "epoch 7741, loss 0.63548, training accuracy 74.75%, test accuracy 54.85%\n",
            "epoch 7742, loss 0.59042, training accuracy 76.54%, test accuracy 52.28%\n",
            "epoch 7743, loss 0.49017, training accuracy 78.73%, test accuracy 44.16%\n",
            "epoch 7744, loss 0.61908, training accuracy 65.81%, test accuracy 53.86%\n",
            "epoch 7745, loss 0.60129, training accuracy 75.55%, test accuracy 53.66%\n",
            "epoch 7746, loss 0.59380, training accuracy 75.35%, test accuracy 46.53%\n",
            "epoch 7747, loss 0.55081, training accuracy 71.77%, test accuracy 54.06%\n",
            "epoch 7748, loss 0.66772, training accuracy 75.15%, test accuracy 54.26%\n",
            "epoch 7749, loss 0.49548, training accuracy 78.33%, test accuracy 50.30%\n",
            "epoch 7750, loss 0.58452, training accuracy 75.55%, test accuracy 55.05%\n",
            "epoch 7751, loss 0.61585, training accuracy 74.95%, test accuracy 51.88%\n",
            "epoch 7752, loss 0.49293, training accuracy 78.33%, test accuracy 48.91%\n",
            "epoch 7753, loss 0.59568, training accuracy 73.56%, test accuracy 54.65%\n",
            "epoch 7754, loss 0.60618, training accuracy 76.54%, test accuracy 53.66%\n",
            "epoch 7755, loss 0.51987, training accuracy 77.73%, test accuracy 45.94%\n",
            "epoch 7756, loss 0.63966, training accuracy 67.40%, test accuracy 55.25%\n",
            "epoch 7757, loss 0.59059, training accuracy 76.54%, test accuracy 55.05%\n",
            "epoch 7758, loss 0.53277, training accuracy 77.93%, test accuracy 45.74%\n",
            "epoch 7759, loss 0.66341, training accuracy 67.40%, test accuracy 54.06%\n",
            "epoch 7760, loss 0.60466, training accuracy 74.75%, test accuracy 54.06%\n",
            "epoch 7761, loss 0.51957, training accuracy 77.73%, test accuracy 46.34%\n",
            "epoch 7762, loss 0.65011, training accuracy 70.58%, test accuracy 52.08%\n",
            "epoch 7763, loss 0.53217, training accuracy 77.34%, test accuracy 54.06%\n",
            "epoch 7764, loss 0.55382, training accuracy 77.34%, test accuracy 49.50%\n",
            "epoch 7765, loss 0.56980, training accuracy 74.75%, test accuracy 51.68%\n",
            "epoch 7766, loss 0.48903, training accuracy 78.33%, test accuracy 53.47%\n",
            "epoch 7767, loss 0.56025, training accuracy 77.14%, test accuracy 51.88%\n",
            "epoch 7768, loss 0.47793, training accuracy 78.73%, test accuracy 51.09%\n",
            "epoch 7769, loss 0.49911, training accuracy 77.93%, test accuracy 53.47%\n",
            "epoch 7770, loss 0.51556, training accuracy 77.93%, test accuracy 52.48%\n",
            "epoch 7771, loss 0.49638, training accuracy 77.34%, test accuracy 51.49%\n",
            "epoch 7772, loss 0.50847, training accuracy 77.73%, test accuracy 52.87%\n",
            "epoch 7773, loss 0.46602, training accuracy 78.53%, test accuracy 53.27%\n",
            "epoch 7774, loss 0.50456, training accuracy 76.94%, test accuracy 51.09%\n",
            "epoch 7775, loss 0.47390, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 7776, loss 0.48142, training accuracy 78.93%, test accuracy 53.66%\n",
            "epoch 7777, loss 0.48856, training accuracy 78.33%, test accuracy 50.69%\n",
            "epoch 7778, loss 0.48191, training accuracy 78.73%, test accuracy 51.29%\n",
            "epoch 7779, loss 0.48207, training accuracy 77.93%, test accuracy 52.87%\n",
            "epoch 7780, loss 0.48381, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 7781, loss 0.46545, training accuracy 79.13%, test accuracy 50.30%\n",
            "epoch 7782, loss 0.48277, training accuracy 78.33%, test accuracy 53.27%\n",
            "epoch 7783, loss 0.46710, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 7784, loss 0.45893, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 7785, loss 0.46854, training accuracy 78.93%, test accuracy 51.49%\n",
            "epoch 7786, loss 0.45806, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7787, loss 0.46934, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 7788, loss 0.45598, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 7789, loss 0.46086, training accuracy 79.13%, test accuracy 52.67%\n",
            "epoch 7790, loss 0.46235, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7791, loss 0.45542, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 7792, loss 0.45930, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7793, loss 0.45458, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 7794, loss 0.45811, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 7795, loss 0.45834, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 7796, loss 0.45441, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7797, loss 0.45453, training accuracy 80.12%, test accuracy 52.08%\n",
            "epoch 7798, loss 0.45649, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7799, loss 0.45154, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 7800, loss 0.45520, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7801, loss 0.45206, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 7802, loss 0.45254, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 7803, loss 0.45314, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7804, loss 0.44984, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 7805, loss 0.45402, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 7806, loss 0.45027, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 7807, loss 0.45144, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7808, loss 0.45075, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7809, loss 0.45026, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7810, loss 0.45085, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 7811, loss 0.44977, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 7812, loss 0.44971, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7813, loss 0.44976, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7814, loss 0.44953, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7815, loss 0.44915, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 7816, loss 0.44938, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 7817, loss 0.44886, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 7818, loss 0.44902, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 7819, loss 0.44885, training accuracy 80.91%, test accuracy 53.07%\n",
            "epoch 7820, loss 0.44847, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 7821, loss 0.44899, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7822, loss 0.44826, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7823, loss 0.44842, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7824, loss 0.44846, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 7825, loss 0.44806, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 7826, loss 0.44838, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7827, loss 0.44802, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7828, loss 0.44804, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 7829, loss 0.44804, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7830, loss 0.44790, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7831, loss 0.44789, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7832, loss 0.44782, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7833, loss 0.44778, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 7834, loss 0.44773, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 7835, loss 0.44772, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7836, loss 0.44759, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7837, loss 0.44765, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 7838, loss 0.44756, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7839, loss 0.44748, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7840, loss 0.44754, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 7841, loss 0.44742, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7842, loss 0.44742, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 7843, loss 0.44739, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 7844, loss 0.44734, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7845, loss 0.44733, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7846, loss 0.44730, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 7847, loss 0.44726, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7848, loss 0.44723, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7849, loss 0.44722, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 7850, loss 0.44716, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7851, loss 0.44716, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7852, loss 0.44713, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7853, loss 0.44710, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7854, loss 0.44709, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7855, loss 0.44705, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7856, loss 0.44703, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7857, loss 0.44701, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7858, loss 0.44698, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7859, loss 0.44696, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7860, loss 0.44695, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7861, loss 0.44692, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7862, loss 0.44689, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7863, loss 0.44688, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7864, loss 0.44685, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7865, loss 0.44684, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7866, loss 0.44682, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7867, loss 0.44680, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7868, loss 0.44678, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7869, loss 0.44676, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7870, loss 0.44674, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7871, loss 0.44672, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7872, loss 0.44670, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7873, loss 0.44668, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7874, loss 0.44667, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7875, loss 0.44665, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7876, loss 0.44663, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7877, loss 0.44661, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7878, loss 0.44660, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7879, loss 0.44658, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7880, loss 0.44656, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7881, loss 0.44655, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7882, loss 0.44653, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7883, loss 0.44651, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 7884, loss 0.44650, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7885, loss 0.44648, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7886, loss 0.44646, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7887, loss 0.44645, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 7888, loss 0.44643, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7889, loss 0.44642, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7890, loss 0.44640, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7891, loss 0.44639, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7892, loss 0.44637, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7893, loss 0.44636, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7894, loss 0.44634, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7895, loss 0.44633, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7896, loss 0.44632, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 7897, loss 0.44630, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7898, loss 0.44629, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7899, loss 0.44627, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7900, loss 0.44626, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7901, loss 0.44625, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7902, loss 0.44623, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7903, loss 0.44622, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7904, loss 0.44620, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7905, loss 0.44619, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7906, loss 0.44618, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7907, loss 0.44617, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7908, loss 0.44615, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7909, loss 0.44614, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7910, loss 0.44613, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7911, loss 0.44611, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7912, loss 0.44610, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7913, loss 0.44609, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7914, loss 0.44608, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7915, loss 0.44606, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7916, loss 0.44605, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7917, loss 0.44604, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7918, loss 0.44603, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7919, loss 0.44601, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7920, loss 0.44600, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7921, loss 0.44599, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7922, loss 0.44598, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7923, loss 0.44597, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7924, loss 0.44596, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7925, loss 0.44594, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7926, loss 0.44593, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7927, loss 0.44592, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7928, loss 0.44591, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7929, loss 0.44590, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7930, loss 0.44589, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7931, loss 0.44588, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7932, loss 0.44587, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7933, loss 0.44585, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7934, loss 0.44584, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7935, loss 0.44583, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7936, loss 0.44582, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7937, loss 0.44581, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7938, loss 0.44580, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7939, loss 0.44579, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7940, loss 0.44578, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7941, loss 0.44577, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7942, loss 0.44576, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7943, loss 0.44575, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7944, loss 0.44574, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7945, loss 0.44573, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7946, loss 0.44572, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7947, loss 0.44570, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7948, loss 0.44569, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7949, loss 0.44568, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7950, loss 0.44567, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7951, loss 0.44566, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7952, loss 0.44565, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7953, loss 0.44564, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7954, loss 0.44563, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7955, loss 0.44562, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7956, loss 0.44561, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7957, loss 0.44560, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7958, loss 0.44559, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7959, loss 0.44558, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7960, loss 0.44557, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7961, loss 0.44557, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7962, loss 0.44556, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7963, loss 0.44555, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7964, loss 0.44554, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7965, loss 0.44553, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7966, loss 0.44552, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7967, loss 0.44551, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7968, loss 0.44550, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7969, loss 0.44549, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7970, loss 0.44548, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7971, loss 0.44547, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7972, loss 0.44546, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7973, loss 0.44545, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7974, loss 0.44544, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7975, loss 0.44543, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7976, loss 0.44542, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7977, loss 0.44541, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7978, loss 0.44540, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7979, loss 0.44540, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7980, loss 0.44539, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7981, loss 0.44538, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7982, loss 0.44537, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7983, loss 0.44536, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7984, loss 0.44535, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7985, loss 0.44534, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7986, loss 0.44533, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7987, loss 0.44532, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7988, loss 0.44531, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7989, loss 0.44530, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7990, loss 0.44530, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7991, loss 0.44529, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7992, loss 0.44528, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7993, loss 0.44527, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7994, loss 0.44526, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7995, loss 0.44525, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7996, loss 0.44524, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7997, loss 0.44523, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7998, loss 0.44522, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 7999, loss 0.44522, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8000, loss 0.44521, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8001, loss 0.44520, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8002, loss 0.44519, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8003, loss 0.44518, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8004, loss 0.44517, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8005, loss 0.44516, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8006, loss 0.44515, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8007, loss 0.44515, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8008, loss 0.44514, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8009, loss 0.44513, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8010, loss 0.44512, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8011, loss 0.44511, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8012, loss 0.44510, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8013, loss 0.44509, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8014, loss 0.44509, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8015, loss 0.44508, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8016, loss 0.44507, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8017, loss 0.44506, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8018, loss 0.44505, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8019, loss 0.44504, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8020, loss 0.44503, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8021, loss 0.44503, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8022, loss 0.44502, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8023, loss 0.44501, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8024, loss 0.44500, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8025, loss 0.44499, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8026, loss 0.44498, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8027, loss 0.44497, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8028, loss 0.44497, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8029, loss 0.44496, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8030, loss 0.44495, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8031, loss 0.44494, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8032, loss 0.44493, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8033, loss 0.44492, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8034, loss 0.44492, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8035, loss 0.44491, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8036, loss 0.44490, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8037, loss 0.44489, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8038, loss 0.44488, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8039, loss 0.44487, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8040, loss 0.44487, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8041, loss 0.44486, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8042, loss 0.44485, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8043, loss 0.44484, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8044, loss 0.44483, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8045, loss 0.44482, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8046, loss 0.44482, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8047, loss 0.44481, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8048, loss 0.44480, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8049, loss 0.44479, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8050, loss 0.44478, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8051, loss 0.44477, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8052, loss 0.44477, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8053, loss 0.44476, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8054, loss 0.44475, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8055, loss 0.44474, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8056, loss 0.44473, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8057, loss 0.44473, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8058, loss 0.44472, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8059, loss 0.44471, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8060, loss 0.44470, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8061, loss 0.44469, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8062, loss 0.44468, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8063, loss 0.44468, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8064, loss 0.44467, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8065, loss 0.44466, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8066, loss 0.44465, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8067, loss 0.44464, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8068, loss 0.44464, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8069, loss 0.44463, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8070, loss 0.44462, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8071, loss 0.44461, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8072, loss 0.44460, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8073, loss 0.44459, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8074, loss 0.44459, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8075, loss 0.44458, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8076, loss 0.44457, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8077, loss 0.44456, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8078, loss 0.44455, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8079, loss 0.44455, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8080, loss 0.44454, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8081, loss 0.44453, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8082, loss 0.44452, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8083, loss 0.44451, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8084, loss 0.44451, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8085, loss 0.44450, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8086, loss 0.44449, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8087, loss 0.44448, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8088, loss 0.44447, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8089, loss 0.44447, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8090, loss 0.44446, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8091, loss 0.44445, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8092, loss 0.44444, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8093, loss 0.44443, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8094, loss 0.44443, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8095, loss 0.44442, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8096, loss 0.44441, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8097, loss 0.44440, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8098, loss 0.44439, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8099, loss 0.44439, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8100, loss 0.44438, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8101, loss 0.44437, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8102, loss 0.44436, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8103, loss 0.44435, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8104, loss 0.44435, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8105, loss 0.44434, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8106, loss 0.44433, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8107, loss 0.44432, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8108, loss 0.44431, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8109, loss 0.44431, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8110, loss 0.44430, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8111, loss 0.44429, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8112, loss 0.44428, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8113, loss 0.44427, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8114, loss 0.44427, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8115, loss 0.44426, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8116, loss 0.44425, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8117, loss 0.44424, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8118, loss 0.44424, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8119, loss 0.44423, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8120, loss 0.44422, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8121, loss 0.44421, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8122, loss 0.44420, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8123, loss 0.44420, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8124, loss 0.44419, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8125, loss 0.44418, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8126, loss 0.44417, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8127, loss 0.44416, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8128, loss 0.44416, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8129, loss 0.44415, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8130, loss 0.44414, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8131, loss 0.44413, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8132, loss 0.44413, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8133, loss 0.44412, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8134, loss 0.44411, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8135, loss 0.44410, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8136, loss 0.44409, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8137, loss 0.44409, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8138, loss 0.44408, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8139, loss 0.44407, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8140, loss 0.44406, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8141, loss 0.44405, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8142, loss 0.44405, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8143, loss 0.44404, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8144, loss 0.44403, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8145, loss 0.44402, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8146, loss 0.44402, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8147, loss 0.44401, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8148, loss 0.44400, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8149, loss 0.44399, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8150, loss 0.44398, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8151, loss 0.44398, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8152, loss 0.44397, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8153, loss 0.44396, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8154, loss 0.44395, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8155, loss 0.44395, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8156, loss 0.44394, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8157, loss 0.44393, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8158, loss 0.44392, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8159, loss 0.44391, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8160, loss 0.44391, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8161, loss 0.44390, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8162, loss 0.44389, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8163, loss 0.44388, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8164, loss 0.44387, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8165, loss 0.44387, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8166, loss 0.44386, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8167, loss 0.44385, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8168, loss 0.44384, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8169, loss 0.44384, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8170, loss 0.44383, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8171, loss 0.44382, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8172, loss 0.44381, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8173, loss 0.44381, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8174, loss 0.44380, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8175, loss 0.44379, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8176, loss 0.44378, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8177, loss 0.44377, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8178, loss 0.44377, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8179, loss 0.44376, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8180, loss 0.44375, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8181, loss 0.44374, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8182, loss 0.44374, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8183, loss 0.44373, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8184, loss 0.44372, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8185, loss 0.44371, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8186, loss 0.44370, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8187, loss 0.44370, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8188, loss 0.44369, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8189, loss 0.44368, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8190, loss 0.44367, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8191, loss 0.44367, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8192, loss 0.44366, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8193, loss 0.44365, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8194, loss 0.44364, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8195, loss 0.44364, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8196, loss 0.44363, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8197, loss 0.44362, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8198, loss 0.44361, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8199, loss 0.44360, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8200, loss 0.44360, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8201, loss 0.44359, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8202, loss 0.44358, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8203, loss 0.44358, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8204, loss 0.44357, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8205, loss 0.44357, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8206, loss 0.44357, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8207, loss 0.44358, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8208, loss 0.44360, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8209, loss 0.44365, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8210, loss 0.44376, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8211, loss 0.44400, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8212, loss 0.44446, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8213, loss 0.44550, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 8214, loss 0.44730, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 8215, loss 0.45161, training accuracy 80.72%, test accuracy 51.49%\n",
            "epoch 8216, loss 0.45754, training accuracy 79.52%, test accuracy 53.66%\n",
            "epoch 8217, loss 0.47290, training accuracy 78.53%, test accuracy 49.11%\n",
            "epoch 8218, loss 0.48213, training accuracy 77.73%, test accuracy 53.27%\n",
            "epoch 8219, loss 0.51244, training accuracy 78.13%, test accuracy 48.32%\n",
            "epoch 8220, loss 0.50397, training accuracy 76.34%, test accuracy 53.27%\n",
            "epoch 8221, loss 0.51667, training accuracy 77.34%, test accuracy 49.70%\n",
            "epoch 8222, loss 0.49337, training accuracy 77.34%, test accuracy 53.27%\n",
            "epoch 8223, loss 0.47596, training accuracy 78.33%, test accuracy 52.08%\n",
            "epoch 8224, loss 0.45011, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 8225, loss 0.44893, training accuracy 79.52%, test accuracy 53.66%\n",
            "epoch 8226, loss 0.47032, training accuracy 78.53%, test accuracy 50.50%\n",
            "epoch 8227, loss 0.47605, training accuracy 78.13%, test accuracy 53.86%\n",
            "epoch 8228, loss 0.47868, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 8229, loss 0.46698, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 8230, loss 0.44921, training accuracy 80.91%, test accuracy 51.68%\n",
            "epoch 8231, loss 0.44979, training accuracy 79.52%, test accuracy 52.28%\n",
            "epoch 8232, loss 0.45720, training accuracy 79.72%, test accuracy 53.86%\n",
            "epoch 8233, loss 0.46435, training accuracy 79.92%, test accuracy 51.49%\n",
            "epoch 8234, loss 0.46479, training accuracy 78.93%, test accuracy 53.66%\n",
            "epoch 8235, loss 0.45261, training accuracy 80.12%, test accuracy 53.47%\n",
            "epoch 8236, loss 0.44745, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 8237, loss 0.44755, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8238, loss 0.45497, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 8239, loss 0.46082, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8240, loss 0.45541, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8241, loss 0.45136, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 8242, loss 0.44613, training accuracy 81.11%, test accuracy 53.27%\n",
            "epoch 8243, loss 0.44765, training accuracy 80.91%, test accuracy 51.68%\n",
            "epoch 8244, loss 0.45096, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8245, loss 0.45253, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8246, loss 0.45232, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 8247, loss 0.44662, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8248, loss 0.44636, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 8249, loss 0.44632, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8250, loss 0.44784, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8251, loss 0.44911, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8252, loss 0.44751, training accuracy 81.11%, test accuracy 53.07%\n",
            "epoch 8253, loss 0.44735, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 8254, loss 0.44417, training accuracy 80.32%, test accuracy 53.66%\n",
            "epoch 8255, loss 0.44558, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8256, loss 0.44546, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8257, loss 0.44609, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8258, loss 0.44607, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8259, loss 0.44445, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8260, loss 0.44492, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8261, loss 0.44362, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8262, loss 0.44483, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8263, loss 0.44449, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8264, loss 0.44490, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8265, loss 0.44455, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8266, loss 0.44393, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8267, loss 0.44406, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8268, loss 0.44346, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8269, loss 0.44398, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8270, loss 0.44368, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8271, loss 0.44399, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8272, loss 0.44377, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8273, loss 0.44369, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8274, loss 0.44358, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8275, loss 0.44344, training accuracy 80.52%, test accuracy 53.66%\n",
            "epoch 8276, loss 0.44350, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8277, loss 0.44344, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8278, loss 0.44356, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8279, loss 0.44347, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8280, loss 0.44352, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8281, loss 0.44340, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8282, loss 0.44339, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8283, loss 0.44330, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8284, loss 0.44328, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8285, loss 0.44329, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8286, loss 0.44327, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8287, loss 0.44332, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8288, loss 0.44327, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8289, loss 0.44331, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8290, loss 0.44325, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8291, loss 0.44323, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8292, loss 0.44320, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8293, loss 0.44316, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 8294, loss 0.44317, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8295, loss 0.44313, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8296, loss 0.44315, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8297, loss 0.44313, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8298, loss 0.44313, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8299, loss 0.44313, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8300, loss 0.44311, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8301, loss 0.44312, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8302, loss 0.44309, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8303, loss 0.44309, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8304, loss 0.44306, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8305, loss 0.44306, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8306, loss 0.44304, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8307, loss 0.44303, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8308, loss 0.44302, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8309, loss 0.44300, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8310, loss 0.44300, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8311, loss 0.44299, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8312, loss 0.44298, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8313, loss 0.44297, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8314, loss 0.44296, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8315, loss 0.44296, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8316, loss 0.44295, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8317, loss 0.44294, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8318, loss 0.44293, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8319, loss 0.44293, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8320, loss 0.44292, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8321, loss 0.44291, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8322, loss 0.44291, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8323, loss 0.44291, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8324, loss 0.44291, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8325, loss 0.44291, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8326, loss 0.44292, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8327, loss 0.44293, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8328, loss 0.44296, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8329, loss 0.44301, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8330, loss 0.44309, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8331, loss 0.44323, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8332, loss 0.44343, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8333, loss 0.44380, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8334, loss 0.44432, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 8335, loss 0.44533, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8336, loss 0.44662, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8337, loss 0.44933, training accuracy 80.32%, test accuracy 51.68%\n",
            "epoch 8338, loss 0.45209, training accuracy 80.32%, test accuracy 53.86%\n",
            "epoch 8339, loss 0.45852, training accuracy 80.52%, test accuracy 50.89%\n",
            "epoch 8340, loss 0.46221, training accuracy 79.92%, test accuracy 53.66%\n",
            "epoch 8341, loss 0.47294, training accuracy 78.53%, test accuracy 50.10%\n",
            "epoch 8342, loss 0.47196, training accuracy 78.13%, test accuracy 53.66%\n",
            "epoch 8343, loss 0.47889, training accuracy 78.13%, test accuracy 49.70%\n",
            "epoch 8344, loss 0.46900, training accuracy 78.33%, test accuracy 53.47%\n",
            "epoch 8345, loss 0.46389, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 8346, loss 0.45181, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8347, loss 0.44516, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8348, loss 0.44297, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8349, loss 0.44533, training accuracy 79.72%, test accuracy 53.07%\n",
            "epoch 8350, loss 0.45056, training accuracy 80.52%, test accuracy 51.68%\n",
            "epoch 8351, loss 0.45442, training accuracy 80.72%, test accuracy 53.86%\n",
            "epoch 8352, loss 0.45882, training accuracy 80.32%, test accuracy 51.88%\n",
            "epoch 8353, loss 0.45625, training accuracy 79.92%, test accuracy 53.47%\n",
            "epoch 8354, loss 0.45393, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8355, loss 0.44778, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 8356, loss 0.44401, training accuracy 80.91%, test accuracy 53.07%\n",
            "epoch 8357, loss 0.44293, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8358, loss 0.44438, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8359, loss 0.44719, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8360, loss 0.44900, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8361, loss 0.45028, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8362, loss 0.44843, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8363, loss 0.44649, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8364, loss 0.44414, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8365, loss 0.44291, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8366, loss 0.44291, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8367, loss 0.44378, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8368, loss 0.44503, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8369, loss 0.44577, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8370, loss 0.44624, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8371, loss 0.44560, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8372, loss 0.44487, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8373, loss 0.44383, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8374, loss 0.44310, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8375, loss 0.44270, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8376, loss 0.44268, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8377, loss 0.44294, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 8378, loss 0.44332, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8379, loss 0.44372, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8380, loss 0.44394, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 8381, loss 0.44411, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 8382, loss 0.44399, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 8383, loss 0.44385, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8384, loss 0.44355, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8385, loss 0.44331, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8386, loss 0.44303, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8387, loss 0.44283, training accuracy 80.91%, test accuracy 53.27%\n",
            "epoch 8388, loss 0.44267, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8389, loss 0.44257, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8390, loss 0.44252, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8391, loss 0.44251, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8392, loss 0.44252, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8393, loss 0.44255, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8394, loss 0.44260, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8395, loss 0.44266, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8396, loss 0.44274, training accuracy 80.91%, test accuracy 53.27%\n",
            "epoch 8397, loss 0.44281, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8398, loss 0.44291, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8399, loss 0.44302, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8400, loss 0.44319, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8401, loss 0.44337, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8402, loss 0.44367, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8403, loss 0.44397, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 8404, loss 0.44454, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 8405, loss 0.44507, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8406, loss 0.44614, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8407, loss 0.44703, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8408, loss 0.44900, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8409, loss 0.45028, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 8410, loss 0.45349, training accuracy 80.52%, test accuracy 51.88%\n",
            "epoch 8411, loss 0.45458, training accuracy 80.52%, test accuracy 53.66%\n",
            "epoch 8412, loss 0.45858, training accuracy 80.52%, test accuracy 51.88%\n",
            "epoch 8413, loss 0.45802, training accuracy 80.12%, test accuracy 53.66%\n",
            "epoch 8414, loss 0.46070, training accuracy 79.92%, test accuracy 51.88%\n",
            "epoch 8415, loss 0.45733, training accuracy 79.92%, test accuracy 53.47%\n",
            "epoch 8416, loss 0.45645, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 8417, loss 0.45140, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8418, loss 0.44820, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8419, loss 0.44477, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 8420, loss 0.44293, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8421, loss 0.44237, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8422, loss 0.44287, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8423, loss 0.44412, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8424, loss 0.44553, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8425, loss 0.44730, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8426, loss 0.44802, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8427, loss 0.44909, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8428, loss 0.44838, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8429, loss 0.44803, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8430, loss 0.44641, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8431, loss 0.44523, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8432, loss 0.44385, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 8433, loss 0.44295, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8434, loss 0.44242, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8435, loss 0.44226, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8436, loss 0.44240, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8437, loss 0.44274, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8438, loss 0.44322, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8439, loss 0.44368, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 8440, loss 0.44427, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 8441, loss 0.44464, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8442, loss 0.44523, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8443, loss 0.44542, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8444, loss 0.44594, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8445, loss 0.44595, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8446, loss 0.44638, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8447, loss 0.44621, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8448, loss 0.44652, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8449, loss 0.44621, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8450, loss 0.44639, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8451, loss 0.44598, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8452, loss 0.44605, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8453, loss 0.44560, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8454, loss 0.44559, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8455, loss 0.44518, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8456, loss 0.44515, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8457, loss 0.44482, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8458, loss 0.44483, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 8459, loss 0.44459, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8460, loss 0.44467, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 8461, loss 0.44453, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8462, loss 0.44470, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 8463, loss 0.44466, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8464, loss 0.44494, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8465, loss 0.44498, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8466, loss 0.44541, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8467, loss 0.44552, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8468, loss 0.44615, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8469, loss 0.44634, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8470, loss 0.44723, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8471, loss 0.44747, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8472, loss 0.44866, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8473, loss 0.44885, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8474, loss 0.45027, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8475, loss 0.45021, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8476, loss 0.45159, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 8477, loss 0.45097, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8478, loss 0.45181, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 8479, loss 0.45044, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8480, loss 0.45032, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8481, loss 0.44844, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8482, loss 0.44749, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8483, loss 0.44572, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8484, loss 0.44460, training accuracy 81.31%, test accuracy 52.48%\n",
            "epoch 8485, loss 0.44344, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 8486, loss 0.44271, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8487, loss 0.44221, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8488, loss 0.44197, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8489, loss 0.44192, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8490, loss 0.44200, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8491, loss 0.44220, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8492, loss 0.44245, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8493, loss 0.44280, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8494, loss 0.44315, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 8495, loss 0.44365, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8496, loss 0.44408, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8497, loss 0.44482, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 8498, loss 0.44537, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8499, loss 0.44650, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8500, loss 0.44719, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8501, loss 0.44890, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8502, loss 0.44967, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8503, loss 0.45203, training accuracy 80.52%, test accuracy 51.68%\n",
            "epoch 8504, loss 0.45257, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8505, loss 0.45532, training accuracy 80.52%, test accuracy 51.09%\n",
            "epoch 8506, loss 0.45496, training accuracy 80.72%, test accuracy 53.66%\n",
            "epoch 8507, loss 0.45710, training accuracy 80.52%, test accuracy 51.09%\n",
            "epoch 8508, loss 0.45514, training accuracy 80.72%, test accuracy 53.47%\n",
            "epoch 8509, loss 0.45532, training accuracy 80.52%, test accuracy 51.49%\n",
            "epoch 8510, loss 0.45184, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8511, loss 0.44984, training accuracy 80.72%, test accuracy 52.08%\n",
            "epoch 8512, loss 0.44643, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8513, loss 0.44421, training accuracy 81.31%, test accuracy 52.28%\n",
            "epoch 8514, loss 0.44254, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8515, loss 0.44184, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8516, loss 0.44192, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8517, loss 0.44254, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8518, loss 0.44354, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 8519, loss 0.44449, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8520, loss 0.44565, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 8521, loss 0.44614, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8522, loss 0.44690, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8523, loss 0.44663, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8524, loss 0.44669, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8525, loss 0.44582, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8526, loss 0.44529, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 8527, loss 0.44429, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8528, loss 0.44359, training accuracy 81.31%, test accuracy 51.88%\n",
            "epoch 8529, loss 0.44285, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8530, loss 0.44235, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 8531, loss 0.44197, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8532, loss 0.44175, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8533, loss 0.44166, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 8534, loss 0.44165, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8535, loss 0.44170, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 8536, loss 0.44181, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8537, loss 0.44196, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8538, loss 0.44214, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8539, loss 0.44240, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 8540, loss 0.44269, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8541, loss 0.44314, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8542, loss 0.44361, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 8543, loss 0.44445, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8544, loss 0.44525, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 8545, loss 0.44684, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8546, loss 0.44814, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8547, loss 0.45108, training accuracy 80.72%, test accuracy 51.68%\n",
            "epoch 8548, loss 0.45291, training accuracy 80.91%, test accuracy 53.86%\n",
            "epoch 8549, loss 0.45783, training accuracy 80.52%, test accuracy 51.49%\n",
            "epoch 8550, loss 0.45964, training accuracy 79.52%, test accuracy 53.86%\n",
            "epoch 8551, loss 0.46652, training accuracy 78.93%, test accuracy 50.10%\n",
            "epoch 8552, loss 0.46708, training accuracy 79.13%, test accuracy 53.66%\n",
            "epoch 8553, loss 0.47438, training accuracy 78.13%, test accuracy 49.50%\n",
            "epoch 8554, loss 0.47255, training accuracy 78.13%, test accuracy 53.66%\n",
            "epoch 8555, loss 0.47637, training accuracy 77.53%, test accuracy 50.30%\n",
            "epoch 8556, loss 0.46938, training accuracy 78.33%, test accuracy 53.66%\n",
            "epoch 8557, loss 0.46396, training accuracy 79.72%, test accuracy 51.68%\n",
            "epoch 8558, loss 0.45191, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8559, loss 0.44453, training accuracy 80.72%, test accuracy 51.88%\n",
            "epoch 8560, loss 0.44190, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8561, loss 0.44513, training accuracy 79.92%, test accuracy 53.47%\n",
            "epoch 8562, loss 0.45119, training accuracy 80.52%, test accuracy 51.49%\n",
            "epoch 8563, loss 0.45366, training accuracy 80.91%, test accuracy 53.47%\n",
            "epoch 8564, loss 0.45553, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8565, loss 0.45212, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8566, loss 0.44812, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8567, loss 0.44374, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8568, loss 0.44178, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8569, loss 0.44282, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8570, loss 0.44506, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8571, loss 0.44720, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8572, loss 0.44738, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8573, loss 0.44623, training accuracy 81.31%, test accuracy 52.28%\n",
            "epoch 8574, loss 0.44396, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 8575, loss 0.44229, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8576, loss 0.44158, training accuracy 80.52%, test accuracy 51.88%\n",
            "epoch 8577, loss 0.44221, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8578, loss 0.44327, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8579, loss 0.44417, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 8580, loss 0.44455, training accuracy 81.11%, test accuracy 52.28%\n",
            "epoch 8581, loss 0.44397, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8582, loss 0.44323, training accuracy 81.31%, test accuracy 52.28%\n",
            "epoch 8583, loss 0.44217, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8584, loss 0.44160, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8585, loss 0.44148, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8586, loss 0.44174, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8587, loss 0.44223, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8588, loss 0.44262, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 8589, loss 0.44294, training accuracy 81.31%, test accuracy 52.28%\n",
            "epoch 8590, loss 0.44282, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 8591, loss 0.44261, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 8592, loss 0.44222, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 8593, loss 0.44184, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8594, loss 0.44155, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8595, loss 0.44136, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8596, loss 0.44133, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8597, loss 0.44140, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8598, loss 0.44151, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8599, loss 0.44166, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8600, loss 0.44180, training accuracy 81.11%, test accuracy 51.88%\n",
            "epoch 8601, loss 0.44189, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8602, loss 0.44197, training accuracy 80.72%, test accuracy 51.88%\n",
            "epoch 8603, loss 0.44196, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8604, loss 0.44196, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 8605, loss 0.44189, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8606, loss 0.44184, training accuracy 80.72%, test accuracy 51.88%\n",
            "epoch 8607, loss 0.44174, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8608, loss 0.44167, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8609, loss 0.44159, training accuracy 80.32%, test accuracy 52.08%\n",
            "epoch 8610, loss 0.44153, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 8611, loss 0.44147, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8612, loss 0.44143, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8613, loss 0.44138, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8614, loss 0.44136, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8615, loss 0.44134, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8616, loss 0.44133, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8617, loss 0.44133, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8618, loss 0.44134, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8619, loss 0.44136, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8620, loss 0.44141, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 8621, loss 0.44147, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8622, loss 0.44158, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 8623, loss 0.44172, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8624, loss 0.44196, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 8625, loss 0.44228, training accuracy 79.52%, test accuracy 52.87%\n",
            "epoch 8626, loss 0.44285, training accuracy 81.31%, test accuracy 52.48%\n",
            "epoch 8627, loss 0.44354, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8628, loss 0.44489, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 8629, loss 0.44636, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8630, loss 0.44946, training accuracy 80.72%, test accuracy 51.68%\n",
            "epoch 8631, loss 0.45235, training accuracy 80.72%, test accuracy 53.86%\n",
            "epoch 8632, loss 0.45926, training accuracy 80.32%, test accuracy 50.30%\n",
            "epoch 8633, loss 0.46389, training accuracy 79.72%, test accuracy 53.66%\n",
            "epoch 8634, loss 0.47797, training accuracy 78.33%, test accuracy 49.11%\n",
            "epoch 8635, loss 0.48691, training accuracy 77.53%, test accuracy 53.47%\n",
            "epoch 8636, loss 0.52105, training accuracy 76.94%, test accuracy 46.73%\n",
            "epoch 8637, loss 0.55887, training accuracy 73.76%, test accuracy 54.65%\n",
            "epoch 8638, loss 0.67475, training accuracy 75.15%, test accuracy 47.13%\n",
            "epoch 8639, loss 0.53199, training accuracy 73.36%, test accuracy 53.27%\n",
            "epoch 8640, loss 0.47591, training accuracy 78.33%, test accuracy 53.47%\n",
            "epoch 8641, loss 0.47103, training accuracy 79.52%, test accuracy 46.34%\n",
            "epoch 8642, loss 0.56954, training accuracy 70.78%, test accuracy 54.65%\n",
            "epoch 8643, loss 0.68607, training accuracy 74.75%, test accuracy 52.67%\n",
            "epoch 8644, loss 0.58790, training accuracy 76.74%, test accuracy 47.72%\n",
            "epoch 8645, loss 0.54704, training accuracy 72.17%, test accuracy 53.47%\n",
            "epoch 8646, loss 0.68488, training accuracy 74.16%, test accuracy 52.08%\n",
            "epoch 8647, loss 0.60113, training accuracy 76.94%, test accuracy 54.26%\n",
            "epoch 8648, loss 0.55172, training accuracy 77.73%, test accuracy 53.47%\n",
            "epoch 8649, loss 0.56825, training accuracy 76.74%, test accuracy 45.74%\n",
            "epoch 8650, loss 0.56452, training accuracy 68.79%, test accuracy 53.47%\n",
            "epoch 8651, loss 0.51496, training accuracy 76.14%, test accuracy 54.85%\n",
            "epoch 8652, loss 0.52658, training accuracy 77.14%, test accuracy 46.14%\n",
            "epoch 8653, loss 0.55531, training accuracy 72.76%, test accuracy 54.26%\n",
            "epoch 8654, loss 0.50344, training accuracy 78.73%, test accuracy 52.87%\n",
            "epoch 8655, loss 0.50860, training accuracy 77.14%, test accuracy 51.88%\n",
            "epoch 8656, loss 0.46693, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 8657, loss 0.51203, training accuracy 77.93%, test accuracy 51.68%\n",
            "epoch 8658, loss 0.45014, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8659, loss 0.49884, training accuracy 78.13%, test accuracy 54.06%\n",
            "epoch 8660, loss 0.46447, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 8661, loss 0.47660, training accuracy 78.73%, test accuracy 51.68%\n",
            "epoch 8662, loss 0.46377, training accuracy 79.52%, test accuracy 53.27%\n",
            "epoch 8663, loss 0.48096, training accuracy 79.13%, test accuracy 53.86%\n",
            "epoch 8664, loss 0.45594, training accuracy 79.72%, test accuracy 50.50%\n",
            "epoch 8665, loss 0.47325, training accuracy 79.32%, test accuracy 52.87%\n",
            "epoch 8666, loss 0.46169, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 8667, loss 0.46668, training accuracy 79.72%, test accuracy 52.28%\n",
            "epoch 8668, loss 0.45228, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8669, loss 0.46669, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 8670, loss 0.45111, training accuracy 80.52%, test accuracy 51.88%\n",
            "epoch 8671, loss 0.46227, training accuracy 79.92%, test accuracy 53.27%\n",
            "epoch 8672, loss 0.45845, training accuracy 79.72%, test accuracy 51.09%\n",
            "epoch 8673, loss 0.46648, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8674, loss 0.44830, training accuracy 80.72%, test accuracy 54.06%\n",
            "epoch 8675, loss 0.45881, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 8676, loss 0.44794, training accuracy 80.12%, test accuracy 50.89%\n",
            "epoch 8677, loss 0.45332, training accuracy 79.52%, test accuracy 51.88%\n",
            "epoch 8678, loss 0.45096, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8679, loss 0.45265, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 8680, loss 0.44515, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 8681, loss 0.45165, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8682, loss 0.44661, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 8683, loss 0.44823, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 8684, loss 0.44591, training accuracy 80.72%, test accuracy 51.49%\n",
            "epoch 8685, loss 0.44788, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 8686, loss 0.44397, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 8687, loss 0.44618, training accuracy 80.91%, test accuracy 53.27%\n",
            "epoch 8688, loss 0.44446, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8689, loss 0.44536, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 8690, loss 0.44361, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 8691, loss 0.44492, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8692, loss 0.44367, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 8693, loss 0.44377, training accuracy 80.32%, test accuracy 51.88%\n",
            "epoch 8694, loss 0.44331, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 8695, loss 0.44421, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 8696, loss 0.44291, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 8697, loss 0.44323, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 8698, loss 0.44301, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8699, loss 0.44302, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8700, loss 0.44258, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8701, loss 0.44315, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 8702, loss 0.44254, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8703, loss 0.44261, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8704, loss 0.44239, training accuracy 80.72%, test accuracy 51.88%\n",
            "epoch 8705, loss 0.44251, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8706, loss 0.44212, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8707, loss 0.44236, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8708, loss 0.44214, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8709, loss 0.44223, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8710, loss 0.44194, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8711, loss 0.44212, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8712, loss 0.44189, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8713, loss 0.44197, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8714, loss 0.44184, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8715, loss 0.44193, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 8716, loss 0.44171, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 8717, loss 0.44180, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8718, loss 0.44170, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 8719, loss 0.44173, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 8720, loss 0.44163, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8721, loss 0.44167, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8722, loss 0.44159, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8723, loss 0.44161, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8724, loss 0.44151, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8725, loss 0.44154, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8726, loss 0.44150, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 8727, loss 0.44149, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8728, loss 0.44143, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8729, loss 0.44145, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8730, loss 0.44140, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8731, loss 0.44139, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8732, loss 0.44136, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8733, loss 0.44136, training accuracy 80.32%, test accuracy 52.28%\n",
            "epoch 8734, loss 0.44133, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8735, loss 0.44131, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8736, loss 0.44129, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8737, loss 0.44129, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8738, loss 0.44126, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8739, loss 0.44125, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 8740, loss 0.44123, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8741, loss 0.44122, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8742, loss 0.44120, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8743, loss 0.44118, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8744, loss 0.44117, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8745, loss 0.44116, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8746, loss 0.44114, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8747, loss 0.44113, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8748, loss 0.44111, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8749, loss 0.44110, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8750, loss 0.44108, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8751, loss 0.44107, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8752, loss 0.44106, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8753, loss 0.44105, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8754, loss 0.44103, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8755, loss 0.44102, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8756, loss 0.44101, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8757, loss 0.44100, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8758, loss 0.44098, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8759, loss 0.44097, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8760, loss 0.44096, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8761, loss 0.44095, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8762, loss 0.44094, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8763, loss 0.44093, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8764, loss 0.44092, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8765, loss 0.44090, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8766, loss 0.44089, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8767, loss 0.44088, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8768, loss 0.44087, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8769, loss 0.44086, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8770, loss 0.44085, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8771, loss 0.44084, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8772, loss 0.44083, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8773, loss 0.44082, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8774, loss 0.44081, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8775, loss 0.44080, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8776, loss 0.44079, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8777, loss 0.44078, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8778, loss 0.44077, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8779, loss 0.44076, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8780, loss 0.44075, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8781, loss 0.44074, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8782, loss 0.44073, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8783, loss 0.44072, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8784, loss 0.44071, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8785, loss 0.44071, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8786, loss 0.44070, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8787, loss 0.44069, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8788, loss 0.44068, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8789, loss 0.44067, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8790, loss 0.44066, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8791, loss 0.44065, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8792, loss 0.44064, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8793, loss 0.44063, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8794, loss 0.44062, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8795, loss 0.44062, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8796, loss 0.44061, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8797, loss 0.44060, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8798, loss 0.44059, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8799, loss 0.44058, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8800, loss 0.44057, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8801, loss 0.44056, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8802, loss 0.44056, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8803, loss 0.44055, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8804, loss 0.44054, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8805, loss 0.44053, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8806, loss 0.44052, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8807, loss 0.44051, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8808, loss 0.44051, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8809, loss 0.44050, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8810, loss 0.44049, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8811, loss 0.44048, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8812, loss 0.44047, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8813, loss 0.44047, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8814, loss 0.44046, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8815, loss 0.44045, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8816, loss 0.44044, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8817, loss 0.44043, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8818, loss 0.44042, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8819, loss 0.44042, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8820, loss 0.44041, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8821, loss 0.44040, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8822, loss 0.44039, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8823, loss 0.44039, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8824, loss 0.44038, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8825, loss 0.44037, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8826, loss 0.44036, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8827, loss 0.44035, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8828, loss 0.44035, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8829, loss 0.44034, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8830, loss 0.44033, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8831, loss 0.44032, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8832, loss 0.44032, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8833, loss 0.44031, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8834, loss 0.44030, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8835, loss 0.44029, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8836, loss 0.44029, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8837, loss 0.44028, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8838, loss 0.44027, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8839, loss 0.44026, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8840, loss 0.44026, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8841, loss 0.44025, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8842, loss 0.44024, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8843, loss 0.44023, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8844, loss 0.44023, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8845, loss 0.44022, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8846, loss 0.44021, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8847, loss 0.44020, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8848, loss 0.44020, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8849, loss 0.44019, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8850, loss 0.44018, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8851, loss 0.44017, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8852, loss 0.44017, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8853, loss 0.44016, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8854, loss 0.44015, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8855, loss 0.44014, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8856, loss 0.44014, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8857, loss 0.44013, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8858, loss 0.44012, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8859, loss 0.44011, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8860, loss 0.44011, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8861, loss 0.44010, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8862, loss 0.44009, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8863, loss 0.44008, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8864, loss 0.44008, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8865, loss 0.44007, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8866, loss 0.44006, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8867, loss 0.44006, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8868, loss 0.44005, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8869, loss 0.44004, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8870, loss 0.44003, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8871, loss 0.44003, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8872, loss 0.44002, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8873, loss 0.44001, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8874, loss 0.44000, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8875, loss 0.44000, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8876, loss 0.43999, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8877, loss 0.43998, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8878, loss 0.43998, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8879, loss 0.43997, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8880, loss 0.43996, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8881, loss 0.43995, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8882, loss 0.43995, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8883, loss 0.43994, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8884, loss 0.43993, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8885, loss 0.43993, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8886, loss 0.43992, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8887, loss 0.43991, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8888, loss 0.43990, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8889, loss 0.43990, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8890, loss 0.43989, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8891, loss 0.43988, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8892, loss 0.43988, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8893, loss 0.43987, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8894, loss 0.43986, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8895, loss 0.43985, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8896, loss 0.43985, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8897, loss 0.43984, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8898, loss 0.43983, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8899, loss 0.43983, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8900, loss 0.43982, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8901, loss 0.43981, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8902, loss 0.43980, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8903, loss 0.43980, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8904, loss 0.43979, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8905, loss 0.43978, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8906, loss 0.43978, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8907, loss 0.43977, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8908, loss 0.43976, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8909, loss 0.43975, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8910, loss 0.43975, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8911, loss 0.43974, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8912, loss 0.43973, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8913, loss 0.43973, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8914, loss 0.43972, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8915, loss 0.43971, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8916, loss 0.43971, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8917, loss 0.43970, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8918, loss 0.43969, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8919, loss 0.43968, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8920, loss 0.43968, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8921, loss 0.43967, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8922, loss 0.43966, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8923, loss 0.43966, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8924, loss 0.43965, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8925, loss 0.43964, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8926, loss 0.43963, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8927, loss 0.43963, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8928, loss 0.43962, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8929, loss 0.43961, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8930, loss 0.43961, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8931, loss 0.43960, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8932, loss 0.43959, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8933, loss 0.43959, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8934, loss 0.43958, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8935, loss 0.43957, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8936, loss 0.43956, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8937, loss 0.43956, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8938, loss 0.43955, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8939, loss 0.43954, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8940, loss 0.43954, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8941, loss 0.43953, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8942, loss 0.43952, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8943, loss 0.43952, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8944, loss 0.43951, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8945, loss 0.43950, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8946, loss 0.43950, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 8947, loss 0.43949, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8948, loss 0.43949, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 8949, loss 0.43949, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 8950, loss 0.43950, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 8951, loss 0.43952, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 8952, loss 0.43957, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 8953, loss 0.43966, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 8954, loss 0.43982, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 8955, loss 0.44015, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 8956, loss 0.44073, training accuracy 79.52%, test accuracy 52.67%\n",
            "epoch 8957, loss 0.44196, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 8958, loss 0.44391, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 8959, loss 0.44834, training accuracy 80.91%, test accuracy 51.49%\n",
            "epoch 8960, loss 0.45386, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 8961, loss 0.46772, training accuracy 78.73%, test accuracy 48.91%\n",
            "epoch 8962, loss 0.47654, training accuracy 77.93%, test accuracy 53.07%\n",
            "epoch 8963, loss 0.50572, training accuracy 78.33%, test accuracy 47.52%\n",
            "epoch 8964, loss 0.51167, training accuracy 75.75%, test accuracy 53.47%\n",
            "epoch 8965, loss 0.55506, training accuracy 76.34%, test accuracy 47.33%\n",
            "epoch 8966, loss 0.54012, training accuracy 73.96%, test accuracy 53.47%\n",
            "epoch 8967, loss 0.54116, training accuracy 76.94%, test accuracy 51.09%\n",
            "epoch 8968, loss 0.46688, training accuracy 78.73%, test accuracy 52.87%\n",
            "epoch 8969, loss 0.44417, training accuracy 79.92%, test accuracy 53.47%\n",
            "epoch 8970, loss 0.47787, training accuracy 78.53%, test accuracy 47.33%\n",
            "epoch 8971, loss 0.50064, training accuracy 75.94%, test accuracy 52.87%\n",
            "epoch 8972, loss 0.51747, training accuracy 77.34%, test accuracy 51.09%\n",
            "epoch 8973, loss 0.47692, training accuracy 79.72%, test accuracy 52.48%\n",
            "epoch 8974, loss 0.44506, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 8975, loss 0.46783, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 8976, loss 0.50983, training accuracy 77.53%, test accuracy 54.46%\n",
            "epoch 8977, loss 0.52093, training accuracy 76.54%, test accuracy 50.10%\n",
            "epoch 8978, loss 0.47770, training accuracy 77.73%, test accuracy 51.88%\n",
            "epoch 8979, loss 0.44896, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 8980, loss 0.48778, training accuracy 78.73%, test accuracy 49.31%\n",
            "epoch 8981, loss 0.48653, training accuracy 77.14%, test accuracy 53.66%\n",
            "epoch 8982, loss 0.47712, training accuracy 79.32%, test accuracy 54.06%\n",
            "epoch 8983, loss 0.46598, training accuracy 78.93%, test accuracy 51.68%\n",
            "epoch 8984, loss 0.45413, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 8985, loss 0.50646, training accuracy 78.53%, test accuracy 51.68%\n",
            "epoch 8986, loss 0.50211, training accuracy 78.53%, test accuracy 53.86%\n",
            "epoch 8987, loss 0.48832, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 8988, loss 0.45706, training accuracy 79.72%, test accuracy 50.89%\n",
            "epoch 8989, loss 0.49308, training accuracy 77.34%, test accuracy 54.06%\n",
            "epoch 8990, loss 0.49127, training accuracy 78.33%, test accuracy 52.67%\n",
            "epoch 8991, loss 0.47850, training accuracy 79.92%, test accuracy 48.51%\n",
            "epoch 8992, loss 0.47314, training accuracy 78.13%, test accuracy 53.27%\n",
            "epoch 8993, loss 0.50720, training accuracy 77.14%, test accuracy 53.27%\n",
            "epoch 8994, loss 0.49444, training accuracy 78.33%, test accuracy 52.87%\n",
            "epoch 8995, loss 0.48538, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 8996, loss 0.48856, training accuracy 79.13%, test accuracy 50.69%\n",
            "epoch 8997, loss 0.50664, training accuracy 77.53%, test accuracy 53.86%\n",
            "epoch 8998, loss 0.47381, training accuracy 78.93%, test accuracy 53.27%\n",
            "epoch 8999, loss 0.49241, training accuracy 79.32%, test accuracy 48.12%\n",
            "epoch 9000, loss 0.48042, training accuracy 77.53%, test accuracy 52.67%\n",
            "epoch 9001, loss 0.50314, training accuracy 77.53%, test accuracy 52.67%\n",
            "epoch 9002, loss 0.47276, training accuracy 79.13%, test accuracy 51.29%\n",
            "epoch 9003, loss 0.48298, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 9004, loss 0.47295, training accuracy 80.32%, test accuracy 51.49%\n",
            "epoch 9005, loss 0.48067, training accuracy 77.53%, test accuracy 51.68%\n",
            "epoch 9006, loss 0.45712, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9007, loss 0.47735, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 9008, loss 0.46285, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 9009, loss 0.46363, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9010, loss 0.45488, training accuracy 80.12%, test accuracy 50.89%\n",
            "epoch 9011, loss 0.45789, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9012, loss 0.45594, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 9013, loss 0.45177, training accuracy 80.32%, test accuracy 51.88%\n",
            "epoch 9014, loss 0.45309, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9015, loss 0.45205, training accuracy 80.72%, test accuracy 51.68%\n",
            "epoch 9016, loss 0.45140, training accuracy 79.92%, test accuracy 52.08%\n",
            "epoch 9017, loss 0.44582, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 9018, loss 0.45034, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9019, loss 0.44446, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9020, loss 0.44899, training accuracy 79.13%, test accuracy 52.28%\n",
            "epoch 9021, loss 0.44231, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 9022, loss 0.44878, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9023, loss 0.44236, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9024, loss 0.44714, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 9025, loss 0.44178, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9026, loss 0.44647, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9027, loss 0.44121, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9028, loss 0.44495, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9029, loss 0.44176, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9030, loss 0.44391, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9031, loss 0.44139, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9032, loss 0.44295, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9033, loss 0.44143, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 9034, loss 0.44229, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9035, loss 0.44103, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9036, loss 0.44180, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9037, loss 0.44098, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 9038, loss 0.44133, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9039, loss 0.44055, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 9040, loss 0.44122, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9041, loss 0.44043, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9042, loss 0.44096, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9043, loss 0.44009, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9044, loss 0.44096, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9045, loss 0.44005, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 9046, loss 0.44064, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9047, loss 0.43989, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9048, loss 0.44057, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9049, loss 0.43988, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9050, loss 0.44029, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9051, loss 0.43981, training accuracy 80.32%, test accuracy 52.08%\n",
            "epoch 9052, loss 0.44016, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9053, loss 0.43983, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9054, loss 0.43996, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9055, loss 0.43979, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9056, loss 0.43987, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9057, loss 0.43978, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9058, loss 0.43976, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9059, loss 0.43972, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 9060, loss 0.43971, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9061, loss 0.43969, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9062, loss 0.43965, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9063, loss 0.43962, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9064, loss 0.43962, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9065, loss 0.43958, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9066, loss 0.43957, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9067, loss 0.43952, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9068, loss 0.43954, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9069, loss 0.43949, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9070, loss 0.43950, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9071, loss 0.43945, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9072, loss 0.43946, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9073, loss 0.43943, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9074, loss 0.43943, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9075, loss 0.43939, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9076, loss 0.43939, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9077, loss 0.43937, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9078, loss 0.43936, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9079, loss 0.43935, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9080, loss 0.43933, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9081, loss 0.43932, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9082, loss 0.43931, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9083, loss 0.43930, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9084, loss 0.43928, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9085, loss 0.43927, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9086, loss 0.43926, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9087, loss 0.43925, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9088, loss 0.43923, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9089, loss 0.43922, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9090, loss 0.43921, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9091, loss 0.43920, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9092, loss 0.43919, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9093, loss 0.43918, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9094, loss 0.43917, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9095, loss 0.43916, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9096, loss 0.43915, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9097, loss 0.43914, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9098, loss 0.43913, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9099, loss 0.43912, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9100, loss 0.43911, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9101, loss 0.43910, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9102, loss 0.43909, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9103, loss 0.43908, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9104, loss 0.43907, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9105, loss 0.43906, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9106, loss 0.43905, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9107, loss 0.43904, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9108, loss 0.43903, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9109, loss 0.43902, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9110, loss 0.43902, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9111, loss 0.43901, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9112, loss 0.43900, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9113, loss 0.43899, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9114, loss 0.43898, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9115, loss 0.43897, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9116, loss 0.43896, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9117, loss 0.43895, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9118, loss 0.43895, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9119, loss 0.43894, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9120, loss 0.43893, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9121, loss 0.43892, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9122, loss 0.43891, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9123, loss 0.43890, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9124, loss 0.43889, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9125, loss 0.43889, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9126, loss 0.43888, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9127, loss 0.43887, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9128, loss 0.43886, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9129, loss 0.43885, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9130, loss 0.43885, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9131, loss 0.43884, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9132, loss 0.43883, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9133, loss 0.43882, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9134, loss 0.43881, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9135, loss 0.43881, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9136, loss 0.43880, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9137, loss 0.43879, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9138, loss 0.43878, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9139, loss 0.43877, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9140, loss 0.43877, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9141, loss 0.43876, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9142, loss 0.43875, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9143, loss 0.43874, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9144, loss 0.43874, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9145, loss 0.43873, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9146, loss 0.43872, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9147, loss 0.43871, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9148, loss 0.43870, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9149, loss 0.43870, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9150, loss 0.43869, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9151, loss 0.43868, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9152, loss 0.43867, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9153, loss 0.43867, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9154, loss 0.43866, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9155, loss 0.43865, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9156, loss 0.43864, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9157, loss 0.43864, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9158, loss 0.43863, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9159, loss 0.43862, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9160, loss 0.43861, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9161, loss 0.43861, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9162, loss 0.43860, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9163, loss 0.43859, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9164, loss 0.43859, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9165, loss 0.43858, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9166, loss 0.43857, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9167, loss 0.43856, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9168, loss 0.43856, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9169, loss 0.43855, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9170, loss 0.43854, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9171, loss 0.43853, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9172, loss 0.43853, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9173, loss 0.43852, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9174, loss 0.43851, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9175, loss 0.43850, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9176, loss 0.43850, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9177, loss 0.43849, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9178, loss 0.43848, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9179, loss 0.43848, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9180, loss 0.43847, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9181, loss 0.43846, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9182, loss 0.43845, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9183, loss 0.43845, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9184, loss 0.43844, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9185, loss 0.43843, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9186, loss 0.43843, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9187, loss 0.43842, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9188, loss 0.43841, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9189, loss 0.43840, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9190, loss 0.43840, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9191, loss 0.43839, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9192, loss 0.43838, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9193, loss 0.43837, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9194, loss 0.43837, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9195, loss 0.43836, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9196, loss 0.43835, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9197, loss 0.43835, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9198, loss 0.43834, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9199, loss 0.43833, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9200, loss 0.43833, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9201, loss 0.43832, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9202, loss 0.43831, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9203, loss 0.43830, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9204, loss 0.43830, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9205, loss 0.43829, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9206, loss 0.43828, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9207, loss 0.43828, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9208, loss 0.43827, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9209, loss 0.43826, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9210, loss 0.43825, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9211, loss 0.43825, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9212, loss 0.43824, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9213, loss 0.43823, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9214, loss 0.43823, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9215, loss 0.43822, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9216, loss 0.43821, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9217, loss 0.43820, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9218, loss 0.43820, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9219, loss 0.43819, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9220, loss 0.43818, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9221, loss 0.43818, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9222, loss 0.43817, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9223, loss 0.43816, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9224, loss 0.43816, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9225, loss 0.43815, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9226, loss 0.43814, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9227, loss 0.43813, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9228, loss 0.43813, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9229, loss 0.43812, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9230, loss 0.43811, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9231, loss 0.43811, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9232, loss 0.43810, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9233, loss 0.43809, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9234, loss 0.43809, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9235, loss 0.43808, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9236, loss 0.43807, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9237, loss 0.43806, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9238, loss 0.43806, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9239, loss 0.43805, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9240, loss 0.43804, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9241, loss 0.43804, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9242, loss 0.43803, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9243, loss 0.43802, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9244, loss 0.43802, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9245, loss 0.43801, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9246, loss 0.43800, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9247, loss 0.43800, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9248, loss 0.43800, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9249, loss 0.43800, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9250, loss 0.43800, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9251, loss 0.43801, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9252, loss 0.43804, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9253, loss 0.43809, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9254, loss 0.43819, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 9255, loss 0.43837, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9256, loss 0.43873, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 9257, loss 0.43932, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 9258, loss 0.44058, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9259, loss 0.44245, training accuracy 79.92%, test accuracy 52.48%\n",
            "epoch 9260, loss 0.44667, training accuracy 80.91%, test accuracy 51.29%\n",
            "epoch 9261, loss 0.45149, training accuracy 80.12%, test accuracy 53.66%\n",
            "epoch 9262, loss 0.46337, training accuracy 79.32%, test accuracy 49.50%\n",
            "epoch 9263, loss 0.46903, training accuracy 78.33%, test accuracy 53.07%\n",
            "epoch 9264, loss 0.48815, training accuracy 78.33%, test accuracy 49.11%\n",
            "epoch 9265, loss 0.48158, training accuracy 77.93%, test accuracy 53.27%\n",
            "epoch 9266, loss 0.48882, training accuracy 78.13%, test accuracy 49.70%\n",
            "epoch 9267, loss 0.47017, training accuracy 78.33%, test accuracy 53.47%\n",
            "epoch 9268, loss 0.45852, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 9269, loss 0.44266, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9270, loss 0.43862, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9271, loss 0.44505, training accuracy 80.72%, test accuracy 51.49%\n",
            "epoch 9272, loss 0.45304, training accuracy 80.72%, test accuracy 53.86%\n",
            "epoch 9273, loss 0.46237, training accuracy 79.92%, test accuracy 51.29%\n",
            "epoch 9274, loss 0.45792, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 9275, loss 0.45235, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9276, loss 0.44252, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 9277, loss 0.43837, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9278, loss 0.44047, training accuracy 81.11%, test accuracy 51.88%\n",
            "epoch 9279, loss 0.44512, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9280, loss 0.44953, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 9281, loss 0.44766, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 9282, loss 0.44407, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9283, loss 0.43984, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9284, loss 0.43817, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9285, loss 0.43960, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9286, loss 0.44181, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 9287, loss 0.44366, training accuracy 81.11%, test accuracy 53.07%\n",
            "epoch 9288, loss 0.44280, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 9289, loss 0.44093, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9290, loss 0.43891, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9291, loss 0.43801, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9292, loss 0.43857, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9293, loss 0.43960, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9294, loss 0.44062, training accuracy 81.51%, test accuracy 52.67%\n",
            "epoch 9295, loss 0.44057, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9296, loss 0.43998, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9297, loss 0.43889, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9298, loss 0.43811, training accuracy 80.91%, test accuracy 53.07%\n",
            "epoch 9299, loss 0.43790, training accuracy 80.32%, test accuracy 52.08%\n",
            "epoch 9300, loss 0.43811, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9301, loss 0.43863, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9302, loss 0.43900, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 9303, loss 0.43918, training accuracy 81.31%, test accuracy 52.48%\n",
            "epoch 9304, loss 0.43888, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9305, loss 0.43849, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9306, loss 0.43809, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9307, loss 0.43782, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9308, loss 0.43777, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9309, loss 0.43787, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9310, loss 0.43808, training accuracy 81.11%, test accuracy 52.28%\n",
            "epoch 9311, loss 0.43822, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9312, loss 0.43834, training accuracy 81.11%, test accuracy 52.28%\n",
            "epoch 9313, loss 0.43830, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9314, loss 0.43822, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9315, loss 0.43805, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9316, loss 0.43789, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9317, loss 0.43776, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9318, loss 0.43768, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9319, loss 0.43766, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9320, loss 0.43767, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9321, loss 0.43771, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9322, loss 0.43776, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9323, loss 0.43781, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9324, loss 0.43784, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9325, loss 0.43786, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9326, loss 0.43785, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9327, loss 0.43784, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9328, loss 0.43781, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9329, loss 0.43779, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9330, loss 0.43775, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9331, loss 0.43772, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9332, loss 0.43769, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9333, loss 0.43766, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 9334, loss 0.43764, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9335, loss 0.43762, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9336, loss 0.43761, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9337, loss 0.43760, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9338, loss 0.43759, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9339, loss 0.43758, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9340, loss 0.43758, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9341, loss 0.43759, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9342, loss 0.43760, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9343, loss 0.43762, training accuracy 81.11%, test accuracy 52.28%\n",
            "epoch 9344, loss 0.43765, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9345, loss 0.43771, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9346, loss 0.43778, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9347, loss 0.43792, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9348, loss 0.43809, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 9349, loss 0.43839, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9350, loss 0.43878, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9351, loss 0.43949, training accuracy 81.51%, test accuracy 53.27%\n",
            "epoch 9352, loss 0.44031, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9353, loss 0.44197, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9354, loss 0.44359, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9355, loss 0.44713, training accuracy 81.11%, test accuracy 51.29%\n",
            "epoch 9356, loss 0.44957, training accuracy 80.72%, test accuracy 53.66%\n",
            "epoch 9357, loss 0.45580, training accuracy 80.52%, test accuracy 51.09%\n",
            "epoch 9358, loss 0.45711, training accuracy 79.32%, test accuracy 53.47%\n",
            "epoch 9359, loss 0.46377, training accuracy 79.13%, test accuracy 50.50%\n",
            "epoch 9360, loss 0.46021, training accuracy 79.13%, test accuracy 53.47%\n",
            "epoch 9361, loss 0.46139, training accuracy 79.32%, test accuracy 51.09%\n",
            "epoch 9362, loss 0.45298, training accuracy 79.52%, test accuracy 53.07%\n",
            "epoch 9363, loss 0.44783, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 9364, loss 0.44125, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 9365, loss 0.43802, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9366, loss 0.43762, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9367, loss 0.43942, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 9368, loss 0.44262, training accuracy 81.11%, test accuracy 51.88%\n",
            "epoch 9369, loss 0.44502, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 9370, loss 0.44773, training accuracy 81.11%, test accuracy 51.49%\n",
            "epoch 9371, loss 0.44653, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 9372, loss 0.44552, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9373, loss 0.44214, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9374, loss 0.43964, training accuracy 81.31%, test accuracy 52.28%\n",
            "epoch 9375, loss 0.43786, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9376, loss 0.43739, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9377, loss 0.43800, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9378, loss 0.43916, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9379, loss 0.44053, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 9380, loss 0.44116, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9381, loss 0.44163, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 9382, loss 0.44088, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9383, loss 0.44017, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9384, loss 0.43901, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9385, loss 0.43815, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9386, loss 0.43753, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9387, loss 0.43727, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9388, loss 0.43733, training accuracy 80.91%, test accuracy 51.88%\n",
            "epoch 9389, loss 0.43760, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9390, loss 0.43799, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9391, loss 0.43837, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9392, loss 0.43877, training accuracy 81.51%, test accuracy 52.67%\n",
            "epoch 9393, loss 0.43897, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9394, loss 0.43921, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 9395, loss 0.43917, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9396, loss 0.43921, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 9397, loss 0.43898, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9398, loss 0.43886, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9399, loss 0.43855, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9400, loss 0.43836, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9401, loss 0.43810, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9402, loss 0.43792, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9403, loss 0.43774, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9404, loss 0.43762, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9405, loss 0.43751, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9406, loss 0.43744, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9407, loss 0.43737, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9408, loss 0.43734, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9409, loss 0.43731, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9410, loss 0.43730, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9411, loss 0.43729, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9412, loss 0.43731, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9413, loss 0.43734, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9414, loss 0.43740, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9415, loss 0.43747, training accuracy 80.12%, test accuracy 52.87%\n",
            "epoch 9416, loss 0.43761, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9417, loss 0.43779, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 9418, loss 0.43811, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9419, loss 0.43850, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9420, loss 0.43922, training accuracy 81.51%, test accuracy 53.07%\n",
            "epoch 9421, loss 0.44005, training accuracy 79.92%, test accuracy 52.67%\n",
            "epoch 9422, loss 0.44171, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 9423, loss 0.44334, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9424, loss 0.44694, training accuracy 81.11%, test accuracy 51.29%\n",
            "epoch 9425, loss 0.44954, training accuracy 80.91%, test accuracy 53.66%\n",
            "epoch 9426, loss 0.45615, training accuracy 80.52%, test accuracy 50.89%\n",
            "epoch 9427, loss 0.45821, training accuracy 79.32%, test accuracy 53.47%\n",
            "epoch 9428, loss 0.46663, training accuracy 79.13%, test accuracy 49.90%\n",
            "epoch 9429, loss 0.46461, training accuracy 78.53%, test accuracy 53.66%\n",
            "epoch 9430, loss 0.46935, training accuracy 78.93%, test accuracy 50.69%\n",
            "epoch 9431, loss 0.46105, training accuracy 78.73%, test accuracy 53.27%\n",
            "epoch 9432, loss 0.45663, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 9433, loss 0.44579, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 9434, loss 0.43945, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9435, loss 0.43714, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9436, loss 0.43910, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 9437, loss 0.44368, training accuracy 81.11%, test accuracy 51.49%\n",
            "epoch 9438, loss 0.44711, training accuracy 80.91%, test accuracy 53.27%\n",
            "epoch 9439, loss 0.45048, training accuracy 80.91%, test accuracy 51.29%\n",
            "epoch 9440, loss 0.44816, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 9441, loss 0.44573, training accuracy 80.91%, test accuracy 53.07%\n",
            "epoch 9442, loss 0.44079, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9443, loss 0.43782, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9444, loss 0.43712, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9445, loss 0.43843, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9446, loss 0.44074, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9447, loss 0.44219, training accuracy 79.72%, test accuracy 53.47%\n",
            "epoch 9448, loss 0.44309, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9449, loss 0.44164, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9450, loss 0.44002, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9451, loss 0.43813, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9452, loss 0.43709, training accuracy 81.11%, test accuracy 53.07%\n",
            "epoch 9453, loss 0.43701, training accuracy 80.91%, test accuracy 52.67%\n",
            "epoch 9454, loss 0.43765, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9455, loss 0.43863, training accuracy 81.51%, test accuracy 52.87%\n",
            "epoch 9456, loss 0.43929, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9457, loss 0.43975, training accuracy 81.31%, test accuracy 52.87%\n",
            "epoch 9458, loss 0.43941, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9459, loss 0.43895, training accuracy 81.51%, test accuracy 52.67%\n",
            "epoch 9460, loss 0.43815, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 9461, loss 0.43750, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9462, loss 0.43703, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9463, loss 0.43684, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9464, loss 0.43687, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 9465, loss 0.43706, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9466, loss 0.43733, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9467, loss 0.43758, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9468, loss 0.43780, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9469, loss 0.43786, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9470, loss 0.43790, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9471, loss 0.43779, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9472, loss 0.43770, training accuracy 81.31%, test accuracy 52.08%\n",
            "epoch 9473, loss 0.43751, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 9474, loss 0.43737, training accuracy 81.11%, test accuracy 52.28%\n",
            "epoch 9475, loss 0.43720, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9476, loss 0.43707, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9477, loss 0.43695, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9478, loss 0.43686, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9479, loss 0.43679, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9480, loss 0.43674, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9481, loss 0.43670, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9482, loss 0.43667, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9483, loss 0.43666, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9484, loss 0.43664, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9485, loss 0.43664, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9486, loss 0.43663, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9487, loss 0.43663, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9488, loss 0.43663, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9489, loss 0.43664, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9490, loss 0.43664, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9491, loss 0.43666, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9492, loss 0.43668, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9493, loss 0.43672, training accuracy 80.91%, test accuracy 52.08%\n",
            "epoch 9494, loss 0.43678, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9495, loss 0.43688, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9496, loss 0.43702, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 9497, loss 0.43726, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9498, loss 0.43760, training accuracy 79.92%, test accuracy 52.87%\n",
            "epoch 9499, loss 0.43823, training accuracy 81.51%, test accuracy 52.87%\n",
            "epoch 9500, loss 0.43906, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9501, loss 0.44070, training accuracy 81.11%, test accuracy 52.48%\n",
            "epoch 9502, loss 0.44261, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 9503, loss 0.44672, training accuracy 81.11%, test accuracy 51.49%\n",
            "epoch 9504, loss 0.45043, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 9505, loss 0.45957, training accuracy 79.72%, test accuracy 49.50%\n",
            "epoch 9506, loss 0.46442, training accuracy 78.33%, test accuracy 53.66%\n",
            "epoch 9507, loss 0.48099, training accuracy 78.53%, test accuracy 48.91%\n",
            "epoch 9508, loss 0.48653, training accuracy 77.53%, test accuracy 53.27%\n",
            "epoch 9509, loss 0.51968, training accuracy 77.14%, test accuracy 47.13%\n",
            "epoch 9510, loss 0.54648, training accuracy 74.55%, test accuracy 54.06%\n",
            "epoch 9511, loss 0.66366, training accuracy 75.15%, test accuracy 47.72%\n",
            "epoch 9512, loss 0.54614, training accuracy 72.37%, test accuracy 53.07%\n",
            "epoch 9513, loss 0.49062, training accuracy 79.13%, test accuracy 51.68%\n",
            "epoch 9514, loss 0.44753, training accuracy 80.72%, test accuracy 47.92%\n",
            "epoch 9515, loss 0.48665, training accuracy 77.34%, test accuracy 53.27%\n",
            "epoch 9516, loss 0.55813, training accuracy 76.94%, test accuracy 50.89%\n",
            "epoch 9517, loss 0.56614, training accuracy 75.75%, test accuracy 52.48%\n",
            "epoch 9518, loss 0.52495, training accuracy 79.72%, test accuracy 51.49%\n",
            "epoch 9519, loss 0.50719, training accuracy 78.33%, test accuracy 53.86%\n",
            "epoch 9520, loss 0.52813, training accuracy 77.53%, test accuracy 53.47%\n",
            "epoch 9521, loss 0.49185, training accuracy 78.53%, test accuracy 50.10%\n",
            "epoch 9522, loss 0.50160, training accuracy 76.94%, test accuracy 51.09%\n",
            "epoch 9523, loss 0.49774, training accuracy 78.13%, test accuracy 53.66%\n",
            "epoch 9524, loss 0.53533, training accuracy 78.53%, test accuracy 51.88%\n",
            "epoch 9525, loss 0.50531, training accuracy 76.94%, test accuracy 51.68%\n",
            "epoch 9526, loss 0.48047, training accuracy 78.33%, test accuracy 54.06%\n",
            "epoch 9527, loss 0.52554, training accuracy 78.13%, test accuracy 51.09%\n",
            "epoch 9528, loss 0.50984, training accuracy 77.53%, test accuracy 52.48%\n",
            "epoch 9529, loss 0.47721, training accuracy 79.13%, test accuracy 51.49%\n",
            "epoch 9530, loss 0.49527, training accuracy 77.34%, test accuracy 50.30%\n",
            "epoch 9531, loss 0.47941, training accuracy 78.53%, test accuracy 52.08%\n",
            "epoch 9532, loss 0.50199, training accuracy 77.34%, test accuracy 52.08%\n",
            "epoch 9533, loss 0.50534, training accuracy 78.33%, test accuracy 53.07%\n",
            "epoch 9534, loss 0.47877, training accuracy 78.53%, test accuracy 50.89%\n",
            "epoch 9535, loss 0.49954, training accuracy 78.93%, test accuracy 52.28%\n",
            "epoch 9536, loss 0.47976, training accuracy 78.73%, test accuracy 53.07%\n",
            "epoch 9537, loss 0.47746, training accuracy 79.32%, test accuracy 52.08%\n",
            "epoch 9538, loss 0.48851, training accuracy 79.52%, test accuracy 51.09%\n",
            "epoch 9539, loss 0.45735, training accuracy 79.32%, test accuracy 52.48%\n",
            "epoch 9540, loss 0.46975, training accuracy 79.72%, test accuracy 51.88%\n",
            "epoch 9541, loss 0.46946, training accuracy 79.72%, test accuracy 51.29%\n",
            "epoch 9542, loss 0.45895, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 9543, loss 0.45584, training accuracy 80.12%, test accuracy 53.07%\n",
            "epoch 9544, loss 0.46364, training accuracy 79.13%, test accuracy 53.07%\n",
            "epoch 9545, loss 0.44935, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9546, loss 0.44954, training accuracy 80.12%, test accuracy 50.50%\n",
            "epoch 9547, loss 0.45272, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 9548, loss 0.44751, training accuracy 80.91%, test accuracy 51.68%\n",
            "epoch 9549, loss 0.44675, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9550, loss 0.44964, training accuracy 80.12%, test accuracy 52.67%\n",
            "epoch 9551, loss 0.44478, training accuracy 80.91%, test accuracy 53.07%\n",
            "epoch 9552, loss 0.44550, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9553, loss 0.44542, training accuracy 80.72%, test accuracy 52.08%\n",
            "epoch 9554, loss 0.44333, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9555, loss 0.44484, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9556, loss 0.44376, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9557, loss 0.44169, training accuracy 81.71%, test accuracy 52.28%\n",
            "epoch 9558, loss 0.44178, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9559, loss 0.44325, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9560, loss 0.43999, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 9561, loss 0.44096, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 9562, loss 0.44038, training accuracy 81.71%, test accuracy 51.88%\n",
            "epoch 9563, loss 0.44106, training accuracy 80.12%, test accuracy 51.88%\n",
            "epoch 9564, loss 0.43874, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9565, loss 0.44037, training accuracy 80.91%, test accuracy 52.28%\n",
            "epoch 9566, loss 0.43922, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 9567, loss 0.43900, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9568, loss 0.43907, training accuracy 81.31%, test accuracy 52.28%\n",
            "epoch 9569, loss 0.43894, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9570, loss 0.43838, training accuracy 81.31%, test accuracy 52.67%\n",
            "epoch 9571, loss 0.43827, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 9572, loss 0.43878, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 9573, loss 0.43784, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 9574, loss 0.43823, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9575, loss 0.43790, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9576, loss 0.43804, training accuracy 80.72%, test accuracy 52.08%\n",
            "epoch 9577, loss 0.43759, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 9578, loss 0.43794, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9579, loss 0.43742, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9580, loss 0.43773, training accuracy 80.12%, test accuracy 52.28%\n",
            "epoch 9581, loss 0.43750, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9582, loss 0.43738, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 9583, loss 0.43732, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9584, loss 0.43746, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9585, loss 0.43721, training accuracy 79.92%, test accuracy 52.28%\n",
            "epoch 9586, loss 0.43713, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9587, loss 0.43725, training accuracy 80.72%, test accuracy 52.08%\n",
            "epoch 9588, loss 0.43711, training accuracy 80.52%, test accuracy 52.08%\n",
            "epoch 9589, loss 0.43709, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9590, loss 0.43702, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9591, loss 0.43706, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9592, loss 0.43693, training accuracy 80.52%, test accuracy 51.88%\n",
            "epoch 9593, loss 0.43700, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9594, loss 0.43687, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9595, loss 0.43692, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9596, loss 0.43684, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9597, loss 0.43687, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9598, loss 0.43678, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9599, loss 0.43682, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9600, loss 0.43675, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9601, loss 0.43675, training accuracy 80.72%, test accuracy 52.28%\n",
            "epoch 9602, loss 0.43671, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9603, loss 0.43672, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9604, loss 0.43667, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9605, loss 0.43667, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9606, loss 0.43664, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9607, loss 0.43663, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9608, loss 0.43661, training accuracy 80.52%, test accuracy 52.28%\n",
            "epoch 9609, loss 0.43660, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9610, loss 0.43657, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9611, loss 0.43656, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9612, loss 0.43655, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9613, loss 0.43653, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9614, loss 0.43652, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9615, loss 0.43650, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9616, loss 0.43649, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9617, loss 0.43647, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9618, loss 0.43647, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9619, loss 0.43645, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9620, loss 0.43644, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9621, loss 0.43642, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9622, loss 0.43641, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9623, loss 0.43640, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9624, loss 0.43639, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9625, loss 0.43638, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9626, loss 0.43637, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9627, loss 0.43635, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9628, loss 0.43634, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9629, loss 0.43633, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9630, loss 0.43632, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9631, loss 0.43631, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9632, loss 0.43630, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9633, loss 0.43629, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9634, loss 0.43628, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9635, loss 0.43627, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9636, loss 0.43626, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9637, loss 0.43625, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9638, loss 0.43624, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9639, loss 0.43623, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9640, loss 0.43622, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9641, loss 0.43621, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9642, loss 0.43620, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9643, loss 0.43619, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9644, loss 0.43618, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9645, loss 0.43617, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9646, loss 0.43616, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9647, loss 0.43615, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9648, loss 0.43614, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9649, loss 0.43613, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9650, loss 0.43612, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9651, loss 0.43611, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9652, loss 0.43610, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9653, loss 0.43609, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9654, loss 0.43608, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9655, loss 0.43608, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9656, loss 0.43607, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9657, loss 0.43606, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9658, loss 0.43605, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9659, loss 0.43604, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9660, loss 0.43603, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9661, loss 0.43602, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9662, loss 0.43601, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9663, loss 0.43601, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9664, loss 0.43600, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9665, loss 0.43599, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9666, loss 0.43598, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9667, loss 0.43597, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9668, loss 0.43596, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9669, loss 0.43595, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9670, loss 0.43595, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9671, loss 0.43594, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9672, loss 0.43593, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9673, loss 0.43592, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9674, loss 0.43591, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9675, loss 0.43590, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9676, loss 0.43590, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9677, loss 0.43589, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9678, loss 0.43588, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9679, loss 0.43587, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9680, loss 0.43586, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9681, loss 0.43586, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9682, loss 0.43585, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9683, loss 0.43584, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9684, loss 0.43583, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9685, loss 0.43582, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9686, loss 0.43582, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9687, loss 0.43581, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9688, loss 0.43580, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9689, loss 0.43579, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9690, loss 0.43578, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9691, loss 0.43578, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9692, loss 0.43577, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9693, loss 0.43576, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9694, loss 0.43575, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9695, loss 0.43574, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9696, loss 0.43574, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9697, loss 0.43573, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9698, loss 0.43572, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9699, loss 0.43571, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9700, loss 0.43571, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9701, loss 0.43570, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9702, loss 0.43569, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9703, loss 0.43568, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9704, loss 0.43567, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9705, loss 0.43567, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9706, loss 0.43566, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9707, loss 0.43565, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9708, loss 0.43564, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9709, loss 0.43564, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9710, loss 0.43563, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9711, loss 0.43562, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9712, loss 0.43561, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9713, loss 0.43561, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9714, loss 0.43560, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9715, loss 0.43559, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9716, loss 0.43558, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9717, loss 0.43558, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9718, loss 0.43557, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9719, loss 0.43556, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9720, loss 0.43555, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9721, loss 0.43555, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9722, loss 0.43554, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9723, loss 0.43553, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9724, loss 0.43552, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9725, loss 0.43552, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9726, loss 0.43551, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9727, loss 0.43550, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9728, loss 0.43549, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9729, loss 0.43549, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9730, loss 0.43548, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9731, loss 0.43547, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9732, loss 0.43546, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9733, loss 0.43546, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9734, loss 0.43545, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9735, loss 0.43544, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9736, loss 0.43543, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9737, loss 0.43543, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9738, loss 0.43542, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9739, loss 0.43541, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9740, loss 0.43540, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9741, loss 0.43540, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9742, loss 0.43539, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9743, loss 0.43538, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9744, loss 0.43537, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9745, loss 0.43537, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9746, loss 0.43536, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9747, loss 0.43535, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9748, loss 0.43534, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9749, loss 0.43534, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9750, loss 0.43533, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9751, loss 0.43532, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9752, loss 0.43531, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9753, loss 0.43531, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9754, loss 0.43530, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9755, loss 0.43529, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9756, loss 0.43528, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9757, loss 0.43528, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9758, loss 0.43527, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9759, loss 0.43526, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9760, loss 0.43525, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9761, loss 0.43525, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9762, loss 0.43524, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9763, loss 0.43523, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9764, loss 0.43523, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9765, loss 0.43522, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9766, loss 0.43521, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9767, loss 0.43520, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9768, loss 0.43520, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9769, loss 0.43519, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9770, loss 0.43518, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9771, loss 0.43517, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9772, loss 0.43517, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9773, loss 0.43516, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9774, loss 0.43515, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9775, loss 0.43514, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9776, loss 0.43514, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9777, loss 0.43513, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9778, loss 0.43512, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9779, loss 0.43511, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9780, loss 0.43511, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9781, loss 0.43510, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9782, loss 0.43509, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9783, loss 0.43508, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9784, loss 0.43508, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9785, loss 0.43507, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9786, loss 0.43506, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9787, loss 0.43506, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9788, loss 0.43505, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9789, loss 0.43504, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9790, loss 0.43503, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9791, loss 0.43503, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9792, loss 0.43502, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9793, loss 0.43501, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9794, loss 0.43500, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9795, loss 0.43500, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9796, loss 0.43499, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9797, loss 0.43498, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9798, loss 0.43498, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9799, loss 0.43497, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 9800, loss 0.43496, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9801, loss 0.43496, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 9802, loss 0.43496, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9803, loss 0.43496, training accuracy 80.72%, test accuracy 53.27%\n",
            "epoch 9804, loss 0.43497, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9805, loss 0.43500, training accuracy 80.72%, test accuracy 53.47%\n",
            "epoch 9806, loss 0.43506, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9807, loss 0.43517, training accuracy 80.32%, test accuracy 53.47%\n",
            "epoch 9808, loss 0.43540, training accuracy 81.11%, test accuracy 52.87%\n",
            "epoch 9809, loss 0.43581, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9810, loss 0.43668, training accuracy 81.51%, test accuracy 53.07%\n",
            "epoch 9811, loss 0.43813, training accuracy 80.12%, test accuracy 53.27%\n",
            "epoch 9812, loss 0.44138, training accuracy 81.11%, test accuracy 52.08%\n",
            "epoch 9813, loss 0.44588, training accuracy 81.11%, test accuracy 53.86%\n",
            "epoch 9814, loss 0.45673, training accuracy 79.92%, test accuracy 49.90%\n",
            "epoch 9815, loss 0.46536, training accuracy 78.73%, test accuracy 53.47%\n",
            "epoch 9816, loss 0.48990, training accuracy 78.73%, test accuracy 48.51%\n",
            "epoch 9817, loss 0.49300, training accuracy 77.53%, test accuracy 53.66%\n",
            "epoch 9818, loss 0.52463, training accuracy 77.14%, test accuracy 47.72%\n",
            "epoch 9819, loss 0.52514, training accuracy 75.55%, test accuracy 54.06%\n",
            "epoch 9820, loss 0.54513, training accuracy 77.14%, test accuracy 49.70%\n",
            "epoch 9821, loss 0.48687, training accuracy 77.53%, test accuracy 53.27%\n",
            "epoch 9822, loss 0.45005, training accuracy 79.72%, test accuracy 52.67%\n",
            "epoch 9823, loss 0.44206, training accuracy 81.11%, test accuracy 49.70%\n",
            "epoch 9824, loss 0.46891, training accuracy 77.93%, test accuracy 53.47%\n",
            "epoch 9825, loss 0.51472, training accuracy 78.53%, test accuracy 50.30%\n",
            "epoch 9826, loss 0.48744, training accuracy 78.53%, test accuracy 52.87%\n",
            "epoch 9827, loss 0.44200, training accuracy 81.11%, test accuracy 51.49%\n",
            "epoch 9828, loss 0.45378, training accuracy 80.32%, test accuracy 51.29%\n",
            "epoch 9829, loss 0.49223, training accuracy 78.53%, test accuracy 54.26%\n",
            "epoch 9830, loss 0.50636, training accuracy 78.13%, test accuracy 51.68%\n",
            "epoch 9831, loss 0.46252, training accuracy 78.93%, test accuracy 53.27%\n",
            "epoch 9832, loss 0.44443, training accuracy 80.32%, test accuracy 53.86%\n",
            "epoch 9833, loss 0.48809, training accuracy 79.32%, test accuracy 51.49%\n",
            "epoch 9834, loss 0.46455, training accuracy 77.93%, test accuracy 52.67%\n",
            "epoch 9835, loss 0.44772, training accuracy 80.91%, test accuracy 53.86%\n",
            "epoch 9836, loss 0.45208, training accuracy 80.52%, test accuracy 51.68%\n",
            "epoch 9837, loss 0.45136, training accuracy 79.32%, test accuracy 52.28%\n",
            "epoch 9838, loss 0.47369, training accuracy 79.32%, test accuracy 52.67%\n",
            "epoch 9839, loss 0.45842, training accuracy 79.92%, test accuracy 54.06%\n",
            "epoch 9840, loss 0.45154, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9841, loss 0.45216, training accuracy 80.12%, test accuracy 51.49%\n",
            "epoch 9842, loss 0.45017, training accuracy 80.12%, test accuracy 54.85%\n",
            "epoch 9843, loss 0.46202, training accuracy 79.13%, test accuracy 52.87%\n",
            "epoch 9844, loss 0.43940, training accuracy 80.12%, test accuracy 52.48%\n",
            "epoch 9845, loss 0.44715, training accuracy 80.12%, test accuracy 54.46%\n",
            "epoch 9846, loss 0.45173, training accuracy 79.72%, test accuracy 52.87%\n",
            "epoch 9847, loss 0.44676, training accuracy 79.72%, test accuracy 52.08%\n",
            "epoch 9848, loss 0.44397, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9849, loss 0.43769, training accuracy 81.71%, test accuracy 52.87%\n",
            "epoch 9850, loss 0.44507, training accuracy 79.72%, test accuracy 53.27%\n",
            "epoch 9851, loss 0.43871, training accuracy 81.51%, test accuracy 52.67%\n",
            "epoch 9852, loss 0.44109, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9853, loss 0.43846, training accuracy 80.12%, test accuracy 53.66%\n",
            "epoch 9854, loss 0.44154, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9855, loss 0.43993, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9856, loss 0.43817, training accuracy 81.51%, test accuracy 53.27%\n",
            "epoch 9857, loss 0.43852, training accuracy 81.11%, test accuracy 53.07%\n",
            "epoch 9858, loss 0.43723, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9859, loss 0.43904, training accuracy 81.11%, test accuracy 53.07%\n",
            "epoch 9860, loss 0.43643, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 9861, loss 0.43739, training accuracy 81.31%, test accuracy 53.27%\n",
            "epoch 9862, loss 0.43548, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 9863, loss 0.43743, training accuracy 80.91%, test accuracy 53.47%\n",
            "epoch 9864, loss 0.43628, training accuracy 81.51%, test accuracy 52.87%\n",
            "epoch 9865, loss 0.43665, training accuracy 80.32%, test accuracy 52.87%\n",
            "epoch 9866, loss 0.43524, training accuracy 80.52%, test accuracy 53.07%\n",
            "epoch 9867, loss 0.43636, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9868, loss 0.43597, training accuracy 79.92%, test accuracy 53.27%\n",
            "epoch 9869, loss 0.43632, training accuracy 81.31%, test accuracy 52.48%\n",
            "epoch 9870, loss 0.43533, training accuracy 80.32%, test accuracy 52.48%\n",
            "epoch 9871, loss 0.43563, training accuracy 80.32%, test accuracy 53.27%\n",
            "epoch 9872, loss 0.43551, training accuracy 81.11%, test accuracy 52.67%\n",
            "epoch 9873, loss 0.43585, training accuracy 80.32%, test accuracy 53.07%\n",
            "epoch 9874, loss 0.43542, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9875, loss 0.43528, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9876, loss 0.43520, training accuracy 80.52%, test accuracy 53.47%\n",
            "epoch 9877, loss 0.43535, training accuracy 80.91%, test accuracy 52.48%\n",
            "epoch 9878, loss 0.43532, training accuracy 79.92%, test accuracy 53.07%\n",
            "epoch 9879, loss 0.43517, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9880, loss 0.43507, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9881, loss 0.43500, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9882, loss 0.43511, training accuracy 80.32%, test accuracy 52.67%\n",
            "epoch 9883, loss 0.43505, training accuracy 80.52%, test accuracy 53.27%\n",
            "epoch 9884, loss 0.43502, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9885, loss 0.43486, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9886, loss 0.43492, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9887, loss 0.43487, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9888, loss 0.43495, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9889, loss 0.43483, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9890, loss 0.43481, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9891, loss 0.43475, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9892, loss 0.43479, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9893, loss 0.43476, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9894, loss 0.43476, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9895, loss 0.43471, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9896, loss 0.43468, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9897, loss 0.43467, training accuracy 80.72%, test accuracy 52.48%\n",
            "epoch 9898, loss 0.43467, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9899, loss 0.43467, training accuracy 80.52%, test accuracy 52.48%\n",
            "epoch 9900, loss 0.43464, training accuracy 80.91%, test accuracy 52.87%\n",
            "epoch 9901, loss 0.43463, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9902, loss 0.43459, training accuracy 80.72%, test accuracy 52.67%\n",
            "epoch 9903, loss 0.43460, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9904, loss 0.43458, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9905, loss 0.43459, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9906, loss 0.43455, training accuracy 80.52%, test accuracy 52.67%\n",
            "epoch 9907, loss 0.43455, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9908, loss 0.43452, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9909, loss 0.43453, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9910, loss 0.43451, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9911, loss 0.43451, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9912, loss 0.43449, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9913, loss 0.43448, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9914, loss 0.43447, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9915, loss 0.43446, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9916, loss 0.43445, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9917, loss 0.43444, training accuracy 80.72%, test accuracy 53.07%\n",
            "epoch 9918, loss 0.43443, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9919, loss 0.43442, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9920, loss 0.43441, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9921, loss 0.43440, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9922, loss 0.43439, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9923, loss 0.43438, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9924, loss 0.43438, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9925, loss 0.43437, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9926, loss 0.43436, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9927, loss 0.43435, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9928, loss 0.43434, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9929, loss 0.43433, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9930, loss 0.43432, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9931, loss 0.43432, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9932, loss 0.43431, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9933, loss 0.43430, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9934, loss 0.43429, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9935, loss 0.43428, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9936, loss 0.43427, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9937, loss 0.43426, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9938, loss 0.43426, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9939, loss 0.43425, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9940, loss 0.43424, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9941, loss 0.43423, training accuracy 80.52%, test accuracy 52.87%\n",
            "epoch 9942, loss 0.43422, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9943, loss 0.43421, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9944, loss 0.43421, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9945, loss 0.43420, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9946, loss 0.43419, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9947, loss 0.43418, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9948, loss 0.43417, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9949, loss 0.43417, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9950, loss 0.43416, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9951, loss 0.43415, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9952, loss 0.43414, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9953, loss 0.43413, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9954, loss 0.43413, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9955, loss 0.43412, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9956, loss 0.43411, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9957, loss 0.43410, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9958, loss 0.43409, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9959, loss 0.43409, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9960, loss 0.43408, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9961, loss 0.43407, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9962, loss 0.43406, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9963, loss 0.43406, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9964, loss 0.43405, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9965, loss 0.43404, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9966, loss 0.43403, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9967, loss 0.43402, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9968, loss 0.43402, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9969, loss 0.43401, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9970, loss 0.43400, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9971, loss 0.43399, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9972, loss 0.43399, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9973, loss 0.43398, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9974, loss 0.43397, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9975, loss 0.43396, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9976, loss 0.43395, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9977, loss 0.43395, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9978, loss 0.43394, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9979, loss 0.43393, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9980, loss 0.43392, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9981, loss 0.43392, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9982, loss 0.43391, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9983, loss 0.43390, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9984, loss 0.43389, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9985, loss 0.43389, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9986, loss 0.43388, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9987, loss 0.43387, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9988, loss 0.43386, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9989, loss 0.43385, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9990, loss 0.43385, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9991, loss 0.43384, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9992, loss 0.43383, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9993, loss 0.43382, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9994, loss 0.43382, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9995, loss 0.43381, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9996, loss 0.43380, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9997, loss 0.43379, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9998, loss 0.43379, training accuracy 80.72%, test accuracy 52.87%\n",
            "epoch 9999, loss 0.43378, training accuracy 80.72%, test accuracy 52.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUeOi2r75BlU"
      },
      "source": [
        "dnn_data = torch.Tensor(features)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bHS5Ojt4suB"
      },
      "source": [
        "# evaluate model:\n",
        "dnn_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  out_data = dnn_model(dnn_data)\n"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtiXj-qlbNab"
      },
      "source": [
        "final_strategy = torch.max(out_data,1).indices\n",
        "final_strategy=lab_enc.inverse_transform(np.array(final_strategy.tolist()))"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txDhTUCCnhZW"
      },
      "source": [
        "# 4- Backtesting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BmMcbjv4d32"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmXeqj2x-eV3"
      },
      "source": [
        "In this section we create a custom backtesting parent class with all the base methods and a child class optimised for shrot-long positioning.\n",
        "\n",
        "The code is a version of the code proposed by:\n",
        "\n",
        "> **Python for Algorithmic Trading**\n",
        "\n",
        "> *(c) Dr. Yves J. Hilpisch*\n",
        "\n",
        "> The Python Quants GmbH\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf_VAIr3tuun"
      },
      "source": [
        "class BaseBacktesting(object): \n",
        "  ''' Base class for event-based backtesting of trading strategies.\n",
        "  Attributes\n",
        "  ==========\n",
        "  symbol: str\n",
        "      TR RIC (financial instrument) to be used. If the .csv file has the \n",
        "      Yahoo Finance format, the pricing element must be specified insted \n",
        "      (i.e. \"Close\").\n",
        "  start: str\n",
        "      start date for data selection\n",
        "  end: str\n",
        "      end date for data selection\n",
        "  amount: float\n",
        "      amount to be invested either once or per trade\n",
        "  datapath: string\n",
        "      path to the csv file to be imported\n",
        "  ftc: float\n",
        "      fixed transaction costs per trade (buy or sell). \n",
        "  ptc: float\n",
        "      proportional transaction costs per trade (buy or sell)\n",
        "\n",
        "  Methods\n",
        "  =======\n",
        "  get_data:\n",
        "      retrieves and prepares the base data set\n",
        "  plot_data:\n",
        "      plots the closing price for the symbol\n",
        "  get_date_price:\n",
        "      returns the date and price for the given bar\n",
        "  print_balance:\n",
        "      prints out the current (cash) balance\n",
        "  print_net_wealth:\n",
        "      prints out the current net wealth\n",
        "  place_buy_order:\n",
        "      places a buy order\n",
        "  place_sell_order:\n",
        "      places a sell order\n",
        "  close_out:\n",
        "      closes out a long or short position\n",
        "  '''\n",
        "  def __init__(self, symbol, start, end, amount, datapath, \n",
        "               ftc=0.0, ptc=0.0, verbose=True):\n",
        "    self.symbol = symbol\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.initial_amount = amount\n",
        "    self.amount = amount\n",
        "    self.datapath = datapath\n",
        "    self.ftc = ftc\n",
        "    self.ptc = ptc\n",
        "    self.units = 0\n",
        "    self.position = 0\n",
        "    self.trades = 0\n",
        "    self.verbose = verbose\n",
        "    self.net_wealth_serie = []\n",
        "    self.get_data()\n",
        "\n",
        "  def get_data(self):\n",
        "    ''' Retrieves and prepares the data.\n",
        "    '''\n",
        "    raw = pd.read_csv(self.datapath, index_col=0, parse_dates=True).dropna()\n",
        "    raw = pd.DataFrame(raw[self.symbol])\n",
        "    raw = raw.loc[self.start:self.end]\n",
        "    raw.rename(columns={self.symbol: 'price'}, inplace=True)\n",
        "    raw['return'] = np.log(raw / raw.shift(1))\n",
        "    self.data = raw.fillna(0)\n",
        "\n",
        "  def plot_data(self, cols=None):\n",
        "    ''' Plots the closing prices for symbol.\n",
        "    '''\n",
        "    if cols is None:\n",
        "      cols = ['price']\n",
        "    self.data['price'].plot(figsize=(10, 6), title=self.symbol)\n",
        "\n",
        "  def get_date_price(self, bar):\n",
        "    ''' Return date and price for bar.\n",
        "    '''\n",
        "    date = str(self.data.index[bar])[:10]\n",
        "    price = self.data.price.iloc[bar]\n",
        "    return date, price\n",
        "\n",
        "  def print_balance(self, bar):\n",
        "    ''' Print out current cash balance info.\n",
        "    '''\n",
        "    date, price = self.get_date_price(bar)\n",
        "    print(f'{date} | current balance {self.amount:.2f}')\n",
        "\n",
        "  def print_net_wealth(self, bar):\n",
        "    ''' Print out current cash balance info.\n",
        "    '''\n",
        "    date, price = self.get_date_price(bar)\n",
        "    net_wealth = self.units * price + self.amount\n",
        "    print(f'{date} | current net wealth {net_wealth:.2f}')\n",
        "\n",
        "  def get_net_wealth(self, bar):\n",
        "    ''' Get current cash balance info.\n",
        "    '''\n",
        "    date, price = self.get_date_price(bar)\n",
        "    return self.units * price + self.amount\n",
        "\n",
        "  def place_buy_order(self, bar, units=None, amount=None):\n",
        "    ''' Place a buy order.\n",
        "    '''\n",
        "    date, price = self.get_date_price(bar)\n",
        "    if units is None:\n",
        "      units = int(amount / price)\n",
        "    self.amount -= (units * price) * (1 + self.ptc) + self.ftc\n",
        "    self.units += units\n",
        "    self.trades += 1\n",
        "    if self.verbose:\n",
        "      print(f'{date} | buying {units} units at {price:.2f}')\n",
        "      self.print_balance(bar)\n",
        "      self.print_net_wealth(bar)\n",
        "\n",
        "  def place_sell_order(self, bar, units=None, amount=None):\n",
        "    ''' Place a sell order.\n",
        "    '''\n",
        "    date, price = self.get_date_price(bar)\n",
        "    if units is None:\n",
        "      units = int(amount / price)\n",
        "    self.amount += (units * price) * (1 - self.ptc) - self.ftc\n",
        "    self.units -= units\n",
        "    self.trades += 1\n",
        "    if self.verbose:\n",
        "      print(f'{date} | selling {units} units at {price:.2f}')\n",
        "      self.print_balance(bar)\n",
        "      self.print_net_wealth(bar)\n",
        "\n",
        "  def close_out(self, bar):\n",
        "    ''' Closing out a long or short position. To be used at the end of the \n",
        "    backtesting strategy, to translate the assets involved in the open \n",
        "    positions into liquidity.\n",
        "        '''\n",
        "    date, price = self.get_date_price(bar)\n",
        "    self.amount += self.units * price\n",
        "    self.units = 0\n",
        "    self.trades += 1\n",
        "    if self.verbose:\n",
        "      print(f'{date} | inventory {self.units} units at {price:.2f}')\n",
        "      print('=' * 55)\n",
        "      print('Final balance [$] {:.2f}'.format(self.amount))\n",
        "      perf = ((self.amount - self.initial_amount) /\n",
        "      self.initial_amount * 100)\n",
        "      print('Net Performance [%] {:.2f}'.format(perf))\n",
        "      print('Trades Executed [#] {:.2f}'.format(self.trades))\n",
        "      print('=' * 55)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbzXhrbzoZzg"
      },
      "source": [
        "class BacktestLongShort(BaseBacktesting):\n",
        "\n",
        "  def go_long(self, bar, units=None, amount=None):\n",
        "      #Note that no alternative is presented for the case in which neither \n",
        "      #unit or amount is defined.\n",
        "    if self.position == -1: #If the position is short, close it\n",
        "      self.place_buy_order(bar, units=-self.units)\n",
        "    #Open a new position\n",
        "    if units:\n",
        "      self.place_buy_order(bar, units=units)\n",
        "    elif amount:\n",
        "      if amount == 'all':\n",
        "        amount = self.amount\n",
        "      elif isinstance(amount, (float,int)):\n",
        "        amount = self.amount*amount\n",
        "      self.place_buy_order(bar, amount=amount)\n",
        "\n",
        "  def go_short(self, bar, units=None, amount=None):\n",
        "      #Note that no alternative is presented for the case in which neither \n",
        "      #unit or amount is defined.\n",
        "    if self.position == 1: #If the position is long, close it\n",
        "      self.place_sell_order(bar, units=self.units)\n",
        "      #Open a new position\n",
        "    if units:\n",
        "      self.place_sell_order(bar, units=units)\n",
        "    elif amount:\n",
        "      if amount == 'all':\n",
        "        amount = self.amount\n",
        "      elif isinstance(amount, (float,int)):\n",
        "        amount = self.amount*amount\n",
        "      self.place_sell_order(bar, amount=amount)\n",
        "\n",
        "  def run_by_matrix(self, signals:pd.Series, units_per_trade=None, amount_per_trade='all', min_signals=1, verbose=True, plot_result=True):\n",
        "    ''' This method implements the strategy based on a matrix of signals\n",
        "    for positioning short (-1) or long (1) on the market\n",
        "    '''\n",
        "    msg = f'\\n\\nRunning strategy based on a matrix of signals'\n",
        "    msg += f'\\nfixed costs {self.ftc} | '\n",
        "    msg += f'proportional costs {self.ptc}'\n",
        "    print(msg)\n",
        "    self.net_wealth_serie = [] #Re-initialise the net_wealth_serie list\n",
        "    self.verbose = verbose #Give the possibility to explicit verbosity\n",
        "    self.position = 0 # initial neutral position\n",
        "    self.trades = 0 # no trades yet\n",
        "    self.amount = self.initial_amount # reset initial capital\n",
        "    for bar in range(len(self.data)):\n",
        "      if (sum([signals.iloc[bar]]) >= min_signals) and self.position != 1:\n",
        "        self.go_long(bar, units=units_per_trade, amount=amount_per_trade)\n",
        "        self.position = 1 # long position\n",
        "      if (sum([signals.iloc[bar]]) <= -min_signals) and self.position != -1:\n",
        "        self.go_short(bar, units=units_per_trade, amount=amount_per_trade)\n",
        "        self.position = -1 # short position\n",
        "      self.net_wealth_serie.append(self.get_net_wealth(bar))\n",
        "    self.close_out(bar)\n",
        "    if plot_result:\n",
        "      self.plot_vector_strategy(signals)\n",
        "\n",
        "  def plot_vector_strategy(self, unidim_matrix):\n",
        "    ''' This function plots the strategy, with the entry long and short \n",
        "    points over the instrument price, and the net wealth value overlaid\n",
        "    '''\n",
        "    # Create figure with secondary y-axis\n",
        "    fig_backt = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    def set_buysell_marker(signal):\n",
        "      #This function is conceived to be used by mapping\n",
        "      # It returns a tuple of properties (color, symbol, size, opacity)\n",
        "      if (signal==1):\n",
        "        return ('green', 'triangle-up', 10, 1)\n",
        "      elif (signal==-1):\n",
        "        return ('red', 'triangle-down', 10, 1)\n",
        "      else:\n",
        "        return ('white', 'circle', 0, 0)\n",
        "    prop_list=list(map(set_buysell_marker, unidim_matrix))\n",
        "    marker_props=dict(color=[i[0] for i in prop_list],\n",
        "                      symbol=[i[1] for i in prop_list],\n",
        "                      size=[i[2] for i in prop_list],\n",
        "                      opacity=[i[3] for i in prop_list])\n",
        "    fig_backt.add_trace(go.Scatter(x=self.data.index, y=self.Basedata.price,\n",
        "                        mode='lines+markers',\n",
        "                        name='S&P500 closing price',\n",
        "                        line=dict(color='black'),\n",
        "                        marker = marker_props))\n",
        "    fig_backt.add_trace(go.Scatter(x=self.data.index, y=self.net_wealth_serie,\n",
        "                        mode='lines',\n",
        "                        fill='tozeroy',\n",
        "                        name='Net wealth of the fund',\n",
        "                        fillcolor='rgba(0, 0, 255, 0.15)',\n",
        "                        line=dict(color='rgba(0, 0, 255, 0.2)')),\n",
        "                        secondary_y=True,)\n",
        "    fig_backt.update_layout(title='Backtesting of the trading strategy',\n",
        "                      xaxis_title='Time',\n",
        "                      yaxis_title='S&P500 closing price ($)',\n",
        "                      )\n",
        "    fig_backt.show()"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVOJ7kuIkzwL"
      },
      "source": [
        "## 4.1- Backtesting strategies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp4P4EX2-zoq"
      },
      "source": [
        "lsbt = BacktestLongShort(symbol='Close', start='2017-06-15', end='2021-06-14', amount=10000,\n",
        "datapath='S&P500.csv', verbose=True)"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYiAU-nqCtFC"
      },
      "source": [
        "signals_backtesting = pd.Series(final_strategy.tolist())"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zR1djniH-6mA",
        "outputId": "9ec93e83-c993-42c1-e5a4-78bdcc82dbf4"
      },
      "source": [
        "lsbt.run_by_matrix(signals=signals_backtesting, verbose=True, plot_result=True)"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Running strategy based on a matrix of signals\n",
            "fixed costs 0.0 | proportional costs 0.0\n",
            "2017-06-16 | buying 4 units at 2433.15\n",
            "2017-06-16 | current balance 267.40\n",
            "2017-06-16 | current net wealth 10000.00\n",
            "2017-06-20 | selling 4 units at 2437.03\n",
            "2017-06-20 | current balance 10015.52\n",
            "2017-06-20 | current net wealth 10015.52\n",
            "2017-06-20 | selling 4 units at 2437.03\n",
            "2017-06-20 | current balance 19763.64\n",
            "2017-06-20 | current net wealth 10015.52\n",
            "2017-06-26 | buying 4 units at 2439.07\n",
            "2017-06-26 | current balance 10007.36\n",
            "2017-06-26 | current net wealth 10007.36\n",
            "2017-06-26 | buying 4 units at 2439.07\n",
            "2017-06-26 | current balance 251.08\n",
            "2017-06-26 | current net wealth 10007.36\n",
            "2017-07-03 | selling 4 units at 2429.01\n",
            "2017-07-03 | current balance 9967.12\n",
            "2017-07-03 | current net wealth 9967.12\n",
            "2017-07-03 | selling 4 units at 2429.01\n",
            "2017-07-03 | current balance 19683.16\n",
            "2017-07-03 | current net wealth 9967.12\n",
            "2017-07-05 | buying 4 units at 2432.54\n",
            "2017-07-05 | current balance 9953.00\n",
            "2017-07-05 | current net wealth 9953.00\n",
            "2017-07-05 | buying 4 units at 2432.54\n",
            "2017-07-05 | current balance 222.84\n",
            "2017-07-05 | current net wealth 9953.00\n",
            "2017-07-06 | selling 4 units at 2409.75\n",
            "2017-07-06 | current balance 9861.84\n",
            "2017-07-06 | current net wealth 9861.84\n",
            "2017-07-06 | selling 4 units at 2409.75\n",
            "2017-07-06 | current balance 19500.84\n",
            "2017-07-06 | current net wealth 9861.84\n",
            "2017-07-10 | buying 4 units at 2427.43\n",
            "2017-07-10 | current balance 9791.12\n",
            "2017-07-10 | current net wealth 9791.12\n",
            "2017-07-10 | buying 4 units at 2427.43\n",
            "2017-07-10 | current balance 81.40\n",
            "2017-07-10 | current net wealth 9791.12\n",
            "2017-08-09 | selling 4 units at 2474.02\n",
            "2017-08-09 | current balance 9977.48\n",
            "2017-08-09 | current net wealth 9977.48\n",
            "2017-08-09 | selling 4 units at 2474.02\n",
            "2017-08-09 | current balance 19873.56\n",
            "2017-08-09 | current net wealth 9977.48\n",
            "2017-08-11 | buying 4 units at 2441.32\n",
            "2017-08-11 | current balance 10108.28\n",
            "2017-08-11 | current net wealth 10108.28\n",
            "2017-08-11 | buying 4 units at 2441.32\n",
            "2017-08-11 | current balance 343.00\n",
            "2017-08-11 | current net wealth 10108.28\n",
            "2017-08-28 | selling 4 units at 2444.24\n",
            "2017-08-28 | current balance 10119.96\n",
            "2017-08-28 | current net wealth 10119.96\n",
            "2017-08-28 | selling 4 units at 2444.24\n",
            "2017-08-28 | current balance 19896.92\n",
            "2017-08-28 | current net wealth 10119.96\n",
            "2017-08-29 | buying 4 units at 2446.30\n",
            "2017-08-29 | current balance 10111.72\n",
            "2017-08-29 | current net wealth 10111.72\n",
            "2017-08-29 | buying 4 units at 2446.30\n",
            "2017-08-29 | current balance 326.52\n",
            "2017-08-29 | current net wealth 10111.72\n",
            "2017-09-08 | selling 4 units at 2461.43\n",
            "2017-09-08 | current balance 10172.24\n",
            "2017-09-08 | current net wealth 10172.24\n",
            "2017-09-08 | selling 4 units at 2461.43\n",
            "2017-09-08 | current balance 20017.96\n",
            "2017-09-08 | current net wealth 10172.24\n",
            "2017-09-11 | buying 4 units at 2488.11\n",
            "2017-09-11 | current balance 10065.52\n",
            "2017-09-11 | current net wealth 10065.52\n",
            "2017-09-11 | buying 4 units at 2488.11\n",
            "2017-09-11 | current balance 113.08\n",
            "2017-09-11 | current net wealth 10065.52\n",
            "2017-11-10 | selling 4 units at 2582.30\n",
            "2017-11-10 | current balance 10442.28\n",
            "2017-11-10 | current net wealth 10442.28\n",
            "2017-11-10 | selling 4 units at 2582.30\n",
            "2017-11-10 | current balance 20771.48\n",
            "2017-11-10 | current net wealth 10442.28\n",
            "2017-11-13 | buying 4 units at 2584.84\n",
            "2017-11-13 | current balance 10432.12\n",
            "2017-11-13 | current net wealth 10432.12\n",
            "2017-11-13 | buying 4 units at 2584.84\n",
            "2017-11-13 | current balance 92.76\n",
            "2017-11-13 | current net wealth 10432.12\n",
            "2018-01-29 | selling 4 units at 2853.53\n",
            "2018-01-29 | current balance 11506.88\n",
            "2018-01-29 | current net wealth 11506.88\n",
            "2018-01-29 | selling 4 units at 2853.53\n",
            "2018-01-29 | current balance 22921.00\n",
            "2018-01-29 | current net wealth 11506.88\n",
            "2018-01-30 | buying 4 units at 2822.43\n",
            "2018-01-30 | current balance 11631.28\n",
            "2018-01-30 | current net wealth 11631.28\n",
            "2018-01-30 | buying 4 units at 2822.43\n",
            "2018-01-30 | current balance 341.56\n",
            "2018-01-30 | current net wealth 11631.28\n",
            "2018-02-01 | selling 4 units at 2821.98\n",
            "2018-02-01 | current balance 11629.48\n",
            "2018-02-01 | current net wealth 11629.48\n",
            "2018-02-01 | selling 4 units at 2821.98\n",
            "2018-02-01 | current balance 22917.40\n",
            "2018-02-01 | current net wealth 11629.48\n",
            "2018-02-02 | buying 4 units at 2762.13\n",
            "2018-02-02 | current balance 11868.88\n",
            "2018-02-02 | current net wealth 11868.88\n",
            "2018-02-02 | buying 4 units at 2762.13\n",
            "2018-02-02 | current balance 820.36\n",
            "2018-02-02 | current net wealth 11868.88\n",
            "2018-02-20 | selling 4 units at 2716.26\n",
            "2018-02-20 | current balance 11685.40\n",
            "2018-02-20 | current net wealth 11685.40\n",
            "2018-02-20 | selling 4 units at 2716.26\n",
            "2018-02-20 | current balance 22550.44\n",
            "2018-02-20 | current net wealth 11685.40\n",
            "2018-02-21 | buying 4 units at 2701.33\n",
            "2018-02-21 | current balance 11745.12\n",
            "2018-02-21 | current net wealth 11745.12\n",
            "2018-02-21 | buying 4 units at 2701.33\n",
            "2018-02-21 | current balance 939.80\n",
            "2018-02-21 | current net wealth 11745.12\n",
            "2018-02-22 | selling 4 units at 2703.96\n",
            "2018-02-22 | current balance 11755.64\n",
            "2018-02-22 | current net wealth 11755.64\n",
            "2018-02-22 | selling 4 units at 2703.96\n",
            "2018-02-22 | current balance 22571.48\n",
            "2018-02-22 | current net wealth 11755.64\n",
            "2018-02-23 | buying 4 units at 2747.30\n",
            "2018-02-23 | current balance 11582.28\n",
            "2018-02-23 | current net wealth 11582.28\n",
            "2018-02-23 | buying 4 units at 2747.30\n",
            "2018-02-23 | current balance 593.08\n",
            "2018-02-23 | current net wealth 11582.28\n",
            "2018-02-27 | selling 4 units at 2744.28\n",
            "2018-02-27 | current balance 11570.20\n",
            "2018-02-27 | current net wealth 11570.20\n",
            "2018-02-27 | selling 4 units at 2744.28\n",
            "2018-02-27 | current balance 22547.32\n",
            "2018-02-27 | current net wealth 11570.20\n",
            "2018-02-28 | buying 4 units at 2713.83\n",
            "2018-02-28 | current balance 11692.00\n",
            "2018-02-28 | current net wealth 11692.00\n",
            "2018-02-28 | buying 4 units at 2713.83\n",
            "2018-02-28 | current balance 836.68\n",
            "2018-02-28 | current net wealth 11692.00\n",
            "2018-03-01 | selling 4 units at 2677.67\n",
            "2018-03-01 | current balance 11547.36\n",
            "2018-03-01 | current net wealth 11547.36\n",
            "2018-03-01 | selling 4 units at 2677.67\n",
            "2018-03-01 | current balance 22258.04\n",
            "2018-03-01 | current net wealth 11547.36\n",
            "2018-03-02 | buying 4 units at 2691.25\n",
            "2018-03-02 | current balance 11493.04\n",
            "2018-03-02 | current net wealth 11493.04\n",
            "2018-03-02 | buying 4 units at 2691.25\n",
            "2018-03-02 | current balance 728.04\n",
            "2018-03-02 | current net wealth 11493.04\n",
            "2018-03-13 | selling 4 units at 2765.31\n",
            "2018-03-13 | current balance 11789.28\n",
            "2018-03-13 | current net wealth 11789.28\n",
            "2018-03-13 | selling 4 units at 2765.31\n",
            "2018-03-13 | current balance 22850.52\n",
            "2018-03-13 | current net wealth 11789.28\n",
            "2018-03-16 | buying 4 units at 2752.01\n",
            "2018-03-16 | current balance 11842.48\n",
            "2018-03-16 | current net wealth 11842.48\n",
            "2018-03-16 | buying 4 units at 2752.01\n",
            "2018-03-16 | current balance 834.44\n",
            "2018-03-16 | current net wealth 11842.48\n",
            "2018-03-19 | selling 4 units at 2712.92\n",
            "2018-03-19 | current balance 11686.12\n",
            "2018-03-19 | current net wealth 11686.12\n",
            "2018-03-19 | selling 4 units at 2712.92\n",
            "2018-03-19 | current balance 22537.80\n",
            "2018-03-19 | current net wealth 11686.12\n",
            "2018-03-22 | buying 4 units at 2643.69\n",
            "2018-03-22 | current balance 11963.04\n",
            "2018-03-22 | current net wealth 11963.04\n",
            "2018-03-22 | buying 4 units at 2643.69\n",
            "2018-03-22 | current balance 1388.28\n",
            "2018-03-22 | current net wealth 11963.04\n",
            "2018-03-26 | selling 4 units at 2658.55\n",
            "2018-03-26 | current balance 12022.48\n",
            "2018-03-26 | current net wealth 12022.48\n",
            "2018-03-26 | selling 4 units at 2658.55\n",
            "2018-03-26 | current balance 22656.68\n",
            "2018-03-26 | current net wealth 12022.48\n",
            "2018-03-27 | buying 4 units at 2612.62\n",
            "2018-03-27 | current balance 12206.20\n",
            "2018-03-27 | current net wealth 12206.20\n",
            "2018-03-27 | buying 4 units at 2612.62\n",
            "2018-03-27 | current balance 1755.72\n",
            "2018-03-27 | current net wealth 12206.20\n",
            "2018-04-19 | selling 4 units at 2693.13\n",
            "2018-04-19 | current balance 12528.24\n",
            "2018-04-19 | current net wealth 12528.24\n",
            "2018-04-19 | selling 4 units at 2693.13\n",
            "2018-04-19 | current balance 23300.76\n",
            "2018-04-19 | current net wealth 12528.24\n",
            "2018-04-20 | buying 4 units at 2670.14\n",
            "2018-04-20 | current balance 12620.20\n",
            "2018-04-20 | current net wealth 12620.20\n",
            "2018-04-20 | buying 4 units at 2670.14\n",
            "2018-04-20 | current balance 1939.64\n",
            "2018-04-20 | current net wealth 12620.20\n",
            "2018-04-23 | selling 4 units at 2670.29\n",
            "2018-04-23 | current balance 12620.80\n",
            "2018-04-23 | current net wealth 12620.80\n",
            "2018-04-23 | selling 4 units at 2670.29\n",
            "2018-04-23 | current balance 23301.96\n",
            "2018-04-23 | current net wealth 12620.80\n",
            "2018-04-26 | buying 4 units at 2666.94\n",
            "2018-04-26 | current balance 12634.20\n",
            "2018-04-26 | current net wealth 12634.20\n",
            "2018-04-26 | buying 4 units at 2666.94\n",
            "2018-04-26 | current balance 1966.44\n",
            "2018-04-26 | current net wealth 12634.20\n",
            "2018-05-15 | selling 4 units at 2711.45\n",
            "2018-05-15 | current balance 12812.24\n",
            "2018-05-15 | current net wealth 12812.24\n",
            "2018-05-15 | selling 4 units at 2711.45\n",
            "2018-05-15 | current balance 23658.04\n",
            "2018-05-15 | current net wealth 12812.24\n",
            "2018-05-16 | buying 4 units at 2722.46\n",
            "2018-05-16 | current balance 12768.20\n",
            "2018-05-16 | current net wealth 12768.20\n",
            "2018-05-16 | buying 4 units at 2722.46\n",
            "2018-05-16 | current balance 1878.36\n",
            "2018-05-16 | current net wealth 12768.20\n",
            "2018-06-15 | selling 4 units at 2779.66\n",
            "2018-06-15 | current balance 12997.00\n",
            "2018-06-15 | current net wealth 12997.00\n",
            "2018-06-15 | selling 4 units at 2779.66\n",
            "2018-06-15 | current balance 24115.64\n",
            "2018-06-15 | current net wealth 12997.00\n",
            "2018-06-18 | buying 4 units at 2773.75\n",
            "2018-06-18 | current balance 13020.64\n",
            "2018-06-18 | current net wealth 13020.64\n",
            "2018-06-18 | buying 4 units at 2773.75\n",
            "2018-06-18 | current balance 1925.64\n",
            "2018-06-18 | current net wealth 13020.64\n",
            "2018-06-19 | selling 4 units at 2762.59\n",
            "2018-06-19 | current balance 12976.00\n",
            "2018-06-19 | current net wealth 12976.00\n",
            "2018-06-19 | selling 4 units at 2762.59\n",
            "2018-06-19 | current balance 24026.36\n",
            "2018-06-19 | current net wealth 12976.00\n",
            "2018-06-20 | buying 4 units at 2767.32\n",
            "2018-06-20 | current balance 12957.08\n",
            "2018-06-20 | current net wealth 12957.08\n",
            "2018-06-20 | buying 4 units at 2767.32\n",
            "2018-06-20 | current balance 1887.80\n",
            "2018-06-20 | current net wealth 12957.08\n",
            "2018-06-21 | selling 4 units at 2749.76\n",
            "2018-06-21 | current balance 12886.84\n",
            "2018-06-21 | current net wealth 12886.84\n",
            "2018-06-21 | selling 4 units at 2749.76\n",
            "2018-06-21 | current balance 23885.88\n",
            "2018-06-21 | current net wealth 12886.84\n",
            "2018-06-22 | buying 4 units at 2754.88\n",
            "2018-06-22 | current balance 12866.36\n",
            "2018-06-22 | current net wealth 12866.36\n",
            "2018-06-22 | buying 4 units at 2754.88\n",
            "2018-06-22 | current balance 1846.84\n",
            "2018-06-22 | current net wealth 12866.36\n",
            "2018-06-26 | selling 4 units at 2723.06\n",
            "2018-06-26 | current balance 12739.08\n",
            "2018-06-26 | current net wealth 12739.08\n",
            "2018-06-26 | selling 4 units at 2723.06\n",
            "2018-06-26 | current balance 23631.32\n",
            "2018-06-26 | current net wealth 12739.08\n",
            "2018-06-27 | buying 4 units at 2699.63\n",
            "2018-06-27 | current balance 12832.80\n",
            "2018-06-27 | current net wealth 12832.80\n",
            "2018-06-27 | buying 4 units at 2699.63\n",
            "2018-06-27 | current balance 2034.28\n",
            "2018-06-27 | current net wealth 12832.80\n",
            "2018-07-11 | selling 4 units at 2774.02\n",
            "2018-07-11 | current balance 13130.36\n",
            "2018-07-11 | current net wealth 13130.36\n",
            "2018-07-11 | selling 4 units at 2774.02\n",
            "2018-07-11 | current balance 24226.44\n",
            "2018-07-11 | current net wealth 13130.36\n",
            "2018-07-12 | buying 4 units at 2798.29\n",
            "2018-07-12 | current balance 13033.28\n",
            "2018-07-12 | current net wealth 13033.28\n",
            "2018-07-12 | buying 4 units at 2798.29\n",
            "2018-07-12 | current balance 1840.12\n",
            "2018-07-12 | current net wealth 13033.28\n",
            "2018-07-20 | selling 4 units at 2801.83\n",
            "2018-07-20 | current balance 13047.44\n",
            "2018-07-20 | current net wealth 13047.44\n",
            "2018-07-20 | selling 4 units at 2801.83\n",
            "2018-07-20 | current balance 24254.76\n",
            "2018-07-20 | current net wealth 13047.44\n",
            "2018-07-23 | buying 4 units at 2806.98\n",
            "2018-07-23 | current balance 13026.84\n",
            "2018-07-23 | current net wealth 13026.84\n",
            "2018-07-23 | buying 4 units at 2806.98\n",
            "2018-07-23 | current balance 1798.92\n",
            "2018-07-23 | current net wealth 13026.84\n",
            "2018-08-09 | selling 4 units at 2853.58\n",
            "2018-08-09 | current balance 13213.24\n",
            "2018-08-09 | current net wealth 13213.24\n",
            "2018-08-09 | selling 4 units at 2853.58\n",
            "2018-08-09 | current balance 24627.56\n",
            "2018-08-09 | current net wealth 13213.24\n",
            "2018-08-10 | buying 4 units at 2833.28\n",
            "2018-08-10 | current balance 13294.44\n",
            "2018-08-10 | current net wealth 13294.44\n",
            "2018-08-10 | buying 4 units at 2833.28\n",
            "2018-08-10 | current balance 1961.32\n",
            "2018-08-10 | current net wealth 13294.44\n",
            "2018-08-30 | selling 4 units at 2901.13\n",
            "2018-08-30 | current balance 13565.84\n",
            "2018-08-30 | current net wealth 13565.84\n",
            "2018-08-30 | selling 4 units at 2901.13\n",
            "2018-08-30 | current balance 25170.36\n",
            "2018-08-30 | current net wealth 13565.84\n",
            "2018-09-05 | buying 4 units at 2888.60\n",
            "2018-09-05 | current balance 13615.96\n",
            "2018-09-05 | current net wealth 13615.96\n",
            "2018-09-05 | buying 4 units at 2888.60\n",
            "2018-09-05 | current balance 2061.56\n",
            "2018-09-05 | current net wealth 13615.96\n",
            "2018-09-07 | selling 4 units at 2871.68\n",
            "2018-09-07 | current balance 13548.28\n",
            "2018-09-07 | current net wealth 13548.28\n",
            "2018-09-07 | selling 4 units at 2871.68\n",
            "2018-09-07 | current balance 25035.00\n",
            "2018-09-07 | current net wealth 13548.28\n",
            "2018-09-10 | buying 4 units at 2877.13\n",
            "2018-09-10 | current balance 13526.48\n",
            "2018-09-10 | current net wealth 13526.48\n",
            "2018-09-10 | buying 4 units at 2877.13\n",
            "2018-09-10 | current balance 2017.96\n",
            "2018-09-10 | current net wealth 13526.48\n",
            "2018-09-25 | selling 4 units at 2915.56\n",
            "2018-09-25 | current balance 13680.20\n",
            "2018-09-25 | current net wealth 13680.20\n",
            "2018-09-25 | selling 4 units at 2915.56\n",
            "2018-09-25 | current balance 25342.44\n",
            "2018-09-25 | current net wealth 13680.20\n",
            "2018-09-26 | buying 4 units at 2905.97\n",
            "2018-09-26 | current balance 13718.56\n",
            "2018-09-26 | current net wealth 13718.56\n",
            "2018-09-26 | buying 4 units at 2905.97\n",
            "2018-09-26 | current balance 2094.68\n",
            "2018-09-26 | current net wealth 13718.56\n",
            "2018-09-27 | selling 4 units at 2914.00\n",
            "2018-09-27 | current balance 13750.68\n",
            "2018-09-27 | current net wealth 13750.68\n",
            "2018-09-27 | selling 4 units at 2914.00\n",
            "2018-09-27 | current balance 25406.68\n",
            "2018-09-27 | current net wealth 13750.68\n",
            "2018-09-28 | buying 4 units at 2913.98\n",
            "2018-09-28 | current balance 13750.76\n",
            "2018-09-28 | current net wealth 13750.76\n",
            "2018-09-28 | buying 4 units at 2913.98\n",
            "2018-09-28 | current balance 2094.84\n",
            "2018-09-28 | current net wealth 13750.76\n",
            "2018-10-04 | selling 4 units at 2901.61\n",
            "2018-10-04 | current balance 13701.28\n",
            "2018-10-04 | current net wealth 13701.28\n",
            "2018-10-04 | selling 4 units at 2901.61\n",
            "2018-10-04 | current balance 25307.72\n",
            "2018-10-04 | current net wealth 13701.28\n",
            "2018-10-10 | buying 4 units at 2785.68\n",
            "2018-10-10 | current balance 14165.00\n",
            "2018-10-10 | current net wealth 14165.00\n",
            "2018-10-10 | buying 5 units at 2785.68\n",
            "2018-10-10 | current balance 236.60\n",
            "2018-10-10 | current net wealth 14165.00\n",
            "2018-10-15 | selling 5 units at 2750.79\n",
            "2018-10-15 | current balance 13990.55\n",
            "2018-10-15 | current net wealth 13990.55\n",
            "2018-10-15 | selling 5 units at 2750.79\n",
            "2018-10-15 | current balance 27744.50\n",
            "2018-10-15 | current net wealth 13990.55\n",
            "2018-10-16 | buying 5 units at 2809.92\n",
            "2018-10-16 | current balance 13694.90\n",
            "2018-10-16 | current net wealth 13694.90\n",
            "2018-10-16 | buying 4 units at 2809.92\n",
            "2018-10-16 | current balance 2455.22\n",
            "2018-10-16 | current net wealth 13694.90\n",
            "2018-10-18 | selling 4 units at 2768.78\n",
            "2018-10-18 | current balance 13530.34\n",
            "2018-10-18 | current net wealth 13530.34\n",
            "2018-10-18 | selling 4 units at 2768.78\n",
            "2018-10-18 | current balance 24605.46\n",
            "2018-10-18 | current net wealth 13530.34\n",
            "2018-10-23 | buying 4 units at 2740.69\n",
            "2018-10-23 | current balance 13642.70\n",
            "2018-10-23 | current net wealth 13642.70\n",
            "2018-10-23 | buying 4 units at 2740.69\n",
            "2018-10-23 | current balance 2679.94\n",
            "2018-10-23 | current net wealth 13642.70\n",
            "2018-10-26 | selling 4 units at 2658.69\n",
            "2018-10-26 | current balance 13314.70\n",
            "2018-10-26 | current net wealth 13314.70\n",
            "2018-10-26 | selling 5 units at 2658.69\n",
            "2018-10-26 | current balance 26608.15\n",
            "2018-10-26 | current net wealth 13314.70\n",
            "2018-10-29 | buying 5 units at 2641.25\n",
            "2018-10-29 | current balance 13401.90\n",
            "2018-10-29 | current net wealth 13401.90\n",
            "2018-10-29 | buying 5 units at 2641.25\n",
            "2018-10-29 | current balance 195.65\n",
            "2018-10-29 | current net wealth 13401.90\n",
            "2018-11-13 | selling 5 units at 2722.18\n",
            "2018-11-13 | current balance 13806.55\n",
            "2018-11-13 | current net wealth 13806.55\n",
            "2018-11-13 | selling 5 units at 2722.18\n",
            "2018-11-13 | current balance 27417.45\n",
            "2018-11-13 | current net wealth 13806.55\n",
            "2018-11-15 | buying 5 units at 2730.20\n",
            "2018-11-15 | current balance 13766.45\n",
            "2018-11-15 | current net wealth 13766.45\n",
            "2018-11-15 | buying 5 units at 2730.20\n",
            "2018-11-15 | current balance 115.45\n",
            "2018-11-15 | current net wealth 13766.45\n",
            "2018-11-19 | selling 5 units at 2690.73\n",
            "2018-11-19 | current balance 13569.10\n",
            "2018-11-19 | current net wealth 13569.10\n",
            "2018-11-19 | selling 5 units at 2690.73\n",
            "2018-11-19 | current balance 27022.75\n",
            "2018-11-19 | current net wealth 13569.10\n",
            "2018-11-23 | buying 5 units at 2632.56\n",
            "2018-11-23 | current balance 13859.95\n",
            "2018-11-23 | current net wealth 13859.95\n",
            "2018-11-23 | buying 5 units at 2632.56\n",
            "2018-11-23 | current balance 697.15\n",
            "2018-11-23 | current net wealth 13859.95\n",
            "2018-12-06 | selling 5 units at 2695.95\n",
            "2018-12-06 | current balance 14176.90\n",
            "2018-12-06 | current net wealth 14176.90\n",
            "2018-12-06 | selling 5 units at 2695.95\n",
            "2018-12-06 | current balance 27656.65\n",
            "2018-12-06 | current net wealth 14176.90\n",
            "2018-12-07 | buying 5 units at 2633.08\n",
            "2018-12-07 | current balance 14491.25\n",
            "2018-12-07 | current net wealth 14491.25\n",
            "2018-12-07 | buying 5 units at 2633.08\n",
            "2018-12-07 | current balance 1325.85\n",
            "2018-12-07 | current net wealth 14491.25\n",
            "2018-12-10 | selling 5 units at 2637.72\n",
            "2018-12-10 | current balance 14514.45\n",
            "2018-12-10 | current net wealth 14514.45\n",
            "2018-12-10 | selling 5 units at 2637.72\n",
            "2018-12-10 | current balance 27703.05\n",
            "2018-12-10 | current net wealth 14514.45\n",
            "2018-12-12 | buying 5 units at 2651.07\n",
            "2018-12-12 | current balance 14447.70\n",
            "2018-12-12 | current net wealth 14447.70\n",
            "2018-12-12 | buying 5 units at 2651.07\n",
            "2018-12-12 | current balance 1192.35\n",
            "2018-12-12 | current net wealth 14447.70\n",
            "2018-12-18 | selling 5 units at 2546.16\n",
            "2018-12-18 | current balance 13923.15\n",
            "2018-12-18 | current net wealth 13923.15\n",
            "2018-12-18 | selling 5 units at 2546.16\n",
            "2018-12-18 | current balance 26653.95\n",
            "2018-12-18 | current net wealth 13923.15\n",
            "2018-12-20 | buying 5 units at 2467.42\n",
            "2018-12-20 | current balance 14316.85\n",
            "2018-12-20 | current net wealth 14316.85\n",
            "2018-12-20 | buying 5 units at 2467.42\n",
            "2018-12-20 | current balance 1979.75\n",
            "2018-12-20 | current net wealth 14316.85\n",
            "2018-12-24 | selling 5 units at 2351.10\n",
            "2018-12-24 | current balance 13735.25\n",
            "2018-12-24 | current net wealth 13735.25\n",
            "2018-12-24 | selling 5 units at 2351.10\n",
            "2018-12-24 | current balance 25490.75\n",
            "2018-12-24 | current net wealth 13735.25\n",
            "2018-12-27 | buying 5 units at 2488.83\n",
            "2018-12-27 | current balance 13046.60\n",
            "2018-12-27 | current net wealth 13046.60\n",
            "2018-12-27 | buying 5 units at 2488.83\n",
            "2018-12-27 | current balance 602.45\n",
            "2018-12-27 | current net wealth 13046.60\n",
            "2019-01-29 | selling 5 units at 2640.00\n",
            "2019-01-29 | current balance 13802.45\n",
            "2019-01-29 | current net wealth 13802.45\n",
            "2019-01-29 | selling 5 units at 2640.00\n",
            "2019-01-29 | current balance 27002.45\n",
            "2019-01-29 | current net wealth 13802.45\n",
            "2019-01-30 | buying 5 units at 2681.05\n",
            "2019-01-30 | current balance 13597.20\n",
            "2019-01-30 | current net wealth 13597.20\n",
            "2019-01-30 | buying 5 units at 2681.05\n",
            "2019-01-30 | current balance 191.95\n",
            "2019-01-30 | current net wealth 13597.20\n",
            "2019-02-07 | selling 5 units at 2706.05\n",
            "2019-02-07 | current balance 13722.20\n",
            "2019-02-07 | current net wealth 13722.20\n",
            "2019-02-07 | selling 5 units at 2706.05\n",
            "2019-02-07 | current balance 27252.45\n",
            "2019-02-07 | current net wealth 13722.20\n",
            "2019-02-08 | buying 5 units at 2707.88\n",
            "2019-02-08 | current balance 13713.05\n",
            "2019-02-08 | current net wealth 13713.05\n",
            "2019-02-08 | buying 5 units at 2707.88\n",
            "2019-02-08 | current balance 173.65\n",
            "2019-02-08 | current net wealth 13713.05\n",
            "2019-02-21 | selling 5 units at 2774.88\n",
            "2019-02-21 | current balance 14048.05\n",
            "2019-02-21 | current net wealth 14048.05\n",
            "2019-02-21 | selling 5 units at 2774.88\n",
            "2019-02-21 | current balance 27922.45\n",
            "2019-02-21 | current net wealth 14048.05\n",
            "2019-02-22 | buying 5 units at 2792.67\n",
            "2019-02-22 | current balance 13959.10\n",
            "2019-02-22 | current net wealth 13959.10\n",
            "2019-02-22 | buying 4 units at 2792.67\n",
            "2019-02-22 | current balance 2788.42\n",
            "2019-02-22 | current net wealth 13959.10\n",
            "2019-02-27 | selling 4 units at 2792.38\n",
            "2019-02-27 | current balance 13957.94\n",
            "2019-02-27 | current net wealth 13957.94\n",
            "2019-02-27 | selling 4 units at 2792.38\n",
            "2019-02-27 | current balance 25127.46\n",
            "2019-02-27 | current net wealth 13957.94\n",
            "2019-03-01 | buying 4 units at 2803.69\n",
            "2019-03-01 | current balance 13912.70\n",
            "2019-03-01 | current net wealth 13912.70\n",
            "2019-03-01 | buying 4 units at 2803.69\n",
            "2019-03-01 | current balance 2697.94\n",
            "2019-03-01 | current net wealth 13912.70\n",
            "2019-03-04 | selling 4 units at 2792.81\n",
            "2019-03-04 | current balance 13869.18\n",
            "2019-03-04 | current net wealth 13869.18\n",
            "2019-03-04 | selling 4 units at 2792.81\n",
            "2019-03-04 | current balance 25040.42\n",
            "2019-03-04 | current net wealth 13869.18\n",
            "2019-03-06 | buying 4 units at 2771.45\n",
            "2019-03-06 | current balance 13954.62\n",
            "2019-03-06 | current net wealth 13954.62\n",
            "2019-03-06 | buying 5 units at 2771.45\n",
            "2019-03-06 | current balance 97.37\n",
            "2019-03-06 | current net wealth 13954.62\n",
            "2019-03-20 | selling 5 units at 2824.23\n",
            "2019-03-20 | current balance 14218.52\n",
            "2019-03-20 | current net wealth 14218.52\n",
            "2019-03-20 | selling 5 units at 2824.23\n",
            "2019-03-20 | current balance 28339.67\n",
            "2019-03-20 | current net wealth 14218.52\n",
            "2019-03-21 | buying 5 units at 2854.88\n",
            "2019-03-21 | current balance 14065.27\n",
            "2019-03-21 | current net wealth 14065.27\n",
            "2019-03-21 | buying 4 units at 2854.88\n",
            "2019-03-21 | current balance 2645.75\n",
            "2019-03-21 | current net wealth 14065.27\n",
            "2019-04-09 | selling 4 units at 2878.20\n",
            "2019-04-09 | current balance 14158.55\n",
            "2019-04-09 | current net wealth 14158.55\n",
            "2019-04-09 | selling 4 units at 2878.20\n",
            "2019-04-09 | current balance 25671.35\n",
            "2019-04-09 | current net wealth 14158.55\n",
            "2019-04-10 | buying 4 units at 2888.21\n",
            "2019-04-10 | current balance 14118.51\n",
            "2019-04-10 | current net wealth 14118.51\n",
            "2019-04-10 | buying 4 units at 2888.21\n",
            "2019-04-10 | current balance 2565.67\n",
            "2019-04-10 | current net wealth 14118.51\n",
            "2019-04-11 | selling 4 units at 2888.32\n",
            "2019-04-11 | current balance 14118.95\n",
            "2019-04-11 | current net wealth 14118.95\n",
            "2019-04-11 | selling 4 units at 2888.32\n",
            "2019-04-11 | current balance 25672.23\n",
            "2019-04-11 | current net wealth 14118.95\n",
            "2019-04-12 | buying 4 units at 2907.41\n",
            "2019-04-12 | current balance 14042.59\n",
            "2019-04-12 | current net wealth 14042.59\n",
            "2019-04-12 | buying 4 units at 2907.41\n",
            "2019-04-12 | current balance 2412.95\n",
            "2019-04-12 | current net wealth 14042.59\n",
            "2019-04-18 | selling 4 units at 2905.03\n",
            "2019-04-18 | current balance 14033.07\n",
            "2019-04-18 | current net wealth 14033.07\n",
            "2019-04-18 | selling 4 units at 2905.03\n",
            "2019-04-18 | current balance 25653.19\n",
            "2019-04-18 | current net wealth 14033.07\n",
            "2019-04-22 | buying 4 units at 2907.97\n",
            "2019-04-22 | current balance 14021.31\n",
            "2019-04-22 | current net wealth 14021.31\n",
            "2019-04-22 | buying 4 units at 2907.97\n",
            "2019-04-22 | current balance 2389.43\n",
            "2019-04-22 | current net wealth 14021.31\n",
            "2019-05-01 | selling 4 units at 2923.73\n",
            "2019-05-01 | current balance 14084.35\n",
            "2019-05-01 | current net wealth 14084.35\n",
            "2019-05-01 | selling 4 units at 2923.73\n",
            "2019-05-01 | current balance 25779.27\n",
            "2019-05-01 | current net wealth 14084.35\n",
            "2019-05-03 | buying 4 units at 2945.64\n",
            "2019-05-03 | current balance 13996.71\n",
            "2019-05-03 | current net wealth 13996.71\n",
            "2019-05-03 | buying 4 units at 2945.64\n",
            "2019-05-03 | current balance 2214.15\n",
            "2019-05-03 | current net wealth 13996.71\n",
            "2019-05-07 | selling 4 units at 2884.05\n",
            "2019-05-07 | current balance 13750.35\n",
            "2019-05-07 | current net wealth 13750.35\n",
            "2019-05-07 | selling 4 units at 2884.05\n",
            "2019-05-07 | current balance 25286.55\n",
            "2019-05-07 | current net wealth 13750.35\n",
            "2019-05-08 | buying 4 units at 2879.42\n",
            "2019-05-08 | current balance 13768.87\n",
            "2019-05-08 | current net wealth 13768.87\n",
            "2019-05-08 | buying 4 units at 2879.42\n",
            "2019-05-08 | current balance 2251.19\n",
            "2019-05-08 | current net wealth 13768.87\n",
            "2019-05-09 | selling 4 units at 2870.72\n",
            "2019-05-09 | current balance 13734.07\n",
            "2019-05-09 | current net wealth 13734.07\n",
            "2019-05-09 | selling 4 units at 2870.72\n",
            "2019-05-09 | current balance 25216.95\n",
            "2019-05-09 | current net wealth 13734.07\n",
            "2019-05-13 | buying 4 units at 2811.87\n",
            "2019-05-13 | current balance 13969.47\n",
            "2019-05-13 | current net wealth 13969.47\n",
            "2019-05-13 | buying 4 units at 2811.87\n",
            "2019-05-13 | current balance 2721.99\n",
            "2019-05-13 | current net wealth 13969.47\n",
            "2019-05-14 | selling 4 units at 2834.41\n",
            "2019-05-14 | current balance 14059.63\n",
            "2019-05-14 | current net wealth 14059.63\n",
            "2019-05-14 | selling 4 units at 2834.41\n",
            "2019-05-14 | current balance 25397.27\n",
            "2019-05-14 | current net wealth 14059.63\n",
            "2019-05-15 | buying 4 units at 2850.96\n",
            "2019-05-15 | current balance 13993.43\n",
            "2019-05-15 | current net wealth 13993.43\n",
            "2019-05-15 | buying 4 units at 2850.96\n",
            "2019-05-15 | current balance 2589.59\n",
            "2019-05-15 | current net wealth 13993.43\n",
            "2019-05-20 | selling 4 units at 2840.23\n",
            "2019-05-20 | current balance 13950.51\n",
            "2019-05-20 | current net wealth 13950.51\n",
            "2019-05-20 | selling 4 units at 2840.23\n",
            "2019-05-20 | current balance 25311.43\n",
            "2019-05-20 | current net wealth 13950.51\n",
            "2019-05-21 | buying 4 units at 2864.36\n",
            "2019-05-21 | current balance 13853.99\n",
            "2019-05-21 | current net wealth 13853.99\n",
            "2019-05-21 | buying 4 units at 2864.36\n",
            "2019-05-21 | current balance 2396.55\n",
            "2019-05-21 | current net wealth 13853.99\n",
            "2019-05-23 | selling 4 units at 2822.24\n",
            "2019-05-23 | current balance 13685.51\n",
            "2019-05-23 | current net wealth 13685.51\n",
            "2019-05-23 | selling 4 units at 2822.24\n",
            "2019-05-23 | current balance 24974.47\n",
            "2019-05-23 | current net wealth 13685.51\n",
            "2019-05-29 | buying 4 units at 2783.02\n",
            "2019-05-29 | current balance 13842.39\n",
            "2019-05-29 | current net wealth 13842.39\n",
            "2019-05-29 | buying 4 units at 2783.02\n",
            "2019-05-29 | current balance 2710.31\n",
            "2019-05-29 | current net wealth 13842.39\n",
            "2019-06-03 | selling 4 units at 2744.45\n",
            "2019-06-03 | current balance 13688.11\n",
            "2019-06-03 | current net wealth 13688.11\n",
            "2019-06-03 | selling 4 units at 2744.45\n",
            "2019-06-03 | current balance 24665.91\n",
            "2019-06-03 | current net wealth 13688.11\n",
            "2019-06-04 | buying 4 units at 2803.27\n",
            "2019-06-04 | current balance 13452.83\n",
            "2019-06-04 | current net wealth 13452.83\n",
            "2019-06-04 | buying 4 units at 2803.27\n",
            "2019-06-04 | current balance 2239.75\n",
            "2019-06-04 | current net wealth 13452.83\n",
            "2019-07-25 | selling 4 units at 3003.67\n",
            "2019-07-25 | current balance 14254.43\n",
            "2019-07-25 | current net wealth 14254.43\n",
            "2019-07-25 | selling 4 units at 3003.67\n",
            "2019-07-25 | current balance 26269.11\n",
            "2019-07-25 | current net wealth 14254.43\n",
            "2019-07-26 | buying 4 units at 3025.86\n",
            "2019-07-26 | current balance 14165.67\n",
            "2019-07-26 | current net wealth 14165.67\n",
            "2019-07-26 | buying 4 units at 3025.86\n",
            "2019-07-26 | current balance 2062.23\n",
            "2019-07-26 | current net wealth 14165.67\n",
            "2019-07-30 | selling 4 units at 3013.18\n",
            "2019-07-30 | current balance 14114.95\n",
            "2019-07-30 | current net wealth 14114.95\n",
            "2019-07-30 | selling 4 units at 3013.18\n",
            "2019-07-30 | current balance 26167.67\n",
            "2019-07-30 | current net wealth 14114.95\n",
            "2019-08-05 | buying 4 units at 2844.74\n",
            "2019-08-05 | current balance 14788.71\n",
            "2019-08-05 | current net wealth 14788.71\n",
            "2019-08-05 | buying 5 units at 2844.74\n",
            "2019-08-05 | current balance 565.01\n",
            "2019-08-05 | current net wealth 14788.71\n",
            "2019-08-07 | selling 5 units at 2883.98\n",
            "2019-08-07 | current balance 14984.91\n",
            "2019-08-07 | current net wealth 14984.91\n",
            "2019-08-07 | selling 5 units at 2883.98\n",
            "2019-08-07 | current balance 29404.81\n",
            "2019-08-07 | current net wealth 14984.91\n",
            "2019-08-08 | buying 5 units at 2938.09\n",
            "2019-08-08 | current balance 14714.36\n",
            "2019-08-08 | current net wealth 14714.36\n",
            "2019-08-08 | buying 5 units at 2938.09\n",
            "2019-08-08 | current balance 23.91\n",
            "2019-08-08 | current net wealth 14714.36\n",
            "2019-08-15 | selling 5 units at 2847.60\n",
            "2019-08-15 | current balance 14261.91\n",
            "2019-08-15 | current net wealth 14261.91\n",
            "2019-08-15 | selling 5 units at 2847.60\n",
            "2019-08-15 | current balance 28499.91\n",
            "2019-08-15 | current net wealth 14261.91\n",
            "2019-08-16 | buying 5 units at 2888.68\n",
            "2019-08-16 | current balance 14056.51\n",
            "2019-08-16 | current net wealth 14056.51\n",
            "2019-08-16 | buying 4 units at 2888.68\n",
            "2019-08-16 | current balance 2501.79\n",
            "2019-08-16 | current net wealth 14056.51\n",
            "2019-09-16 | selling 4 units at 2997.96\n",
            "2019-09-16 | current balance 14493.63\n",
            "2019-09-16 | current net wealth 14493.63\n",
            "2019-09-16 | selling 4 units at 2997.96\n",
            "2019-09-16 | current balance 26485.47\n",
            "2019-09-16 | current net wealth 14493.63\n",
            "2019-09-17 | buying 4 units at 3005.70\n",
            "2019-09-17 | current balance 14462.67\n",
            "2019-09-17 | current net wealth 14462.67\n",
            "2019-09-17 | buying 4 units at 3005.70\n",
            "2019-09-17 | current balance 2439.87\n",
            "2019-09-17 | current net wealth 14462.67\n",
            "2019-09-23 | selling 4 units at 2991.78\n",
            "2019-09-23 | current balance 14406.99\n",
            "2019-09-23 | current net wealth 14406.99\n",
            "2019-09-23 | selling 4 units at 2991.78\n",
            "2019-09-23 | current balance 26374.11\n",
            "2019-09-23 | current net wealth 14406.99\n",
            "2019-09-25 | buying 4 units at 2984.87\n",
            "2019-09-25 | current balance 14434.63\n",
            "2019-09-25 | current net wealth 14434.63\n",
            "2019-09-25 | buying 4 units at 2984.87\n",
            "2019-09-25 | current balance 2495.15\n",
            "2019-09-25 | current net wealth 14434.63\n",
            "2019-09-26 | selling 4 units at 2977.62\n",
            "2019-09-26 | current balance 14405.63\n",
            "2019-09-26 | current net wealth 14405.63\n",
            "2019-09-26 | selling 4 units at 2977.62\n",
            "2019-09-26 | current balance 26316.11\n",
            "2019-09-26 | current net wealth 14405.63\n",
            "2019-09-27 | buying 4 units at 2961.79\n",
            "2019-09-27 | current balance 14468.95\n",
            "2019-09-27 | current net wealth 14468.95\n",
            "2019-09-27 | buying 4 units at 2961.79\n",
            "2019-09-27 | current balance 2621.79\n",
            "2019-09-27 | current net wealth 14468.95\n",
            "2019-10-01 | selling 4 units at 2940.25\n",
            "2019-10-01 | current balance 14382.79\n",
            "2019-10-01 | current net wealth 14382.79\n",
            "2019-10-01 | selling 4 units at 2940.25\n",
            "2019-10-01 | current balance 26143.79\n",
            "2019-10-01 | current net wealth 14382.79\n",
            "2019-10-04 | buying 4 units at 2952.01\n",
            "2019-10-04 | current balance 14335.75\n",
            "2019-10-04 | current net wealth 14335.75\n",
            "2019-10-04 | buying 4 units at 2952.01\n",
            "2019-10-04 | current balance 2527.71\n",
            "2019-10-04 | current net wealth 14335.75\n",
            "2019-12-10 | selling 4 units at 3132.52\n",
            "2019-12-10 | current balance 15057.79\n",
            "2019-12-10 | current net wealth 15057.79\n",
            "2019-12-10 | selling 4 units at 3132.52\n",
            "2019-12-10 | current balance 27587.87\n",
            "2019-12-10 | current net wealth 15057.79\n",
            "2019-12-11 | buying 4 units at 3141.63\n",
            "2019-12-11 | current balance 15021.35\n",
            "2019-12-11 | current net wealth 15021.35\n",
            "2019-12-11 | buying 4 units at 3141.63\n",
            "2019-12-11 | current balance 2454.83\n",
            "2019-12-11 | current net wealth 15021.35\n",
            "2019-12-30 | selling 4 units at 3221.29\n",
            "2019-12-30 | current balance 15339.99\n",
            "2019-12-30 | current net wealth 15339.99\n",
            "2019-12-30 | selling 4 units at 3221.29\n",
            "2019-12-30 | current balance 28225.15\n",
            "2019-12-30 | current net wealth 15339.99\n",
            "2019-12-31 | buying 4 units at 3230.78\n",
            "2019-12-31 | current balance 15302.03\n",
            "2019-12-31 | current net wealth 15302.03\n",
            "2019-12-31 | buying 4 units at 3230.78\n",
            "2019-12-31 | current balance 2378.91\n",
            "2019-12-31 | current net wealth 15302.03\n",
            "2020-01-06 | selling 4 units at 3246.28\n",
            "2020-01-06 | current balance 15364.03\n",
            "2020-01-06 | current net wealth 15364.03\n",
            "2020-01-06 | selling 4 units at 3246.28\n",
            "2020-01-06 | current balance 28349.15\n",
            "2020-01-06 | current net wealth 15364.03\n",
            "2020-01-07 | buying 4 units at 3237.18\n",
            "2020-01-07 | current balance 15400.43\n",
            "2020-01-07 | current net wealth 15400.43\n",
            "2020-01-07 | buying 4 units at 3237.18\n",
            "2020-01-07 | current balance 2451.71\n",
            "2020-01-07 | current net wealth 15400.43\n",
            "2020-01-22 | selling 4 units at 3321.75\n",
            "2020-01-22 | current balance 15738.71\n",
            "2020-01-22 | current net wealth 15738.71\n",
            "2020-01-22 | selling 4 units at 3321.75\n",
            "2020-01-22 | current balance 29025.71\n",
            "2020-01-22 | current net wealth 15738.71\n",
            "2020-01-23 | buying 4 units at 3325.54\n",
            "2020-01-23 | current balance 15723.55\n",
            "2020-01-23 | current net wealth 15723.55\n",
            "2020-01-23 | buying 4 units at 3325.54\n",
            "2020-01-23 | current balance 2421.39\n",
            "2020-01-23 | current net wealth 15723.55\n",
            "2020-01-24 | selling 4 units at 3295.47\n",
            "2020-01-24 | current balance 15603.27\n",
            "2020-01-24 | current net wealth 15603.27\n",
            "2020-01-24 | selling 4 units at 3295.47\n",
            "2020-01-24 | current balance 28785.15\n",
            "2020-01-24 | current net wealth 15603.27\n",
            "2020-01-27 | buying 4 units at 3243.63\n",
            "2020-01-27 | current balance 15810.63\n",
            "2020-01-27 | current net wealth 15810.63\n",
            "2020-01-27 | buying 4 units at 3243.63\n",
            "2020-01-27 | current balance 2836.11\n",
            "2020-01-27 | current net wealth 15810.63\n",
            "2020-01-29 | selling 4 units at 3273.40\n",
            "2020-01-29 | current balance 15929.71\n",
            "2020-01-29 | current net wealth 15929.71\n",
            "2020-01-29 | selling 4 units at 3273.40\n",
            "2020-01-29 | current balance 29023.31\n",
            "2020-01-29 | current net wealth 15929.71\n",
            "2020-01-30 | buying 4 units at 3283.66\n",
            "2020-01-30 | current balance 15888.67\n",
            "2020-01-30 | current net wealth 15888.67\n",
            "2020-01-30 | buying 4 units at 3283.66\n",
            "2020-01-30 | current balance 2754.03\n",
            "2020-01-30 | current net wealth 15888.67\n",
            "2020-02-18 | selling 4 units at 3370.29\n",
            "2020-02-18 | current balance 16235.19\n",
            "2020-02-18 | current net wealth 16235.19\n",
            "2020-02-18 | selling 4 units at 3370.29\n",
            "2020-02-18 | current balance 29716.35\n",
            "2020-02-18 | current net wealth 16235.19\n",
            "2020-02-19 | buying 4 units at 3386.15\n",
            "2020-02-19 | current balance 16171.75\n",
            "2020-02-19 | current net wealth 16171.75\n",
            "2020-02-19 | buying 4 units at 3386.15\n",
            "2020-02-19 | current balance 2627.15\n",
            "2020-02-19 | current net wealth 16171.75\n",
            "2020-02-20 | selling 4 units at 3373.23\n",
            "2020-02-20 | current balance 16120.07\n",
            "2020-02-20 | current net wealth 16120.07\n",
            "2020-02-20 | selling 4 units at 3373.23\n",
            "2020-02-20 | current balance 29612.99\n",
            "2020-02-20 | current net wealth 16120.07\n",
            "2020-02-21 | buying 4 units at 3337.75\n",
            "2020-02-21 | current balance 16261.99\n",
            "2020-02-21 | current net wealth 16261.99\n",
            "2020-02-21 | buying 4 units at 3337.75\n",
            "2020-02-21 | current balance 2910.99\n",
            "2020-02-21 | current net wealth 16261.99\n",
            "2020-03-02 | selling 4 units at 3090.23\n",
            "2020-03-02 | current balance 15271.91\n",
            "2020-03-02 | current net wealth 15271.91\n",
            "2020-03-02 | selling 4 units at 3090.23\n",
            "2020-03-02 | current balance 27632.83\n",
            "2020-03-02 | current net wealth 15271.91\n",
            "2020-03-05 | buying 4 units at 3023.94\n",
            "2020-03-05 | current balance 15537.07\n",
            "2020-03-05 | current net wealth 15537.07\n",
            "2020-03-05 | buying 5 units at 3023.94\n",
            "2020-03-05 | current balance 417.37\n",
            "2020-03-05 | current net wealth 15537.07\n",
            "2020-03-10 | selling 5 units at 2882.23\n",
            "2020-03-10 | current balance 14828.52\n",
            "2020-03-10 | current net wealth 14828.52\n",
            "2020-03-10 | selling 5 units at 2882.23\n",
            "2020-03-10 | current balance 29239.67\n",
            "2020-03-10 | current net wealth 14828.52\n",
            "2020-03-12 | buying 5 units at 2480.64\n",
            "2020-03-12 | current balance 16836.47\n",
            "2020-03-12 | current net wealth 16836.47\n",
            "2020-03-12 | buying 6 units at 2480.64\n",
            "2020-03-12 | current balance 1952.63\n",
            "2020-03-12 | current net wealth 16836.47\n",
            "2020-03-17 | selling 6 units at 2529.19\n",
            "2020-03-17 | current balance 17127.77\n",
            "2020-03-17 | current net wealth 17127.77\n",
            "2020-03-17 | selling 6 units at 2529.19\n",
            "2020-03-17 | current balance 32302.91\n",
            "2020-03-17 | current net wealth 17127.77\n",
            "2020-03-18 | buying 6 units at 2398.10\n",
            "2020-03-18 | current balance 17914.31\n",
            "2020-03-18 | current net wealth 17914.31\n",
            "2020-03-18 | buying 7 units at 2398.10\n",
            "2020-03-18 | current balance 1127.61\n",
            "2020-03-18 | current net wealth 17914.31\n",
            "2020-03-25 | selling 7 units at 2475.56\n",
            "2020-03-25 | current balance 18456.53\n",
            "2020-03-25 | current net wealth 18456.53\n",
            "2020-03-25 | selling 7 units at 2475.56\n",
            "2020-03-25 | current balance 35785.45\n",
            "2020-03-25 | current net wealth 18456.53\n",
            "2020-03-26 | buying 7 units at 2630.07\n",
            "2020-03-26 | current balance 17374.96\n",
            "2020-03-26 | current net wealth 17374.96\n",
            "2020-03-26 | buying 6 units at 2630.07\n",
            "2020-03-26 | current balance 1594.54\n",
            "2020-03-26 | current net wealth 17374.96\n",
            "2020-03-27 | selling 6 units at 2541.47\n",
            "2020-03-27 | current balance 16843.36\n",
            "2020-03-27 | current net wealth 16843.36\n",
            "2020-03-27 | selling 6 units at 2541.47\n",
            "2020-03-27 | current balance 32092.18\n",
            "2020-03-27 | current net wealth 16843.36\n",
            "2020-03-30 | buying 6 units at 2626.65\n",
            "2020-03-30 | current balance 16332.28\n",
            "2020-03-30 | current net wealth 16332.28\n",
            "2020-03-30 | buying 6 units at 2626.65\n",
            "2020-03-30 | current balance 572.38\n",
            "2020-03-30 | current net wealth 16332.28\n",
            "2020-04-06 | selling 6 units at 2663.68\n",
            "2020-04-06 | current balance 16554.46\n",
            "2020-04-06 | current net wealth 16554.46\n",
            "2020-04-06 | selling 6 units at 2663.68\n",
            "2020-04-06 | current balance 32536.54\n",
            "2020-04-06 | current net wealth 16554.46\n",
            "2020-04-07 | buying 6 units at 2659.41\n",
            "2020-04-07 | current balance 16580.08\n",
            "2020-04-07 | current net wealth 16580.08\n",
            "2020-04-07 | buying 6 units at 2659.41\n",
            "2020-04-07 | current balance 623.62\n",
            "2020-04-07 | current net wealth 16580.08\n",
            "2020-04-20 | selling 6 units at 2823.16\n",
            "2020-04-20 | current balance 17562.58\n",
            "2020-04-20 | current net wealth 17562.58\n",
            "2020-04-20 | selling 6 units at 2823.16\n",
            "2020-04-20 | current balance 34501.54\n",
            "2020-04-20 | current net wealth 17562.58\n",
            "2020-04-21 | buying 6 units at 2736.56\n",
            "2020-04-21 | current balance 18082.18\n",
            "2020-04-21 | current net wealth 18082.18\n",
            "2020-04-21 | buying 6 units at 2736.56\n",
            "2020-04-21 | current balance 1662.82\n",
            "2020-04-21 | current net wealth 18082.18\n",
            "2020-04-23 | selling 6 units at 2797.80\n",
            "2020-04-23 | current balance 18449.62\n",
            "2020-04-23 | current net wealth 18449.62\n",
            "2020-04-23 | selling 6 units at 2797.80\n",
            "2020-04-23 | current balance 35236.42\n",
            "2020-04-23 | current net wealth 18449.62\n",
            "2020-04-24 | buying 6 units at 2836.74\n",
            "2020-04-24 | current balance 18215.98\n",
            "2020-04-24 | current net wealth 18215.98\n",
            "2020-04-24 | buying 6 units at 2836.74\n",
            "2020-04-24 | current balance 1195.54\n",
            "2020-04-24 | current net wealth 18215.98\n",
            "2020-04-30 | selling 6 units at 2912.43\n",
            "2020-04-30 | current balance 18670.12\n",
            "2020-04-30 | current net wealth 18670.12\n",
            "2020-04-30 | selling 6 units at 2912.43\n",
            "2020-04-30 | current balance 36144.70\n",
            "2020-04-30 | current net wealth 18670.12\n",
            "2020-05-01 | buying 6 units at 2830.71\n",
            "2020-05-01 | current balance 19160.44\n",
            "2020-05-01 | current net wealth 19160.44\n",
            "2020-05-01 | buying 6 units at 2830.71\n",
            "2020-05-01 | current balance 2176.18\n",
            "2020-05-01 | current net wealth 19160.44\n",
            "2020-05-04 | selling 6 units at 2842.74\n",
            "2020-05-04 | current balance 19232.62\n",
            "2020-05-04 | current net wealth 19232.62\n",
            "2020-05-04 | selling 6 units at 2842.74\n",
            "2020-05-04 | current balance 36289.06\n",
            "2020-05-04 | current net wealth 19232.62\n",
            "2020-05-05 | buying 6 units at 2868.44\n",
            "2020-05-05 | current balance 19078.42\n",
            "2020-05-05 | current net wealth 19078.42\n",
            "2020-05-05 | buying 6 units at 2868.44\n",
            "2020-05-05 | current balance 1867.78\n",
            "2020-05-05 | current net wealth 19078.42\n",
            "2020-05-06 | selling 6 units at 2848.42\n",
            "2020-05-06 | current balance 18958.30\n",
            "2020-05-06 | current net wealth 18958.30\n",
            "2020-05-06 | selling 6 units at 2848.42\n",
            "2020-05-06 | current balance 36048.82\n",
            "2020-05-06 | current net wealth 18958.30\n",
            "2020-05-07 | buying 6 units at 2881.19\n",
            "2020-05-07 | current balance 18761.68\n",
            "2020-05-07 | current net wealth 18761.68\n",
            "2020-05-07 | buying 6 units at 2881.19\n",
            "2020-05-07 | current balance 1474.54\n",
            "2020-05-07 | current net wealth 18761.68\n",
            "2020-09-04 | selling 6 units at 3426.96\n",
            "2020-09-04 | current balance 22036.30\n",
            "2020-09-04 | current net wealth 22036.30\n",
            "2020-09-04 | selling 6 units at 3426.96\n",
            "2020-09-04 | current balance 42598.06\n",
            "2020-09-04 | current net wealth 22036.30\n",
            "2020-09-08 | buying 6 units at 3331.84\n",
            "2020-09-08 | current balance 22607.02\n",
            "2020-09-08 | current net wealth 22607.02\n",
            "2020-09-08 | buying 6 units at 3331.84\n",
            "2020-09-08 | current balance 2615.98\n",
            "2020-09-08 | current net wealth 22607.02\n",
            "2020-09-09 | selling 6 units at 3398.96\n",
            "2020-09-09 | current balance 23009.74\n",
            "2020-09-09 | current net wealth 23009.74\n",
            "2020-09-09 | selling 6 units at 3398.96\n",
            "2020-09-09 | current balance 43403.50\n",
            "2020-09-09 | current net wealth 23009.74\n",
            "2020-09-10 | buying 6 units at 3339.19\n",
            "2020-09-10 | current balance 23368.36\n",
            "2020-09-10 | current net wealth 23368.36\n",
            "2020-09-10 | buying 6 units at 3339.19\n",
            "2020-09-10 | current balance 3333.22\n",
            "2020-09-10 | current net wealth 23368.36\n",
            "2020-09-11 | selling 6 units at 3340.97\n",
            "2020-09-11 | current balance 23379.04\n",
            "2020-09-11 | current net wealth 23379.04\n",
            "2020-09-11 | selling 6 units at 3340.97\n",
            "2020-09-11 | current balance 43424.86\n",
            "2020-09-11 | current net wealth 23379.04\n",
            "2020-09-14 | buying 6 units at 3383.54\n",
            "2020-09-14 | current balance 23123.62\n",
            "2020-09-14 | current net wealth 23123.62\n",
            "2020-09-14 | buying 6 units at 3383.54\n",
            "2020-09-14 | current balance 2822.38\n",
            "2020-09-14 | current net wealth 23123.62\n",
            "2020-09-17 | selling 6 units at 3357.01\n",
            "2020-09-17 | current balance 22964.44\n",
            "2020-09-17 | current net wealth 22964.44\n",
            "2020-09-17 | selling 6 units at 3357.01\n",
            "2020-09-17 | current balance 43106.50\n",
            "2020-09-17 | current net wealth 22964.44\n",
            "2020-09-21 | buying 6 units at 3281.06\n",
            "2020-09-21 | current balance 23420.14\n",
            "2020-09-21 | current net wealth 23420.14\n",
            "2020-09-21 | buying 7 units at 3281.06\n",
            "2020-09-21 | current balance 452.72\n",
            "2020-09-21 | current net wealth 23420.14\n",
            "2020-09-22 | selling 7 units at 3315.57\n",
            "2020-09-22 | current balance 23661.71\n",
            "2020-09-22 | current net wealth 23661.71\n",
            "2020-09-22 | selling 7 units at 3315.57\n",
            "2020-09-22 | current balance 46870.70\n",
            "2020-09-22 | current net wealth 23661.71\n",
            "2020-09-23 | buying 7 units at 3236.92\n",
            "2020-09-23 | current balance 24212.26\n",
            "2020-09-23 | current net wealth 24212.26\n",
            "2020-09-23 | buying 7 units at 3236.92\n",
            "2020-09-23 | current balance 1553.82\n",
            "2020-09-23 | current net wealth 24212.26\n",
            "2020-10-13 | selling 7 units at 3511.93\n",
            "2020-10-13 | current balance 26137.33\n",
            "2020-10-13 | current net wealth 26137.33\n",
            "2020-10-13 | selling 7 units at 3511.93\n",
            "2020-10-13 | current balance 50720.84\n",
            "2020-10-13 | current net wealth 26137.33\n",
            "2020-10-14 | buying 7 units at 3488.67\n",
            "2020-10-14 | current balance 26300.15\n",
            "2020-10-14 | current net wealth 26300.15\n",
            "2020-10-14 | buying 7 units at 3488.67\n",
            "2020-10-14 | current balance 1879.46\n",
            "2020-10-14 | current net wealth 26300.15\n",
            "2020-10-15 | selling 7 units at 3483.34\n",
            "2020-10-15 | current balance 26262.84\n",
            "2020-10-15 | current net wealth 26262.84\n",
            "2020-10-15 | selling 7 units at 3483.34\n",
            "2020-10-15 | current balance 50646.22\n",
            "2020-10-15 | current net wealth 26262.84\n",
            "2020-10-20 | buying 7 units at 3443.12\n",
            "2020-10-20 | current balance 26544.38\n",
            "2020-10-20 | current net wealth 26544.38\n",
            "2020-10-20 | buying 7 units at 3443.12\n",
            "2020-10-20 | current balance 2442.54\n",
            "2020-10-20 | current net wealth 26544.38\n",
            "2020-10-21 | selling 7 units at 3435.56\n",
            "2020-10-21 | current balance 26491.46\n",
            "2020-10-21 | current net wealth 26491.46\n",
            "2020-10-21 | selling 7 units at 3435.56\n",
            "2020-10-21 | current balance 50540.38\n",
            "2020-10-21 | current net wealth 26491.46\n",
            "2020-10-22 | buying 7 units at 3453.49\n",
            "2020-10-22 | current balance 26365.95\n",
            "2020-10-22 | current net wealth 26365.95\n",
            "2020-10-22 | buying 7 units at 3453.49\n",
            "2020-10-22 | current balance 2191.52\n",
            "2020-10-22 | current net wealth 26365.95\n",
            "2020-10-26 | selling 7 units at 3400.97\n",
            "2020-10-26 | current balance 25998.31\n",
            "2020-10-26 | current net wealth 25998.31\n",
            "2020-10-26 | selling 7 units at 3400.97\n",
            "2020-10-26 | current balance 49805.10\n",
            "2020-10-26 | current net wealth 25998.31\n",
            "2020-10-27 | buying 7 units at 3390.68\n",
            "2020-10-27 | current balance 26070.34\n",
            "2020-10-27 | current net wealth 26070.34\n",
            "2020-10-27 | buying 7 units at 3390.68\n",
            "2020-10-27 | current balance 2335.58\n",
            "2020-10-27 | current net wealth 26070.34\n",
            "2020-10-30 | selling 7 units at 3269.96\n",
            "2020-10-30 | current balance 25225.30\n",
            "2020-10-30 | current net wealth 25225.30\n",
            "2020-10-30 | selling 7 units at 3269.96\n",
            "2020-10-30 | current balance 48115.02\n",
            "2020-10-30 | current net wealth 25225.30\n",
            "2020-11-02 | buying 7 units at 3310.24\n",
            "2020-11-02 | current balance 24943.34\n",
            "2020-11-02 | current net wealth 24943.34\n",
            "2020-11-02 | buying 7 units at 3310.24\n",
            "2020-11-02 | current balance 1771.66\n",
            "2020-11-02 | current net wealth 24943.34\n",
            "2021-01-26 | selling 7 units at 3849.62\n",
            "2021-01-26 | current balance 28719.00\n",
            "2021-01-26 | current net wealth 28719.00\n",
            "2021-01-26 | selling 7 units at 3849.62\n",
            "2021-01-26 | current balance 55666.34\n",
            "2021-01-26 | current net wealth 28719.00\n",
            "2021-01-27 | buying 7 units at 3750.77\n",
            "2021-01-27 | current balance 29410.95\n",
            "2021-01-27 | current net wealth 29410.95\n",
            "2021-01-27 | buying 7 units at 3750.77\n",
            "2021-01-27 | current balance 3155.56\n",
            "2021-01-27 | current net wealth 29410.95\n",
            "2021-02-18 | selling 7 units at 3913.97\n",
            "2021-02-18 | current balance 30553.35\n",
            "2021-02-18 | current net wealth 30553.35\n",
            "2021-02-18 | selling 7 units at 3913.97\n",
            "2021-02-18 | current balance 57951.14\n",
            "2021-02-18 | current net wealth 30553.35\n",
            "2021-02-24 | buying 7 units at 3925.43\n",
            "2021-02-24 | current balance 30473.13\n",
            "2021-02-24 | current net wealth 30473.13\n",
            "2021-02-24 | buying 7 units at 3925.43\n",
            "2021-02-24 | current balance 2995.12\n",
            "2021-02-24 | current net wealth 30473.13\n",
            "2021-03-01 | selling 7 units at 3901.82\n",
            "2021-03-01 | current balance 30307.86\n",
            "2021-03-01 | current net wealth 30307.86\n",
            "2021-03-01 | selling 7 units at 3901.82\n",
            "2021-03-01 | current balance 57620.61\n",
            "2021-03-01 | current net wealth 30307.86\n",
            "2021-03-02 | buying 7 units at 3870.29\n",
            "2021-03-02 | current balance 30528.58\n",
            "2021-03-02 | current net wealth 30528.58\n",
            "2021-03-02 | buying 7 units at 3870.29\n",
            "2021-03-02 | current balance 3436.54\n",
            "2021-03-02 | current net wealth 30528.58\n",
            "2021-03-05 | selling 7 units at 3841.94\n",
            "2021-03-05 | current balance 30330.12\n",
            "2021-03-05 | current net wealth 30330.12\n",
            "2021-03-05 | selling 7 units at 3841.94\n",
            "2021-03-05 | current balance 57223.70\n",
            "2021-03-05 | current net wealth 30330.12\n",
            "2021-03-08 | buying 7 units at 3821.35\n",
            "2021-03-08 | current balance 30474.25\n",
            "2021-03-08 | current net wealth 30474.25\n",
            "2021-03-08 | buying 7 units at 3821.35\n",
            "2021-03-08 | current balance 3724.80\n",
            "2021-03-08 | current net wealth 30474.25\n",
            "2021-04-19 | selling 7 units at 4163.26\n",
            "2021-04-19 | current balance 32867.62\n",
            "2021-04-19 | current net wealth 32867.62\n",
            "2021-04-19 | selling 7 units at 4163.26\n",
            "2021-04-19 | current balance 62010.44\n",
            "2021-04-19 | current net wealth 32867.62\n",
            "2021-04-20 | buying 7 units at 4134.94\n",
            "2021-04-20 | current balance 33065.86\n",
            "2021-04-20 | current net wealth 33065.86\n",
            "2021-04-20 | buying 7 units at 4134.94\n",
            "2021-04-20 | current balance 4121.28\n",
            "2021-04-20 | current net wealth 33065.86\n",
            "2021-04-22 | selling 7 units at 4134.98\n",
            "2021-04-22 | current balance 33066.14\n",
            "2021-04-22 | current net wealth 33066.14\n",
            "2021-04-22 | selling 7 units at 4134.98\n",
            "2021-04-22 | current balance 62011.00\n",
            "2021-04-22 | current net wealth 33066.14\n",
            "2021-04-23 | buying 7 units at 4180.17\n",
            "2021-04-23 | current balance 32749.81\n",
            "2021-04-23 | current net wealth 32749.81\n",
            "2021-04-23 | buying 7 units at 4180.17\n",
            "2021-04-23 | current balance 3488.62\n",
            "2021-04-23 | current net wealth 32749.81\n",
            "2021-04-28 | selling 7 units at 4183.18\n",
            "2021-04-28 | current balance 32770.88\n",
            "2021-04-28 | current net wealth 32770.88\n",
            "2021-04-28 | selling 7 units at 4183.18\n",
            "2021-04-28 | current balance 62053.14\n",
            "2021-04-28 | current net wealth 32770.88\n",
            "2021-04-29 | buying 7 units at 4211.47\n",
            "2021-04-29 | current balance 32572.85\n",
            "2021-04-29 | current net wealth 32572.85\n",
            "2021-04-29 | buying 7 units at 4211.47\n",
            "2021-04-29 | current balance 3092.56\n",
            "2021-04-29 | current net wealth 32572.85\n",
            "2021-05-04 | selling 7 units at 4164.66\n",
            "2021-05-04 | current balance 32245.18\n",
            "2021-05-04 | current net wealth 32245.18\n",
            "2021-05-04 | selling 7 units at 4164.66\n",
            "2021-05-04 | current balance 61397.80\n",
            "2021-05-04 | current net wealth 32245.18\n",
            "2021-05-05 | buying 7 units at 4167.59\n",
            "2021-05-05 | current balance 32224.67\n",
            "2021-05-05 | current net wealth 32224.67\n",
            "2021-05-05 | buying 7 units at 4167.59\n",
            "2021-05-05 | current balance 3051.54\n",
            "2021-05-05 | current net wealth 32224.67\n",
            "2021-05-10 | selling 7 units at 4188.43\n",
            "2021-05-10 | current balance 32370.56\n",
            "2021-05-10 | current net wealth 32370.56\n",
            "2021-05-10 | selling 7 units at 4188.43\n",
            "2021-05-10 | current balance 61689.57\n",
            "2021-05-10 | current net wealth 32370.56\n",
            "2021-05-11 | buying 7 units at 4152.10\n",
            "2021-05-11 | current balance 32624.87\n",
            "2021-05-11 | current net wealth 32624.87\n",
            "2021-05-11 | buying 7 units at 4152.10\n",
            "2021-05-11 | current balance 3560.17\n",
            "2021-05-11 | current net wealth 32624.87\n",
            "2021-05-14 | selling 7 units at 4173.85\n",
            "2021-05-14 | current balance 32777.12\n",
            "2021-05-14 | current net wealth 32777.12\n",
            "2021-05-14 | selling 7 units at 4173.85\n",
            "2021-05-14 | current balance 61994.07\n",
            "2021-05-14 | current net wealth 32777.12\n",
            "2021-05-17 | buying 7 units at 4163.29\n",
            "2021-05-17 | current balance 32851.04\n",
            "2021-05-17 | current net wealth 32851.04\n",
            "2021-05-17 | buying 7 units at 4163.29\n",
            "2021-05-17 | current balance 3708.01\n",
            "2021-05-17 | current net wealth 32851.04\n",
            "2021-06-14 | inventory 0 units at 4255.15\n",
            "=======================================================\n",
            "Final balance [$] 33494.06\n",
            "Net Performance [%] 234.94\n",
            "Trades Executed [#] 414.00\n",
            "=======================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b4d0b22c-ba74-4ff2-9e89-93f47b9f6454\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b4d0b22c-ba74-4ff2-9e89-93f47b9f6454\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b4d0b22c-ba74-4ff2-9e89-93f47b9f6454',\n",
              "                        [{\"line\": {\"color\": \"black\"}, \"marker\": {\"color\": [\"white\", \"green\", \"green\", \"red\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"green\", \"green\", \"white\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"green\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"red\", \"white\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"green\", \"red\", \"green\", \"red\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"red\", \"red\", \"white\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"red\", \"red\", \"red\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"red\", \"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"white\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"red\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"red\", \"red\", \"red\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"red\", \"red\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"red\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"white\", \"green\", \"green\", \"green\", \"white\", \"white\"], \"opacity\": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0], \"size\": [0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 0, 0], \"symbol\": [\"circle\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-down\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"triangle-up\", \"triangle-up\", \"triangle-up\", \"circle\", \"circle\"]}, \"mode\": \"lines+markers\", \"name\": \"S&P500 closing price\", \"type\": \"scatter\", \"x\": [\"2017-06-15T00:00:00\", \"2017-06-16T00:00:00\", \"2017-06-19T00:00:00\", \"2017-06-20T00:00:00\", \"2017-06-21T00:00:00\", \"2017-06-22T00:00:00\", \"2017-06-23T00:00:00\", \"2017-06-26T00:00:00\", \"2017-06-27T00:00:00\", \"2017-06-28T00:00:00\", \"2017-06-29T00:00:00\", \"2017-06-30T00:00:00\", \"2017-07-03T00:00:00\", \"2017-07-05T00:00:00\", \"2017-07-06T00:00:00\", \"2017-07-07T00:00:00\", \"2017-07-10T00:00:00\", \"2017-07-11T00:00:00\", \"2017-07-12T00:00:00\", \"2017-07-13T00:00:00\", \"2017-07-14T00:00:00\", \"2017-07-17T00:00:00\", \"2017-07-18T00:00:00\", \"2017-07-19T00:00:00\", \"2017-07-20T00:00:00\", \"2017-07-21T00:00:00\", \"2017-07-24T00:00:00\", \"2017-07-25T00:00:00\", \"2017-07-26T00:00:00\", \"2017-07-27T00:00:00\", \"2017-07-28T00:00:00\", \"2017-07-31T00:00:00\", \"2017-08-01T00:00:00\", \"2017-08-02T00:00:00\", \"2017-08-03T00:00:00\", \"2017-08-04T00:00:00\", \"2017-08-07T00:00:00\", \"2017-08-08T00:00:00\", \"2017-08-09T00:00:00\", \"2017-08-10T00:00:00\", \"2017-08-11T00:00:00\", \"2017-08-14T00:00:00\", \"2017-08-15T00:00:00\", \"2017-08-16T00:00:00\", \"2017-08-17T00:00:00\", \"2017-08-18T00:00:00\", \"2017-08-21T00:00:00\", \"2017-08-22T00:00:00\", \"2017-08-23T00:00:00\", \"2017-08-24T00:00:00\", \"2017-08-25T00:00:00\", \"2017-08-28T00:00:00\", \"2017-08-29T00:00:00\", \"2017-08-30T00:00:00\", \"2017-08-31T00:00:00\", \"2017-09-01T00:00:00\", \"2017-09-05T00:00:00\", \"2017-09-06T00:00:00\", \"2017-09-07T00:00:00\", \"2017-09-08T00:00:00\", \"2017-09-11T00:00:00\", \"2017-09-12T00:00:00\", \"2017-09-13T00:00:00\", \"2017-09-14T00:00:00\", \"2017-09-15T00:00:00\", \"2017-09-18T00:00:00\", \"2017-09-19T00:00:00\", \"2017-09-20T00:00:00\", \"2017-09-21T00:00:00\", \"2017-09-22T00:00:00\", \"2017-09-25T00:00:00\", \"2017-09-26T00:00:00\", \"2017-09-27T00:00:00\", \"2017-09-28T00:00:00\", \"2017-09-29T00:00:00\", \"2017-10-02T00:00:00\", \"2017-10-03T00:00:00\", \"2017-10-04T00:00:00\", \"2017-10-05T00:00:00\", \"2017-10-06T00:00:00\", \"2017-10-09T00:00:00\", \"2017-10-10T00:00:00\", \"2017-10-11T00:00:00\", \"2017-10-12T00:00:00\", \"2017-10-13T00:00:00\", \"2017-10-16T00:00:00\", \"2017-10-17T00:00:00\", \"2017-10-18T00:00:00\", \"2017-10-19T00:00:00\", \"2017-10-20T00:00:00\", \"2017-10-23T00:00:00\", \"2017-10-24T00:00:00\", \"2017-10-25T00:00:00\", \"2017-10-26T00:00:00\", \"2017-10-27T00:00:00\", \"2017-10-30T00:00:00\", \"2017-10-31T00:00:00\", \"2017-11-01T00:00:00\", \"2017-11-02T00:00:00\", \"2017-11-03T00:00:00\", \"2017-11-06T00:00:00\", \"2017-11-07T00:00:00\", \"2017-11-08T00:00:00\", \"2017-11-09T00:00:00\", \"2017-11-10T00:00:00\", \"2017-11-13T00:00:00\", \"2017-11-14T00:00:00\", \"2017-11-15T00:00:00\", \"2017-11-16T00:00:00\", \"2017-11-17T00:00:00\", \"2017-11-20T00:00:00\", \"2017-11-21T00:00:00\", \"2017-11-22T00:00:00\", \"2017-11-24T00:00:00\", \"2017-11-27T00:00:00\", \"2017-11-28T00:00:00\", \"2017-11-29T00:00:00\", \"2017-11-30T00:00:00\", \"2017-12-01T00:00:00\", \"2017-12-04T00:00:00\", \"2017-12-05T00:00:00\", \"2017-12-06T00:00:00\", \"2017-12-07T00:00:00\", \"2017-12-08T00:00:00\", \"2017-12-11T00:00:00\", \"2017-12-12T00:00:00\", \"2017-12-13T00:00:00\", \"2017-12-14T00:00:00\", \"2017-12-15T00:00:00\", \"2017-12-18T00:00:00\", \"2017-12-19T00:00:00\", \"2017-12-20T00:00:00\", \"2017-12-21T00:00:00\", \"2017-12-22T00:00:00\", \"2017-12-26T00:00:00\", \"2017-12-27T00:00:00\", \"2017-12-28T00:00:00\", \"2017-12-29T00:00:00\", \"2018-01-02T00:00:00\", \"2018-01-03T00:00:00\", \"2018-01-04T00:00:00\", \"2018-01-05T00:00:00\", \"2018-01-08T00:00:00\", \"2018-01-09T00:00:00\", \"2018-01-10T00:00:00\", \"2018-01-11T00:00:00\", \"2018-01-12T00:00:00\", \"2018-01-16T00:00:00\", \"2018-01-17T00:00:00\", \"2018-01-18T00:00:00\", \"2018-01-19T00:00:00\", \"2018-01-22T00:00:00\", \"2018-01-23T00:00:00\", \"2018-01-24T00:00:00\", \"2018-01-25T00:00:00\", \"2018-01-26T00:00:00\", \"2018-01-29T00:00:00\", \"2018-01-30T00:00:00\", \"2018-01-31T00:00:00\", \"2018-02-01T00:00:00\", \"2018-02-02T00:00:00\", \"2018-02-05T00:00:00\", \"2018-02-06T00:00:00\", \"2018-02-07T00:00:00\", \"2018-02-08T00:00:00\", \"2018-02-09T00:00:00\", \"2018-02-12T00:00:00\", \"2018-02-13T00:00:00\", \"2018-02-14T00:00:00\", \"2018-02-15T00:00:00\", \"2018-02-16T00:00:00\", \"2018-02-20T00:00:00\", \"2018-02-21T00:00:00\", \"2018-02-22T00:00:00\", \"2018-02-23T00:00:00\", \"2018-02-26T00:00:00\", \"2018-02-27T00:00:00\", \"2018-02-28T00:00:00\", \"2018-03-01T00:00:00\", \"2018-03-02T00:00:00\", \"2018-03-05T00:00:00\", \"2018-03-06T00:00:00\", \"2018-03-07T00:00:00\", \"2018-03-08T00:00:00\", \"2018-03-09T00:00:00\", \"2018-03-12T00:00:00\", \"2018-03-13T00:00:00\", \"2018-03-14T00:00:00\", \"2018-03-15T00:00:00\", \"2018-03-16T00:00:00\", \"2018-03-19T00:00:00\", \"2018-03-20T00:00:00\", \"2018-03-21T00:00:00\", \"2018-03-22T00:00:00\", \"2018-03-23T00:00:00\", \"2018-03-26T00:00:00\", \"2018-03-27T00:00:00\", \"2018-03-28T00:00:00\", \"2018-03-29T00:00:00\", \"2018-04-02T00:00:00\", \"2018-04-03T00:00:00\", \"2018-04-04T00:00:00\", \"2018-04-05T00:00:00\", \"2018-04-06T00:00:00\", \"2018-04-09T00:00:00\", \"2018-04-10T00:00:00\", \"2018-04-11T00:00:00\", \"2018-04-12T00:00:00\", \"2018-04-13T00:00:00\", \"2018-04-16T00:00:00\", \"2018-04-17T00:00:00\", \"2018-04-18T00:00:00\", \"2018-04-19T00:00:00\", \"2018-04-20T00:00:00\", \"2018-04-23T00:00:00\", \"2018-04-24T00:00:00\", \"2018-04-25T00:00:00\", \"2018-04-26T00:00:00\", \"2018-04-27T00:00:00\", \"2018-04-30T00:00:00\", \"2018-05-01T00:00:00\", \"2018-05-02T00:00:00\", \"2018-05-03T00:00:00\", \"2018-05-04T00:00:00\", \"2018-05-07T00:00:00\", \"2018-05-08T00:00:00\", \"2018-05-09T00:00:00\", \"2018-05-10T00:00:00\", \"2018-05-11T00:00:00\", \"2018-05-14T00:00:00\", \"2018-05-15T00:00:00\", \"2018-05-16T00:00:00\", \"2018-05-17T00:00:00\", \"2018-05-18T00:00:00\", \"2018-05-21T00:00:00\", \"2018-05-22T00:00:00\", \"2018-05-23T00:00:00\", \"2018-05-24T00:00:00\", \"2018-05-25T00:00:00\", \"2018-05-29T00:00:00\", \"2018-05-30T00:00:00\", \"2018-05-31T00:00:00\", \"2018-06-01T00:00:00\", \"2018-06-04T00:00:00\", \"2018-06-05T00:00:00\", \"2018-06-06T00:00:00\", \"2018-06-07T00:00:00\", \"2018-06-08T00:00:00\", \"2018-06-11T00:00:00\", \"2018-06-12T00:00:00\", \"2018-06-13T00:00:00\", \"2018-06-14T00:00:00\", \"2018-06-15T00:00:00\", \"2018-06-18T00:00:00\", \"2018-06-19T00:00:00\", \"2018-06-20T00:00:00\", \"2018-06-21T00:00:00\", \"2018-06-22T00:00:00\", \"2018-06-25T00:00:00\", \"2018-06-26T00:00:00\", \"2018-06-27T00:00:00\", \"2018-06-28T00:00:00\", \"2018-06-29T00:00:00\", \"2018-07-02T00:00:00\", \"2018-07-03T00:00:00\", \"2018-07-05T00:00:00\", \"2018-07-06T00:00:00\", \"2018-07-09T00:00:00\", \"2018-07-10T00:00:00\", \"2018-07-11T00:00:00\", \"2018-07-12T00:00:00\", \"2018-07-13T00:00:00\", \"2018-07-16T00:00:00\", \"2018-07-17T00:00:00\", \"2018-07-18T00:00:00\", \"2018-07-19T00:00:00\", \"2018-07-20T00:00:00\", \"2018-07-23T00:00:00\", \"2018-07-24T00:00:00\", \"2018-07-25T00:00:00\", \"2018-07-26T00:00:00\", \"2018-07-27T00:00:00\", \"2018-07-30T00:00:00\", \"2018-07-31T00:00:00\", \"2018-08-01T00:00:00\", \"2018-08-02T00:00:00\", \"2018-08-03T00:00:00\", \"2018-08-06T00:00:00\", \"2018-08-07T00:00:00\", \"2018-08-08T00:00:00\", \"2018-08-09T00:00:00\", \"2018-08-10T00:00:00\", \"2018-08-13T00:00:00\", \"2018-08-14T00:00:00\", \"2018-08-15T00:00:00\", \"2018-08-16T00:00:00\", \"2018-08-17T00:00:00\", \"2018-08-20T00:00:00\", \"2018-08-21T00:00:00\", \"2018-08-22T00:00:00\", \"2018-08-23T00:00:00\", \"2018-08-24T00:00:00\", \"2018-08-27T00:00:00\", \"2018-08-28T00:00:00\", \"2018-08-29T00:00:00\", \"2018-08-30T00:00:00\", \"2018-08-31T00:00:00\", \"2018-09-04T00:00:00\", \"2018-09-05T00:00:00\", \"2018-09-06T00:00:00\", \"2018-09-07T00:00:00\", \"2018-09-10T00:00:00\", \"2018-09-11T00:00:00\", \"2018-09-12T00:00:00\", \"2018-09-13T00:00:00\", \"2018-09-14T00:00:00\", \"2018-09-17T00:00:00\", \"2018-09-18T00:00:00\", \"2018-09-19T00:00:00\", \"2018-09-20T00:00:00\", \"2018-09-21T00:00:00\", \"2018-09-24T00:00:00\", \"2018-09-25T00:00:00\", \"2018-09-26T00:00:00\", \"2018-09-27T00:00:00\", \"2018-09-28T00:00:00\", \"2018-10-01T00:00:00\", \"2018-10-02T00:00:00\", \"2018-10-03T00:00:00\", \"2018-10-04T00:00:00\", \"2018-10-05T00:00:00\", \"2018-10-08T00:00:00\", \"2018-10-09T00:00:00\", \"2018-10-10T00:00:00\", \"2018-10-11T00:00:00\", \"2018-10-12T00:00:00\", \"2018-10-15T00:00:00\", \"2018-10-16T00:00:00\", \"2018-10-17T00:00:00\", \"2018-10-18T00:00:00\", \"2018-10-19T00:00:00\", \"2018-10-22T00:00:00\", \"2018-10-23T00:00:00\", \"2018-10-24T00:00:00\", \"2018-10-25T00:00:00\", \"2018-10-26T00:00:00\", \"2018-10-29T00:00:00\", \"2018-10-30T00:00:00\", \"2018-10-31T00:00:00\", \"2018-11-01T00:00:00\", \"2018-11-02T00:00:00\", \"2018-11-05T00:00:00\", \"2018-11-06T00:00:00\", \"2018-11-07T00:00:00\", \"2018-11-08T00:00:00\", \"2018-11-09T00:00:00\", \"2018-11-12T00:00:00\", \"2018-11-13T00:00:00\", \"2018-11-14T00:00:00\", \"2018-11-15T00:00:00\", \"2018-11-16T00:00:00\", \"2018-11-19T00:00:00\", \"2018-11-20T00:00:00\", \"2018-11-21T00:00:00\", \"2018-11-23T00:00:00\", \"2018-11-26T00:00:00\", \"2018-11-27T00:00:00\", \"2018-11-28T00:00:00\", \"2018-11-29T00:00:00\", \"2018-11-30T00:00:00\", \"2018-12-03T00:00:00\", \"2018-12-04T00:00:00\", \"2018-12-06T00:00:00\", \"2018-12-07T00:00:00\", \"2018-12-10T00:00:00\", \"2018-12-11T00:00:00\", \"2018-12-12T00:00:00\", \"2018-12-13T00:00:00\", \"2018-12-14T00:00:00\", \"2018-12-17T00:00:00\", \"2018-12-18T00:00:00\", \"2018-12-19T00:00:00\", \"2018-12-20T00:00:00\", \"2018-12-21T00:00:00\", \"2018-12-24T00:00:00\", \"2018-12-26T00:00:00\", \"2018-12-27T00:00:00\", \"2018-12-28T00:00:00\", \"2018-12-31T00:00:00\", \"2019-01-02T00:00:00\", \"2019-01-03T00:00:00\", \"2019-01-04T00:00:00\", \"2019-01-07T00:00:00\", \"2019-01-08T00:00:00\", \"2019-01-09T00:00:00\", \"2019-01-10T00:00:00\", \"2019-01-11T00:00:00\", \"2019-01-14T00:00:00\", \"2019-01-15T00:00:00\", \"2019-01-16T00:00:00\", \"2019-01-17T00:00:00\", \"2019-01-18T00:00:00\", \"2019-01-22T00:00:00\", \"2019-01-23T00:00:00\", \"2019-01-24T00:00:00\", \"2019-01-25T00:00:00\", \"2019-01-28T00:00:00\", \"2019-01-29T00:00:00\", \"2019-01-30T00:00:00\", \"2019-01-31T00:00:00\", \"2019-02-01T00:00:00\", \"2019-02-04T00:00:00\", \"2019-02-05T00:00:00\", \"2019-02-06T00:00:00\", \"2019-02-07T00:00:00\", \"2019-02-08T00:00:00\", \"2019-02-11T00:00:00\", \"2019-02-12T00:00:00\", \"2019-02-13T00:00:00\", \"2019-02-14T00:00:00\", \"2019-02-15T00:00:00\", \"2019-02-19T00:00:00\", \"2019-02-20T00:00:00\", \"2019-02-21T00:00:00\", \"2019-02-22T00:00:00\", \"2019-02-25T00:00:00\", \"2019-02-26T00:00:00\", \"2019-02-27T00:00:00\", \"2019-02-28T00:00:00\", \"2019-03-01T00:00:00\", \"2019-03-04T00:00:00\", \"2019-03-05T00:00:00\", \"2019-03-06T00:00:00\", \"2019-03-07T00:00:00\", \"2019-03-08T00:00:00\", \"2019-03-11T00:00:00\", \"2019-03-12T00:00:00\", \"2019-03-13T00:00:00\", \"2019-03-14T00:00:00\", \"2019-03-15T00:00:00\", \"2019-03-18T00:00:00\", \"2019-03-19T00:00:00\", \"2019-03-20T00:00:00\", \"2019-03-21T00:00:00\", \"2019-03-22T00:00:00\", \"2019-03-25T00:00:00\", \"2019-03-26T00:00:00\", \"2019-03-27T00:00:00\", \"2019-03-28T00:00:00\", \"2019-03-29T00:00:00\", \"2019-04-01T00:00:00\", \"2019-04-02T00:00:00\", \"2019-04-03T00:00:00\", \"2019-04-04T00:00:00\", \"2019-04-05T00:00:00\", \"2019-04-08T00:00:00\", \"2019-04-09T00:00:00\", \"2019-04-10T00:00:00\", \"2019-04-11T00:00:00\", \"2019-04-12T00:00:00\", \"2019-04-15T00:00:00\", \"2019-04-16T00:00:00\", \"2019-04-17T00:00:00\", \"2019-04-18T00:00:00\", \"2019-04-22T00:00:00\", \"2019-04-23T00:00:00\", \"2019-04-24T00:00:00\", \"2019-04-25T00:00:00\", \"2019-04-26T00:00:00\", \"2019-04-29T00:00:00\", \"2019-04-30T00:00:00\", \"2019-05-01T00:00:00\", \"2019-05-02T00:00:00\", \"2019-05-03T00:00:00\", \"2019-05-06T00:00:00\", \"2019-05-07T00:00:00\", \"2019-05-08T00:00:00\", \"2019-05-09T00:00:00\", \"2019-05-10T00:00:00\", \"2019-05-13T00:00:00\", \"2019-05-14T00:00:00\", \"2019-05-15T00:00:00\", \"2019-05-16T00:00:00\", \"2019-05-17T00:00:00\", \"2019-05-20T00:00:00\", \"2019-05-21T00:00:00\", \"2019-05-22T00:00:00\", \"2019-05-23T00:00:00\", \"2019-05-24T00:00:00\", \"2019-05-28T00:00:00\", \"2019-05-29T00:00:00\", \"2019-05-30T00:00:00\", \"2019-05-31T00:00:00\", \"2019-06-03T00:00:00\", \"2019-06-04T00:00:00\", \"2019-06-05T00:00:00\", \"2019-06-06T00:00:00\", \"2019-06-07T00:00:00\", \"2019-06-10T00:00:00\", \"2019-06-11T00:00:00\", \"2019-06-12T00:00:00\", \"2019-06-13T00:00:00\", \"2019-06-14T00:00:00\", \"2019-06-17T00:00:00\", \"2019-06-18T00:00:00\", \"2019-06-19T00:00:00\", \"2019-06-20T00:00:00\", \"2019-06-21T00:00:00\", \"2019-06-24T00:00:00\", \"2019-06-25T00:00:00\", \"2019-06-26T00:00:00\", \"2019-06-27T00:00:00\", \"2019-06-28T00:00:00\", \"2019-07-01T00:00:00\", \"2019-07-02T00:00:00\", \"2019-07-03T00:00:00\", \"2019-07-05T00:00:00\", \"2019-07-08T00:00:00\", \"2019-07-09T00:00:00\", \"2019-07-10T00:00:00\", \"2019-07-11T00:00:00\", \"2019-07-12T00:00:00\", \"2019-07-15T00:00:00\", \"2019-07-16T00:00:00\", \"2019-07-17T00:00:00\", \"2019-07-18T00:00:00\", \"2019-07-19T00:00:00\", \"2019-07-22T00:00:00\", \"2019-07-23T00:00:00\", \"2019-07-24T00:00:00\", \"2019-07-25T00:00:00\", \"2019-07-26T00:00:00\", \"2019-07-29T00:00:00\", \"2019-07-30T00:00:00\", \"2019-07-31T00:00:00\", \"2019-08-01T00:00:00\", \"2019-08-02T00:00:00\", \"2019-08-05T00:00:00\", \"2019-08-06T00:00:00\", \"2019-08-07T00:00:00\", \"2019-08-08T00:00:00\", \"2019-08-09T00:00:00\", \"2019-08-12T00:00:00\", \"2019-08-13T00:00:00\", \"2019-08-14T00:00:00\", \"2019-08-15T00:00:00\", \"2019-08-16T00:00:00\", \"2019-08-19T00:00:00\", \"2019-08-20T00:00:00\", \"2019-08-21T00:00:00\", \"2019-08-22T00:00:00\", \"2019-08-23T00:00:00\", \"2019-08-26T00:00:00\", \"2019-08-27T00:00:00\", \"2019-08-28T00:00:00\", \"2019-08-29T00:00:00\", \"2019-08-30T00:00:00\", \"2019-09-03T00:00:00\", \"2019-09-04T00:00:00\", \"2019-09-05T00:00:00\", \"2019-09-06T00:00:00\", \"2019-09-09T00:00:00\", \"2019-09-10T00:00:00\", \"2019-09-11T00:00:00\", \"2019-09-12T00:00:00\", \"2019-09-13T00:00:00\", \"2019-09-16T00:00:00\", \"2019-09-17T00:00:00\", \"2019-09-18T00:00:00\", \"2019-09-19T00:00:00\", \"2019-09-20T00:00:00\", \"2019-09-23T00:00:00\", \"2019-09-24T00:00:00\", \"2019-09-25T00:00:00\", \"2019-09-26T00:00:00\", \"2019-09-27T00:00:00\", \"2019-09-30T00:00:00\", \"2019-10-01T00:00:00\", \"2019-10-02T00:00:00\", \"2019-10-03T00:00:00\", \"2019-10-04T00:00:00\", \"2019-10-07T00:00:00\", \"2019-10-08T00:00:00\", \"2019-10-09T00:00:00\", \"2019-10-10T00:00:00\", \"2019-10-11T00:00:00\", \"2019-10-14T00:00:00\", \"2019-10-15T00:00:00\", \"2019-10-16T00:00:00\", \"2019-10-17T00:00:00\", \"2019-10-18T00:00:00\", \"2019-10-21T00:00:00\", \"2019-10-22T00:00:00\", \"2019-10-23T00:00:00\", \"2019-10-24T00:00:00\", \"2019-10-25T00:00:00\", \"2019-10-28T00:00:00\", \"2019-10-29T00:00:00\", \"2019-10-30T00:00:00\", \"2019-10-31T00:00:00\", \"2019-11-01T00:00:00\", \"2019-11-04T00:00:00\", \"2019-11-05T00:00:00\", \"2019-11-06T00:00:00\", \"2019-11-07T00:00:00\", \"2019-11-08T00:00:00\", \"2019-11-11T00:00:00\", \"2019-11-12T00:00:00\", \"2019-11-13T00:00:00\", \"2019-11-14T00:00:00\", \"2019-11-15T00:00:00\", \"2019-11-18T00:00:00\", \"2019-11-19T00:00:00\", \"2019-11-20T00:00:00\", \"2019-11-21T00:00:00\", \"2019-11-22T00:00:00\", \"2019-11-25T00:00:00\", \"2019-11-26T00:00:00\", \"2019-11-27T00:00:00\", \"2019-11-29T00:00:00\", \"2019-12-02T00:00:00\", \"2019-12-03T00:00:00\", \"2019-12-04T00:00:00\", \"2019-12-05T00:00:00\", \"2019-12-06T00:00:00\", \"2019-12-09T00:00:00\", \"2019-12-10T00:00:00\", \"2019-12-11T00:00:00\", \"2019-12-12T00:00:00\", \"2019-12-13T00:00:00\", \"2019-12-16T00:00:00\", \"2019-12-17T00:00:00\", \"2019-12-18T00:00:00\", \"2019-12-19T00:00:00\", \"2019-12-20T00:00:00\", \"2019-12-23T00:00:00\", \"2019-12-24T00:00:00\", \"2019-12-26T00:00:00\", \"2019-12-27T00:00:00\", \"2019-12-30T00:00:00\", \"2019-12-31T00:00:00\", \"2020-01-02T00:00:00\", \"2020-01-03T00:00:00\", \"2020-01-06T00:00:00\", \"2020-01-07T00:00:00\", \"2020-01-08T00:00:00\", \"2020-01-09T00:00:00\", \"2020-01-10T00:00:00\", \"2020-01-13T00:00:00\", \"2020-01-14T00:00:00\", \"2020-01-15T00:00:00\", \"2020-01-16T00:00:00\", \"2020-01-17T00:00:00\", \"2020-01-21T00:00:00\", \"2020-01-22T00:00:00\", \"2020-01-23T00:00:00\", \"2020-01-24T00:00:00\", \"2020-01-27T00:00:00\", \"2020-01-28T00:00:00\", \"2020-01-29T00:00:00\", \"2020-01-30T00:00:00\", \"2020-01-31T00:00:00\", \"2020-02-03T00:00:00\", \"2020-02-04T00:00:00\", \"2020-02-05T00:00:00\", \"2020-02-06T00:00:00\", \"2020-02-07T00:00:00\", \"2020-02-10T00:00:00\", \"2020-02-11T00:00:00\", \"2020-02-12T00:00:00\", \"2020-02-13T00:00:00\", \"2020-02-14T00:00:00\", \"2020-02-18T00:00:00\", \"2020-02-19T00:00:00\", \"2020-02-20T00:00:00\", \"2020-02-21T00:00:00\", \"2020-02-24T00:00:00\", \"2020-02-25T00:00:00\", \"2020-02-26T00:00:00\", \"2020-02-27T00:00:00\", \"2020-02-28T00:00:00\", \"2020-03-02T00:00:00\", \"2020-03-03T00:00:00\", \"2020-03-04T00:00:00\", \"2020-03-05T00:00:00\", \"2020-03-06T00:00:00\", \"2020-03-09T00:00:00\", \"2020-03-10T00:00:00\", \"2020-03-11T00:00:00\", \"2020-03-12T00:00:00\", \"2020-03-13T00:00:00\", \"2020-03-16T00:00:00\", \"2020-03-17T00:00:00\", \"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\", \"2021-04-23T00:00:00\", \"2021-04-26T00:00:00\", \"2021-04-27T00:00:00\", \"2021-04-28T00:00:00\", \"2021-04-29T00:00:00\", \"2021-04-30T00:00:00\", \"2021-05-03T00:00:00\", \"2021-05-04T00:00:00\", \"2021-05-05T00:00:00\", \"2021-05-06T00:00:00\", \"2021-05-07T00:00:00\", \"2021-05-10T00:00:00\", \"2021-05-11T00:00:00\", \"2021-05-12T00:00:00\", \"2021-05-13T00:00:00\", \"2021-05-14T00:00:00\", \"2021-05-17T00:00:00\", \"2021-05-18T00:00:00\", \"2021-05-19T00:00:00\", \"2021-05-20T00:00:00\", \"2021-05-21T00:00:00\", \"2021-05-24T00:00:00\", \"2021-05-25T00:00:00\", \"2021-05-26T00:00:00\", \"2021-05-27T00:00:00\", \"2021-05-28T00:00:00\", \"2021-06-01T00:00:00\", \"2021-06-02T00:00:00\", \"2021-06-03T00:00:00\", \"2021-06-04T00:00:00\", \"2021-06-07T00:00:00\", \"2021-06-08T00:00:00\", \"2021-06-09T00:00:00\", \"2021-06-10T00:00:00\", \"2021-06-11T00:00:00\", \"2021-06-14T00:00:00\"], \"y\": [2432.4599609375, 2433.14990234375, 2453.4599609375, 2437.030029296875, 2435.610107421875, 2434.5, 2438.300048828125, 2439.070068359375, 2419.3798828125, 2440.68994140625, 2419.699951171875, 2423.409912109375, 2429.010009765625, 2432.5400390625, 2409.75, 2425.179931640625, 2427.429931640625, 2425.530029296875, 2443.25, 2447.830078125, 2459.27001953125, 2459.139892578125, 2460.610107421875, 2473.830078125, 2473.449951171875, 2472.5400390625, 2469.909912109375, 2477.1298828125, 2477.830078125, 2475.419921875, 2472.10009765625, 2470.300048828125, 2476.35009765625, 2477.570068359375, 2472.159912109375, 2476.830078125, 2480.909912109375, 2474.919921875, 2474.02001953125, 2438.2099609375, 2441.320068359375, 2465.840087890625, 2464.610107421875, 2468.110107421875, 2430.010009765625, 2425.550048828125, 2428.3701171875, 2452.510009765625, 2444.0400390625, 2438.969970703125, 2443.050048828125, 2444.239990234375, 2446.300048828125, 2457.590087890625, 2471.64990234375, 2476.550048828125, 2457.85009765625, 2465.5400390625, 2465.10009765625, 2461.429931640625, 2488.110107421875, 2496.47998046875, 2498.3701171875, 2495.6201171875, 2500.22998046875, 2503.8701171875, 2506.64990234375, 2508.239990234375, 2500.60009765625, 2502.219970703125, 2496.659912109375, 2496.840087890625, 2507.0400390625, 2510.06005859375, 2519.360107421875, 2529.1201171875, 2534.580078125, 2537.739990234375, 2552.070068359375, 2549.330078125, 2544.72998046875, 2550.639892578125, 2555.239990234375, 2550.929931640625, 2553.169921875, 2557.639892578125, 2559.360107421875, 2561.260009765625, 2562.10009765625, 2575.2099609375, 2564.97998046875, 2569.1298828125, 2557.14990234375, 2560.39990234375, 2581.070068359375, 2572.830078125, 2575.260009765625, 2579.360107421875, 2579.85009765625, 2587.840087890625, 2591.1298828125, 2590.639892578125, 2594.3798828125, 2584.6201171875, 2582.300048828125, 2584.840087890625, 2578.8701171875, 2564.6201171875, 2585.639892578125, 2578.85009765625, 2582.139892578125, 2599.030029296875, 2597.080078125, 2602.419921875, 2601.419921875, 2627.0400390625, 2626.070068359375, 2647.580078125, 2642.219970703125, 2639.43994140625, 2629.570068359375, 2629.27001953125, 2636.97998046875, 2651.5, 2659.989990234375, 2664.110107421875, 2662.85009765625, 2652.010009765625, 2675.81005859375, 2690.159912109375, 2681.469970703125, 2679.25, 2684.570068359375, 2683.340087890625, 2680.5, 2682.6201171875, 2687.5400390625, 2673.610107421875, 2695.81005859375, 2713.06005859375, 2723.989990234375, 2743.14990234375, 2747.7099609375, 2751.2900390625, 2748.22998046875, 2767.56005859375, 2786.239990234375, 2776.419921875, 2802.56005859375, 2798.030029296875, 2810.300048828125, 2832.969970703125, 2839.1298828125, 2837.5400390625, 2839.25, 2872.8701171875, 2853.530029296875, 2822.429931640625, 2823.81005859375, 2821.97998046875, 2762.1298828125, 2648.93994140625, 2695.139892578125, 2681.659912109375, 2581.0, 2619.550048828125, 2656.0, 2662.93994140625, 2698.6298828125, 2731.199951171875, 2732.219970703125, 2716.260009765625, 2701.330078125, 2703.9599609375, 2747.300048828125, 2779.60009765625, 2744.280029296875, 2713.830078125, 2677.669921875, 2691.25, 2720.93994140625, 2728.1201171875, 2726.800048828125, 2738.969970703125, 2786.570068359375, 2783.02001953125, 2765.31005859375, 2749.47998046875, 2747.330078125, 2752.010009765625, 2712.919921875, 2716.93994140625, 2711.929931640625, 2643.68994140625, 2588.260009765625, 2658.550048828125, 2612.6201171875, 2605.0, 2640.8701171875, 2581.8798828125, 2614.449951171875, 2644.68994140625, 2662.840087890625, 2604.469970703125, 2613.159912109375, 2656.8701171875, 2642.18994140625, 2663.989990234375, 2656.300048828125, 2677.840087890625, 2706.389892578125, 2708.639892578125, 2693.1298828125, 2670.139892578125, 2670.2900390625, 2634.56005859375, 2639.39990234375, 2666.93994140625, 2669.909912109375, 2648.050048828125, 2654.800048828125, 2635.669921875, 2629.72998046875, 2663.419921875, 2672.6298828125, 2671.919921875, 2697.7900390625, 2723.070068359375, 2727.719970703125, 2730.1298828125, 2711.449951171875, 2722.4599609375, 2720.1298828125, 2712.969970703125, 2733.010009765625, 2724.43994140625, 2733.2900390625, 2727.760009765625, 2721.330078125, 2689.860107421875, 2724.010009765625, 2705.27001953125, 2734.6201171875, 2746.8701171875, 2748.800048828125, 2772.35009765625, 2770.3701171875, 2779.030029296875, 2782.0, 2786.85009765625, 2775.6298828125, 2782.489990234375, 2779.659912109375, 2773.75, 2762.590087890625, 2767.320068359375, 2749.760009765625, 2754.8798828125, 2717.070068359375, 2723.06005859375, 2699.6298828125, 2716.31005859375, 2718.3701171875, 2726.7099609375, 2713.219970703125, 2736.610107421875, 2759.820068359375, 2784.169921875, 2793.840087890625, 2774.02001953125, 2798.2900390625, 2801.31005859375, 2798.429931640625, 2809.550048828125, 2815.6201171875, 2804.489990234375, 2801.830078125, 2806.97998046875, 2820.39990234375, 2846.070068359375, 2837.43994140625, 2818.820068359375, 2802.60009765625, 2816.2900390625, 2813.360107421875, 2827.219970703125, 2840.35009765625, 2850.39990234375, 2858.449951171875, 2857.699951171875, 2853.580078125, 2833.280029296875, 2821.929931640625, 2839.9599609375, 2818.3701171875, 2840.68994140625, 2850.1298828125, 2857.050048828125, 2862.9599609375, 2861.820068359375, 2856.97998046875, 2874.68994140625, 2896.739990234375, 2897.52001953125, 2914.0400390625, 2901.1298828125, 2901.52001953125, 2896.719970703125, 2888.60009765625, 2878.050048828125, 2871.679931640625, 2877.1298828125, 2887.889892578125, 2888.919921875, 2904.179931640625, 2904.97998046875, 2888.800048828125, 2904.31005859375, 2907.949951171875, 2930.75, 2929.669921875, 2919.3701171875, 2915.56005859375, 2905.969970703125, 2914.0, 2913.97998046875, 2924.590087890625, 2923.429931640625, 2925.510009765625, 2901.610107421875, 2885.570068359375, 2884.429931640625, 2880.340087890625, 2785.679931640625, 2728.3701171875, 2767.1298828125, 2750.7900390625, 2809.919921875, 2809.2099609375, 2768.780029296875, 2767.780029296875, 2755.8798828125, 2740.68994140625, 2656.10009765625, 2705.570068359375, 2658.68994140625, 2641.25, 2682.6298828125, 2711.739990234375, 2740.3701171875, 2723.06005859375, 2738.31005859375, 2755.449951171875, 2813.889892578125, 2806.830078125, 2781.010009765625, 2726.219970703125, 2722.179931640625, 2701.580078125, 2730.199951171875, 2736.27001953125, 2690.72998046875, 2641.889892578125, 2649.929931640625, 2632.56005859375, 2673.449951171875, 2682.169921875, 2743.7900390625, 2737.800048828125, 2760.169921875, 2790.3701171875, 2700.06005859375, 2695.949951171875, 2633.080078125, 2637.719970703125, 2636.780029296875, 2651.070068359375, 2650.5400390625, 2599.949951171875, 2545.93994140625, 2546.159912109375, 2506.9599609375, 2467.419921875, 2416.6201171875, 2351.10009765625, 2467.699951171875, 2488.830078125, 2485.739990234375, 2506.85009765625, 2510.030029296875, 2447.889892578125, 2531.93994140625, 2549.68994140625, 2574.409912109375, 2584.9599609375, 2596.639892578125, 2596.260009765625, 2582.610107421875, 2610.300048828125, 2616.10009765625, 2635.9599609375, 2670.7099609375, 2632.89990234375, 2638.699951171875, 2642.330078125, 2664.760009765625, 2643.85009765625, 2640.0, 2681.050048828125, 2704.10009765625, 2706.530029296875, 2724.8701171875, 2737.699951171875, 2731.610107421875, 2706.050048828125, 2707.8798828125, 2709.800048828125, 2744.72998046875, 2753.030029296875, 2745.72998046875, 2775.60009765625, 2779.760009765625, 2784.699951171875, 2774.8798828125, 2792.669921875, 2796.110107421875, 2793.89990234375, 2792.3798828125, 2784.489990234375, 2803.68994140625, 2792.81005859375, 2789.64990234375, 2771.449951171875, 2748.929931640625, 2743.070068359375, 2783.300048828125, 2791.52001953125, 2810.919921875, 2808.47998046875, 2822.47998046875, 2832.93994140625, 2832.570068359375, 2824.22998046875, 2854.8798828125, 2800.7099609375, 2798.360107421875, 2818.4599609375, 2805.3701171875, 2815.43994140625, 2834.39990234375, 2867.18994140625, 2867.239990234375, 2873.39990234375, 2879.389892578125, 2892.739990234375, 2895.77001953125, 2878.199951171875, 2888.2099609375, 2888.320068359375, 2907.409912109375, 2905.580078125, 2907.06005859375, 2900.449951171875, 2905.030029296875, 2907.969970703125, 2933.679931640625, 2927.25, 2926.169921875, 2939.8798828125, 2943.030029296875, 2945.830078125, 2923.72998046875, 2917.52001953125, 2945.639892578125, 2932.469970703125, 2884.050048828125, 2879.419921875, 2870.719970703125, 2881.39990234375, 2811.8701171875, 2834.409912109375, 2850.9599609375, 2876.320068359375, 2859.530029296875, 2840.22998046875, 2864.360107421875, 2856.27001953125, 2822.239990234375, 2826.06005859375, 2802.389892578125, 2783.02001953125, 2788.860107421875, 2752.06005859375, 2744.449951171875, 2803.27001953125, 2826.14990234375, 2843.489990234375, 2873.340087890625, 2886.72998046875, 2885.719970703125, 2879.840087890625, 2891.639892578125, 2886.97998046875, 2889.669921875, 2917.75, 2926.4599609375, 2954.179931640625, 2950.4599609375, 2945.35009765625, 2917.3798828125, 2913.780029296875, 2924.919921875, 2941.760009765625, 2964.330078125, 2973.010009765625, 2995.820068359375, 2990.409912109375, 2975.949951171875, 2979.6298828125, 2993.070068359375, 2999.909912109375, 3013.77001953125, 3014.300048828125, 3004.0400390625, 2984.419921875, 2995.110107421875, 2976.610107421875, 2985.030029296875, 3005.469970703125, 3019.56005859375, 3003.669921875, 3025.860107421875, 3020.969970703125, 3013.179931640625, 2980.3798828125, 2953.56005859375, 2932.050048828125, 2844.739990234375, 2881.77001953125, 2883.97998046875, 2938.090087890625, 2918.64990234375, 2882.699951171875, 2926.320068359375, 2840.60009765625, 2847.60009765625, 2888.679931640625, 2923.64990234375, 2900.510009765625, 2924.429931640625, 2922.949951171875, 2847.110107421875, 2878.3798828125, 2869.159912109375, 2887.93994140625, 2924.580078125, 2926.4599609375, 2906.27001953125, 2937.780029296875, 2976.0, 2978.7099609375, 2978.429931640625, 2979.389892578125, 3000.929931640625, 3009.570068359375, 3007.389892578125, 2997.9599609375, 3005.699951171875, 3006.72998046875, 3006.7900390625, 2992.070068359375, 2991.780029296875, 2966.60009765625, 2984.8701171875, 2977.6201171875, 2961.7900390625, 2976.739990234375, 2940.25, 2887.610107421875, 2910.6298828125, 2952.010009765625, 2938.7900390625, 2893.06005859375, 2919.39990234375, 2938.1298828125, 2970.27001953125, 2966.14990234375, 2995.679931640625, 2989.68994140625, 2997.949951171875, 2986.199951171875, 3006.719970703125, 2995.989990234375, 3004.52001953125, 3010.2900390625, 3022.550048828125, 3039.419921875, 3036.889892578125, 3046.77001953125, 3037.56005859375, 3066.909912109375, 3078.27001953125, 3074.6201171875, 3076.780029296875, 3085.179931640625, 3093.080078125, 3087.010009765625, 3091.840087890625, 3094.0400390625, 3096.6298828125, 3120.4599609375, 3122.030029296875, 3120.179931640625, 3108.4599609375, 3103.5400390625, 3110.2900390625, 3133.639892578125, 3140.52001953125, 3153.6298828125, 3140.97998046875, 3113.8701171875, 3093.199951171875, 3112.760009765625, 3117.429931640625, 3145.909912109375, 3135.9599609375, 3132.52001953125, 3141.6298828125, 3168.570068359375, 3168.800048828125, 3191.449951171875, 3192.52001953125, 3191.139892578125, 3205.3701171875, 3221.219970703125, 3224.010009765625, 3223.3798828125, 3239.909912109375, 3240.02001953125, 3221.2900390625, 3230.780029296875, 3257.85009765625, 3234.85009765625, 3246.280029296875, 3237.179931640625, 3253.050048828125, 3274.699951171875, 3265.35009765625, 3288.1298828125, 3283.14990234375, 3289.2900390625, 3316.81005859375, 3329.6201171875, 3320.7900390625, 3321.75, 3325.5400390625, 3295.469970703125, 3243.6298828125, 3276.239990234375, 3273.39990234375, 3283.659912109375, 3225.52001953125, 3248.919921875, 3297.590087890625, 3334.68994140625, 3345.780029296875, 3327.7099609375, 3352.090087890625, 3357.75, 3379.449951171875, 3373.93994140625, 3380.159912109375, 3370.2900390625, 3386.14990234375, 3373.22998046875, 3337.75, 3225.889892578125, 3128.2099609375, 3116.389892578125, 2978.760009765625, 2954.219970703125, 3090.22998046875, 3003.3701171875, 3130.1201171875, 3023.93994140625, 2972.3701171875, 2746.56005859375, 2882.22998046875, 2741.3798828125, 2480.639892578125, 2711.02001953125, 2386.1298828125, 2529.18994140625, 2398.10009765625, 2409.389892578125, 2304.919921875, 2237.39990234375, 2447.330078125, 2475.56005859375, 2630.070068359375, 2541.469970703125, 2626.64990234375, 2584.590087890625, 2470.5, 2526.89990234375, 2488.64990234375, 2663.679931640625, 2659.409912109375, 2749.97998046875, 2789.820068359375, 2761.6298828125, 2846.06005859375, 2783.360107421875, 2799.550048828125, 2874.56005859375, 2823.159912109375, 2736.56005859375, 2799.31005859375, 2797.800048828125, 2836.739990234375, 2878.47998046875, 2863.389892578125, 2939.510009765625, 2912.429931640625, 2830.7099609375, 2842.739990234375, 2868.43994140625, 2848.419921875, 2881.18994140625, 2929.800048828125, 2930.18994140625, 2870.1201171875, 2820.0, 2852.5, 2863.699951171875, 2953.909912109375, 2922.93994140625, 2971.610107421875, 2948.510009765625, 2955.449951171875, 2991.77001953125, 3036.1298828125, 3029.72998046875, 3044.31005859375, 3055.72998046875, 3080.820068359375, 3122.8701171875, 3112.35009765625, 3193.929931640625, 3232.389892578125, 3207.179931640625, 3190.139892578125, 3002.10009765625, 3041.31005859375, 3066.590087890625, 3124.739990234375, 3113.489990234375, 3115.340087890625, 3097.739990234375, 3117.860107421875, 3131.2900390625, 3050.330078125, 3083.760009765625, 3009.050048828125, 3053.239990234375, 3100.2900390625, 3115.860107421875, 3130.010009765625, 3179.719970703125, 3145.320068359375, 3169.93994140625, 3152.050048828125, 3185.0400390625, 3155.219970703125, 3197.52001953125, 3226.56005859375, 3215.570068359375, 3224.72998046875, 3251.840087890625, 3257.300048828125, 3276.02001953125, 3235.659912109375, 3215.6298828125, 3239.409912109375, 3218.43994140625, 3258.43994140625, 3246.219970703125, 3271.1201171875, 3294.610107421875, 3306.510009765625, 3327.77001953125, 3349.159912109375, 3351.280029296875, 3360.469970703125, 3333.68994140625, 3380.35009765625, 3373.429931640625, 3372.85009765625, 3381.989990234375, 3389.780029296875, 3374.85009765625, 3385.510009765625, 3397.159912109375, 3431.280029296875, 3443.6201171875, 3478.72998046875, 3484.550048828125, 3508.010009765625, 3500.31005859375, 3526.64990234375, 3580.840087890625, 3455.06005859375, 3426.9599609375, 3331.840087890625, 3398.9599609375, 3339.18994140625, 3340.969970703125, 3383.5400390625, 3401.199951171875, 3385.489990234375, 3357.010009765625, 3319.469970703125, 3281.06005859375, 3315.570068359375, 3236.919921875, 3246.590087890625, 3298.4599609375, 3351.60009765625, 3335.469970703125, 3363.0, 3380.800048828125, 3348.419921875, 3408.60009765625, 3360.969970703125, 3419.43994140625, 3446.830078125, 3477.139892578125, 3534.219970703125, 3511.929931640625, 3488.669921875, 3483.340087890625, 3483.81005859375, 3426.919921875, 3443.1201171875, 3435.56005859375, 3453.489990234375, 3465.389892578125, 3400.969970703125, 3390.679931640625, 3271.030029296875, 3310.110107421875, 3269.9599609375, 3310.239990234375, 3369.159912109375, 3443.43994140625, 3510.449951171875, 3509.43994140625, 3550.5, 3545.530029296875, 3572.659912109375, 3537.010009765625, 3585.14990234375, 3626.909912109375, 3609.530029296875, 3567.7900390625, 3581.8701171875, 3557.5400390625, 3577.590087890625, 3635.409912109375, 3629.64990234375, 3638.35009765625, 3621.6298828125, 3662.449951171875, 3669.010009765625, 3666.719970703125, 3699.1201171875, 3691.9599609375, 3702.25, 3672.820068359375, 3668.10009765625, 3663.4599609375, 3647.489990234375, 3694.6201171875, 3701.169921875, 3722.47998046875, 3709.409912109375, 3694.919921875, 3687.260009765625, 3690.010009765625, 3703.06005859375, 3735.360107421875, 3727.0400390625, 3732.0400390625, 3756.070068359375, 3700.64990234375, 3726.860107421875, 3748.139892578125, 3803.7900390625, 3824.679931640625, 3799.610107421875, 3801.18994140625, 3809.840087890625, 3795.5400390625, 3768.25, 3798.909912109375, 3851.85009765625, 3853.070068359375, 3841.469970703125, 3855.360107421875, 3849.6201171875, 3750.77001953125, 3787.3798828125, 3714.239990234375, 3773.860107421875, 3826.31005859375, 3830.169921875, 3871.739990234375, 3886.830078125, 3915.590087890625, 3911.22998046875, 3909.8798828125, 3916.3798828125, 3934.830078125, 3932.590087890625, 3931.330078125, 3913.969970703125, 3906.7099609375, 3876.5, 3881.3701171875, 3925.429931640625, 3829.340087890625, 3811.14990234375, 3901.820068359375, 3870.2900390625, 3819.719970703125, 3768.469970703125, 3841.93994140625, 3821.35009765625, 3875.43994140625, 3898.81005859375, 3939.340087890625, 3943.340087890625, 3968.93994140625, 3962.7099609375, 3974.1201171875, 3915.4599609375, 3913.10009765625, 3940.590087890625, 3910.52001953125, 3889.139892578125, 3909.52001953125, 3974.5400390625, 3971.090087890625, 3958.550048828125, 3972.889892578125, 4019.8701171875, 4077.909912109375, 4073.93994140625, 4079.949951171875, 4097.169921875, 4128.7998046875, 4127.990234375, 4141.58984375, 4124.66015625, 4170.419921875, 4185.47021484375, 4163.259765625, 4134.93994140625, 4173.419921875, 4134.97998046875, 4180.169921875, 4187.6201171875, 4186.72021484375, 4183.18017578125, 4211.47021484375, 4181.169921875, 4192.66015625, 4164.66015625, 4167.58984375, 4201.6201171875, 4232.60009765625, 4188.43017578125, 4152.10009765625, 4063.0400390625, 4112.5, 4173.85009765625, 4163.2900390625, 4127.830078125, 4115.68017578125, 4159.1201171875, 4155.85986328125, 4197.0498046875, 4188.1298828125, 4195.990234375, 4200.8798828125, 4204.10986328125, 4202.0400390625, 4208.1201171875, 4192.85009765625, 4229.89013671875, 4226.52001953125, 4227.259765625, 4219.5498046875, 4239.18017578125, 4247.43994140625, 4255.14990234375]}, {\"fill\": \"tozeroy\", \"fillcolor\": \"rgba(0, 0, 255, 0.15)\", \"line\": {\"color\": \"rgba(0, 0, 255, 0.2)\"}, \"mode\": \"lines\", \"name\": \"Net wealth of the fund\", \"type\": \"scatter\", \"x\": [\"2017-06-15T00:00:00\", \"2017-06-16T00:00:00\", \"2017-06-19T00:00:00\", \"2017-06-20T00:00:00\", \"2017-06-21T00:00:00\", \"2017-06-22T00:00:00\", \"2017-06-23T00:00:00\", \"2017-06-26T00:00:00\", \"2017-06-27T00:00:00\", \"2017-06-28T00:00:00\", \"2017-06-29T00:00:00\", \"2017-06-30T00:00:00\", \"2017-07-03T00:00:00\", \"2017-07-05T00:00:00\", \"2017-07-06T00:00:00\", \"2017-07-07T00:00:00\", \"2017-07-10T00:00:00\", \"2017-07-11T00:00:00\", \"2017-07-12T00:00:00\", \"2017-07-13T00:00:00\", \"2017-07-14T00:00:00\", \"2017-07-17T00:00:00\", \"2017-07-18T00:00:00\", \"2017-07-19T00:00:00\", \"2017-07-20T00:00:00\", \"2017-07-21T00:00:00\", \"2017-07-24T00:00:00\", \"2017-07-25T00:00:00\", \"2017-07-26T00:00:00\", \"2017-07-27T00:00:00\", \"2017-07-28T00:00:00\", \"2017-07-31T00:00:00\", \"2017-08-01T00:00:00\", \"2017-08-02T00:00:00\", \"2017-08-03T00:00:00\", \"2017-08-04T00:00:00\", \"2017-08-07T00:00:00\", \"2017-08-08T00:00:00\", \"2017-08-09T00:00:00\", \"2017-08-10T00:00:00\", \"2017-08-11T00:00:00\", \"2017-08-14T00:00:00\", \"2017-08-15T00:00:00\", \"2017-08-16T00:00:00\", \"2017-08-17T00:00:00\", \"2017-08-18T00:00:00\", \"2017-08-21T00:00:00\", \"2017-08-22T00:00:00\", \"2017-08-23T00:00:00\", \"2017-08-24T00:00:00\", \"2017-08-25T00:00:00\", \"2017-08-28T00:00:00\", \"2017-08-29T00:00:00\", \"2017-08-30T00:00:00\", \"2017-08-31T00:00:00\", \"2017-09-01T00:00:00\", \"2017-09-05T00:00:00\", \"2017-09-06T00:00:00\", \"2017-09-07T00:00:00\", \"2017-09-08T00:00:00\", \"2017-09-11T00:00:00\", \"2017-09-12T00:00:00\", \"2017-09-13T00:00:00\", \"2017-09-14T00:00:00\", \"2017-09-15T00:00:00\", \"2017-09-18T00:00:00\", \"2017-09-19T00:00:00\", \"2017-09-20T00:00:00\", \"2017-09-21T00:00:00\", \"2017-09-22T00:00:00\", \"2017-09-25T00:00:00\", \"2017-09-26T00:00:00\", \"2017-09-27T00:00:00\", \"2017-09-28T00:00:00\", \"2017-09-29T00:00:00\", \"2017-10-02T00:00:00\", \"2017-10-03T00:00:00\", \"2017-10-04T00:00:00\", \"2017-10-05T00:00:00\", \"2017-10-06T00:00:00\", \"2017-10-09T00:00:00\", \"2017-10-10T00:00:00\", \"2017-10-11T00:00:00\", \"2017-10-12T00:00:00\", \"2017-10-13T00:00:00\", \"2017-10-16T00:00:00\", \"2017-10-17T00:00:00\", \"2017-10-18T00:00:00\", \"2017-10-19T00:00:00\", \"2017-10-20T00:00:00\", \"2017-10-23T00:00:00\", \"2017-10-24T00:00:00\", \"2017-10-25T00:00:00\", \"2017-10-26T00:00:00\", \"2017-10-27T00:00:00\", \"2017-10-30T00:00:00\", \"2017-10-31T00:00:00\", \"2017-11-01T00:00:00\", \"2017-11-02T00:00:00\", \"2017-11-03T00:00:00\", \"2017-11-06T00:00:00\", \"2017-11-07T00:00:00\", \"2017-11-08T00:00:00\", \"2017-11-09T00:00:00\", \"2017-11-10T00:00:00\", \"2017-11-13T00:00:00\", \"2017-11-14T00:00:00\", \"2017-11-15T00:00:00\", \"2017-11-16T00:00:00\", \"2017-11-17T00:00:00\", \"2017-11-20T00:00:00\", \"2017-11-21T00:00:00\", \"2017-11-22T00:00:00\", \"2017-11-24T00:00:00\", \"2017-11-27T00:00:00\", \"2017-11-28T00:00:00\", \"2017-11-29T00:00:00\", \"2017-11-30T00:00:00\", \"2017-12-01T00:00:00\", \"2017-12-04T00:00:00\", \"2017-12-05T00:00:00\", \"2017-12-06T00:00:00\", \"2017-12-07T00:00:00\", \"2017-12-08T00:00:00\", \"2017-12-11T00:00:00\", \"2017-12-12T00:00:00\", \"2017-12-13T00:00:00\", \"2017-12-14T00:00:00\", \"2017-12-15T00:00:00\", \"2017-12-18T00:00:00\", \"2017-12-19T00:00:00\", \"2017-12-20T00:00:00\", \"2017-12-21T00:00:00\", \"2017-12-22T00:00:00\", \"2017-12-26T00:00:00\", \"2017-12-27T00:00:00\", \"2017-12-28T00:00:00\", \"2017-12-29T00:00:00\", \"2018-01-02T00:00:00\", \"2018-01-03T00:00:00\", \"2018-01-04T00:00:00\", \"2018-01-05T00:00:00\", \"2018-01-08T00:00:00\", \"2018-01-09T00:00:00\", \"2018-01-10T00:00:00\", \"2018-01-11T00:00:00\", \"2018-01-12T00:00:00\", \"2018-01-16T00:00:00\", \"2018-01-17T00:00:00\", \"2018-01-18T00:00:00\", \"2018-01-19T00:00:00\", \"2018-01-22T00:00:00\", \"2018-01-23T00:00:00\", \"2018-01-24T00:00:00\", \"2018-01-25T00:00:00\", \"2018-01-26T00:00:00\", \"2018-01-29T00:00:00\", \"2018-01-30T00:00:00\", \"2018-01-31T00:00:00\", \"2018-02-01T00:00:00\", \"2018-02-02T00:00:00\", \"2018-02-05T00:00:00\", \"2018-02-06T00:00:00\", \"2018-02-07T00:00:00\", \"2018-02-08T00:00:00\", \"2018-02-09T00:00:00\", \"2018-02-12T00:00:00\", \"2018-02-13T00:00:00\", \"2018-02-14T00:00:00\", \"2018-02-15T00:00:00\", \"2018-02-16T00:00:00\", \"2018-02-20T00:00:00\", \"2018-02-21T00:00:00\", \"2018-02-22T00:00:00\", \"2018-02-23T00:00:00\", \"2018-02-26T00:00:00\", \"2018-02-27T00:00:00\", \"2018-02-28T00:00:00\", \"2018-03-01T00:00:00\", \"2018-03-02T00:00:00\", \"2018-03-05T00:00:00\", \"2018-03-06T00:00:00\", \"2018-03-07T00:00:00\", \"2018-03-08T00:00:00\", \"2018-03-09T00:00:00\", \"2018-03-12T00:00:00\", \"2018-03-13T00:00:00\", \"2018-03-14T00:00:00\", \"2018-03-15T00:00:00\", \"2018-03-16T00:00:00\", \"2018-03-19T00:00:00\", \"2018-03-20T00:00:00\", \"2018-03-21T00:00:00\", \"2018-03-22T00:00:00\", \"2018-03-23T00:00:00\", \"2018-03-26T00:00:00\", \"2018-03-27T00:00:00\", \"2018-03-28T00:00:00\", \"2018-03-29T00:00:00\", \"2018-04-02T00:00:00\", \"2018-04-03T00:00:00\", \"2018-04-04T00:00:00\", \"2018-04-05T00:00:00\", \"2018-04-06T00:00:00\", \"2018-04-09T00:00:00\", \"2018-04-10T00:00:00\", \"2018-04-11T00:00:00\", \"2018-04-12T00:00:00\", \"2018-04-13T00:00:00\", \"2018-04-16T00:00:00\", \"2018-04-17T00:00:00\", \"2018-04-18T00:00:00\", \"2018-04-19T00:00:00\", \"2018-04-20T00:00:00\", \"2018-04-23T00:00:00\", \"2018-04-24T00:00:00\", \"2018-04-25T00:00:00\", \"2018-04-26T00:00:00\", \"2018-04-27T00:00:00\", \"2018-04-30T00:00:00\", \"2018-05-01T00:00:00\", \"2018-05-02T00:00:00\", \"2018-05-03T00:00:00\", \"2018-05-04T00:00:00\", \"2018-05-07T00:00:00\", \"2018-05-08T00:00:00\", \"2018-05-09T00:00:00\", \"2018-05-10T00:00:00\", \"2018-05-11T00:00:00\", \"2018-05-14T00:00:00\", \"2018-05-15T00:00:00\", \"2018-05-16T00:00:00\", \"2018-05-17T00:00:00\", \"2018-05-18T00:00:00\", \"2018-05-21T00:00:00\", \"2018-05-22T00:00:00\", \"2018-05-23T00:00:00\", \"2018-05-24T00:00:00\", \"2018-05-25T00:00:00\", \"2018-05-29T00:00:00\", \"2018-05-30T00:00:00\", \"2018-05-31T00:00:00\", \"2018-06-01T00:00:00\", \"2018-06-04T00:00:00\", \"2018-06-05T00:00:00\", \"2018-06-06T00:00:00\", \"2018-06-07T00:00:00\", \"2018-06-08T00:00:00\", \"2018-06-11T00:00:00\", \"2018-06-12T00:00:00\", \"2018-06-13T00:00:00\", \"2018-06-14T00:00:00\", \"2018-06-15T00:00:00\", \"2018-06-18T00:00:00\", \"2018-06-19T00:00:00\", \"2018-06-20T00:00:00\", \"2018-06-21T00:00:00\", \"2018-06-22T00:00:00\", \"2018-06-25T00:00:00\", \"2018-06-26T00:00:00\", \"2018-06-27T00:00:00\", \"2018-06-28T00:00:00\", \"2018-06-29T00:00:00\", \"2018-07-02T00:00:00\", \"2018-07-03T00:00:00\", \"2018-07-05T00:00:00\", \"2018-07-06T00:00:00\", \"2018-07-09T00:00:00\", \"2018-07-10T00:00:00\", \"2018-07-11T00:00:00\", \"2018-07-12T00:00:00\", \"2018-07-13T00:00:00\", \"2018-07-16T00:00:00\", \"2018-07-17T00:00:00\", \"2018-07-18T00:00:00\", \"2018-07-19T00:00:00\", \"2018-07-20T00:00:00\", \"2018-07-23T00:00:00\", \"2018-07-24T00:00:00\", \"2018-07-25T00:00:00\", \"2018-07-26T00:00:00\", \"2018-07-27T00:00:00\", \"2018-07-30T00:00:00\", \"2018-07-31T00:00:00\", \"2018-08-01T00:00:00\", \"2018-08-02T00:00:00\", \"2018-08-03T00:00:00\", \"2018-08-06T00:00:00\", \"2018-08-07T00:00:00\", \"2018-08-08T00:00:00\", \"2018-08-09T00:00:00\", \"2018-08-10T00:00:00\", \"2018-08-13T00:00:00\", \"2018-08-14T00:00:00\", \"2018-08-15T00:00:00\", \"2018-08-16T00:00:00\", \"2018-08-17T00:00:00\", \"2018-08-20T00:00:00\", \"2018-08-21T00:00:00\", \"2018-08-22T00:00:00\", \"2018-08-23T00:00:00\", \"2018-08-24T00:00:00\", \"2018-08-27T00:00:00\", \"2018-08-28T00:00:00\", \"2018-08-29T00:00:00\", \"2018-08-30T00:00:00\", \"2018-08-31T00:00:00\", \"2018-09-04T00:00:00\", \"2018-09-05T00:00:00\", \"2018-09-06T00:00:00\", \"2018-09-07T00:00:00\", \"2018-09-10T00:00:00\", \"2018-09-11T00:00:00\", \"2018-09-12T00:00:00\", \"2018-09-13T00:00:00\", \"2018-09-14T00:00:00\", \"2018-09-17T00:00:00\", \"2018-09-18T00:00:00\", \"2018-09-19T00:00:00\", \"2018-09-20T00:00:00\", \"2018-09-21T00:00:00\", \"2018-09-24T00:00:00\", \"2018-09-25T00:00:00\", \"2018-09-26T00:00:00\", \"2018-09-27T00:00:00\", \"2018-09-28T00:00:00\", \"2018-10-01T00:00:00\", \"2018-10-02T00:00:00\", \"2018-10-03T00:00:00\", \"2018-10-04T00:00:00\", \"2018-10-05T00:00:00\", \"2018-10-08T00:00:00\", \"2018-10-09T00:00:00\", \"2018-10-10T00:00:00\", \"2018-10-11T00:00:00\", \"2018-10-12T00:00:00\", \"2018-10-15T00:00:00\", \"2018-10-16T00:00:00\", \"2018-10-17T00:00:00\", \"2018-10-18T00:00:00\", \"2018-10-19T00:00:00\", \"2018-10-22T00:00:00\", \"2018-10-23T00:00:00\", \"2018-10-24T00:00:00\", \"2018-10-25T00:00:00\", \"2018-10-26T00:00:00\", \"2018-10-29T00:00:00\", \"2018-10-30T00:00:00\", \"2018-10-31T00:00:00\", \"2018-11-01T00:00:00\", \"2018-11-02T00:00:00\", \"2018-11-05T00:00:00\", \"2018-11-06T00:00:00\", \"2018-11-07T00:00:00\", \"2018-11-08T00:00:00\", \"2018-11-09T00:00:00\", \"2018-11-12T00:00:00\", \"2018-11-13T00:00:00\", \"2018-11-14T00:00:00\", \"2018-11-15T00:00:00\", \"2018-11-16T00:00:00\", \"2018-11-19T00:00:00\", \"2018-11-20T00:00:00\", \"2018-11-21T00:00:00\", \"2018-11-23T00:00:00\", \"2018-11-26T00:00:00\", \"2018-11-27T00:00:00\", \"2018-11-28T00:00:00\", \"2018-11-29T00:00:00\", \"2018-11-30T00:00:00\", \"2018-12-03T00:00:00\", \"2018-12-04T00:00:00\", \"2018-12-06T00:00:00\", \"2018-12-07T00:00:00\", \"2018-12-10T00:00:00\", \"2018-12-11T00:00:00\", \"2018-12-12T00:00:00\", \"2018-12-13T00:00:00\", \"2018-12-14T00:00:00\", \"2018-12-17T00:00:00\", \"2018-12-18T00:00:00\", \"2018-12-19T00:00:00\", \"2018-12-20T00:00:00\", \"2018-12-21T00:00:00\", \"2018-12-24T00:00:00\", \"2018-12-26T00:00:00\", \"2018-12-27T00:00:00\", \"2018-12-28T00:00:00\", \"2018-12-31T00:00:00\", \"2019-01-02T00:00:00\", \"2019-01-03T00:00:00\", \"2019-01-04T00:00:00\", \"2019-01-07T00:00:00\", \"2019-01-08T00:00:00\", \"2019-01-09T00:00:00\", \"2019-01-10T00:00:00\", \"2019-01-11T00:00:00\", \"2019-01-14T00:00:00\", \"2019-01-15T00:00:00\", \"2019-01-16T00:00:00\", \"2019-01-17T00:00:00\", \"2019-01-18T00:00:00\", \"2019-01-22T00:00:00\", \"2019-01-23T00:00:00\", \"2019-01-24T00:00:00\", \"2019-01-25T00:00:00\", \"2019-01-28T00:00:00\", \"2019-01-29T00:00:00\", \"2019-01-30T00:00:00\", \"2019-01-31T00:00:00\", \"2019-02-01T00:00:00\", \"2019-02-04T00:00:00\", \"2019-02-05T00:00:00\", \"2019-02-06T00:00:00\", \"2019-02-07T00:00:00\", \"2019-02-08T00:00:00\", \"2019-02-11T00:00:00\", \"2019-02-12T00:00:00\", \"2019-02-13T00:00:00\", \"2019-02-14T00:00:00\", \"2019-02-15T00:00:00\", \"2019-02-19T00:00:00\", \"2019-02-20T00:00:00\", \"2019-02-21T00:00:00\", \"2019-02-22T00:00:00\", \"2019-02-25T00:00:00\", \"2019-02-26T00:00:00\", \"2019-02-27T00:00:00\", \"2019-02-28T00:00:00\", \"2019-03-01T00:00:00\", \"2019-03-04T00:00:00\", \"2019-03-05T00:00:00\", \"2019-03-06T00:00:00\", \"2019-03-07T00:00:00\", \"2019-03-08T00:00:00\", \"2019-03-11T00:00:00\", \"2019-03-12T00:00:00\", \"2019-03-13T00:00:00\", \"2019-03-14T00:00:00\", \"2019-03-15T00:00:00\", \"2019-03-18T00:00:00\", \"2019-03-19T00:00:00\", \"2019-03-20T00:00:00\", \"2019-03-21T00:00:00\", \"2019-03-22T00:00:00\", \"2019-03-25T00:00:00\", \"2019-03-26T00:00:00\", \"2019-03-27T00:00:00\", \"2019-03-28T00:00:00\", \"2019-03-29T00:00:00\", \"2019-04-01T00:00:00\", \"2019-04-02T00:00:00\", \"2019-04-03T00:00:00\", \"2019-04-04T00:00:00\", \"2019-04-05T00:00:00\", \"2019-04-08T00:00:00\", \"2019-04-09T00:00:00\", \"2019-04-10T00:00:00\", \"2019-04-11T00:00:00\", \"2019-04-12T00:00:00\", \"2019-04-15T00:00:00\", \"2019-04-16T00:00:00\", \"2019-04-17T00:00:00\", \"2019-04-18T00:00:00\", \"2019-04-22T00:00:00\", \"2019-04-23T00:00:00\", \"2019-04-24T00:00:00\", \"2019-04-25T00:00:00\", \"2019-04-26T00:00:00\", \"2019-04-29T00:00:00\", \"2019-04-30T00:00:00\", \"2019-05-01T00:00:00\", \"2019-05-02T00:00:00\", \"2019-05-03T00:00:00\", \"2019-05-06T00:00:00\", \"2019-05-07T00:00:00\", \"2019-05-08T00:00:00\", \"2019-05-09T00:00:00\", \"2019-05-10T00:00:00\", \"2019-05-13T00:00:00\", \"2019-05-14T00:00:00\", \"2019-05-15T00:00:00\", \"2019-05-16T00:00:00\", \"2019-05-17T00:00:00\", \"2019-05-20T00:00:00\", \"2019-05-21T00:00:00\", \"2019-05-22T00:00:00\", \"2019-05-23T00:00:00\", \"2019-05-24T00:00:00\", \"2019-05-28T00:00:00\", \"2019-05-29T00:00:00\", \"2019-05-30T00:00:00\", \"2019-05-31T00:00:00\", \"2019-06-03T00:00:00\", \"2019-06-04T00:00:00\", \"2019-06-05T00:00:00\", \"2019-06-06T00:00:00\", \"2019-06-07T00:00:00\", \"2019-06-10T00:00:00\", \"2019-06-11T00:00:00\", \"2019-06-12T00:00:00\", \"2019-06-13T00:00:00\", \"2019-06-14T00:00:00\", \"2019-06-17T00:00:00\", \"2019-06-18T00:00:00\", \"2019-06-19T00:00:00\", \"2019-06-20T00:00:00\", \"2019-06-21T00:00:00\", \"2019-06-24T00:00:00\", \"2019-06-25T00:00:00\", \"2019-06-26T00:00:00\", \"2019-06-27T00:00:00\", \"2019-06-28T00:00:00\", \"2019-07-01T00:00:00\", \"2019-07-02T00:00:00\", \"2019-07-03T00:00:00\", \"2019-07-05T00:00:00\", \"2019-07-08T00:00:00\", \"2019-07-09T00:00:00\", \"2019-07-10T00:00:00\", \"2019-07-11T00:00:00\", \"2019-07-12T00:00:00\", \"2019-07-15T00:00:00\", \"2019-07-16T00:00:00\", \"2019-07-17T00:00:00\", \"2019-07-18T00:00:00\", \"2019-07-19T00:00:00\", \"2019-07-22T00:00:00\", \"2019-07-23T00:00:00\", \"2019-07-24T00:00:00\", \"2019-07-25T00:00:00\", \"2019-07-26T00:00:00\", \"2019-07-29T00:00:00\", \"2019-07-30T00:00:00\", \"2019-07-31T00:00:00\", \"2019-08-01T00:00:00\", \"2019-08-02T00:00:00\", \"2019-08-05T00:00:00\", \"2019-08-06T00:00:00\", \"2019-08-07T00:00:00\", \"2019-08-08T00:00:00\", \"2019-08-09T00:00:00\", \"2019-08-12T00:00:00\", \"2019-08-13T00:00:00\", \"2019-08-14T00:00:00\", \"2019-08-15T00:00:00\", \"2019-08-16T00:00:00\", \"2019-08-19T00:00:00\", \"2019-08-20T00:00:00\", \"2019-08-21T00:00:00\", \"2019-08-22T00:00:00\", \"2019-08-23T00:00:00\", \"2019-08-26T00:00:00\", \"2019-08-27T00:00:00\", \"2019-08-28T00:00:00\", \"2019-08-29T00:00:00\", \"2019-08-30T00:00:00\", \"2019-09-03T00:00:00\", \"2019-09-04T00:00:00\", \"2019-09-05T00:00:00\", \"2019-09-06T00:00:00\", \"2019-09-09T00:00:00\", \"2019-09-10T00:00:00\", \"2019-09-11T00:00:00\", \"2019-09-12T00:00:00\", \"2019-09-13T00:00:00\", \"2019-09-16T00:00:00\", \"2019-09-17T00:00:00\", \"2019-09-18T00:00:00\", \"2019-09-19T00:00:00\", \"2019-09-20T00:00:00\", \"2019-09-23T00:00:00\", \"2019-09-24T00:00:00\", \"2019-09-25T00:00:00\", \"2019-09-26T00:00:00\", \"2019-09-27T00:00:00\", \"2019-09-30T00:00:00\", \"2019-10-01T00:00:00\", \"2019-10-02T00:00:00\", \"2019-10-03T00:00:00\", \"2019-10-04T00:00:00\", \"2019-10-07T00:00:00\", \"2019-10-08T00:00:00\", \"2019-10-09T00:00:00\", \"2019-10-10T00:00:00\", \"2019-10-11T00:00:00\", \"2019-10-14T00:00:00\", \"2019-10-15T00:00:00\", \"2019-10-16T00:00:00\", \"2019-10-17T00:00:00\", \"2019-10-18T00:00:00\", \"2019-10-21T00:00:00\", \"2019-10-22T00:00:00\", \"2019-10-23T00:00:00\", \"2019-10-24T00:00:00\", \"2019-10-25T00:00:00\", \"2019-10-28T00:00:00\", \"2019-10-29T00:00:00\", \"2019-10-30T00:00:00\", \"2019-10-31T00:00:00\", \"2019-11-01T00:00:00\", \"2019-11-04T00:00:00\", \"2019-11-05T00:00:00\", \"2019-11-06T00:00:00\", \"2019-11-07T00:00:00\", \"2019-11-08T00:00:00\", \"2019-11-11T00:00:00\", \"2019-11-12T00:00:00\", \"2019-11-13T00:00:00\", \"2019-11-14T00:00:00\", \"2019-11-15T00:00:00\", \"2019-11-18T00:00:00\", \"2019-11-19T00:00:00\", \"2019-11-20T00:00:00\", \"2019-11-21T00:00:00\", \"2019-11-22T00:00:00\", \"2019-11-25T00:00:00\", \"2019-11-26T00:00:00\", \"2019-11-27T00:00:00\", \"2019-11-29T00:00:00\", \"2019-12-02T00:00:00\", \"2019-12-03T00:00:00\", \"2019-12-04T00:00:00\", \"2019-12-05T00:00:00\", \"2019-12-06T00:00:00\", \"2019-12-09T00:00:00\", \"2019-12-10T00:00:00\", \"2019-12-11T00:00:00\", \"2019-12-12T00:00:00\", \"2019-12-13T00:00:00\", \"2019-12-16T00:00:00\", \"2019-12-17T00:00:00\", \"2019-12-18T00:00:00\", \"2019-12-19T00:00:00\", \"2019-12-20T00:00:00\", \"2019-12-23T00:00:00\", \"2019-12-24T00:00:00\", \"2019-12-26T00:00:00\", \"2019-12-27T00:00:00\", \"2019-12-30T00:00:00\", \"2019-12-31T00:00:00\", \"2020-01-02T00:00:00\", \"2020-01-03T00:00:00\", \"2020-01-06T00:00:00\", \"2020-01-07T00:00:00\", \"2020-01-08T00:00:00\", \"2020-01-09T00:00:00\", \"2020-01-10T00:00:00\", \"2020-01-13T00:00:00\", \"2020-01-14T00:00:00\", \"2020-01-15T00:00:00\", \"2020-01-16T00:00:00\", \"2020-01-17T00:00:00\", \"2020-01-21T00:00:00\", \"2020-01-22T00:00:00\", \"2020-01-23T00:00:00\", \"2020-01-24T00:00:00\", \"2020-01-27T00:00:00\", \"2020-01-28T00:00:00\", \"2020-01-29T00:00:00\", \"2020-01-30T00:00:00\", \"2020-01-31T00:00:00\", \"2020-02-03T00:00:00\", \"2020-02-04T00:00:00\", \"2020-02-05T00:00:00\", \"2020-02-06T00:00:00\", \"2020-02-07T00:00:00\", \"2020-02-10T00:00:00\", \"2020-02-11T00:00:00\", \"2020-02-12T00:00:00\", \"2020-02-13T00:00:00\", \"2020-02-14T00:00:00\", \"2020-02-18T00:00:00\", \"2020-02-19T00:00:00\", \"2020-02-20T00:00:00\", \"2020-02-21T00:00:00\", \"2020-02-24T00:00:00\", \"2020-02-25T00:00:00\", \"2020-02-26T00:00:00\", \"2020-02-27T00:00:00\", \"2020-02-28T00:00:00\", \"2020-03-02T00:00:00\", \"2020-03-03T00:00:00\", \"2020-03-04T00:00:00\", \"2020-03-05T00:00:00\", \"2020-03-06T00:00:00\", \"2020-03-09T00:00:00\", \"2020-03-10T00:00:00\", \"2020-03-11T00:00:00\", \"2020-03-12T00:00:00\", \"2020-03-13T00:00:00\", \"2020-03-16T00:00:00\", \"2020-03-17T00:00:00\", \"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\", \"2021-04-23T00:00:00\", \"2021-04-26T00:00:00\", \"2021-04-27T00:00:00\", \"2021-04-28T00:00:00\", \"2021-04-29T00:00:00\", \"2021-04-30T00:00:00\", \"2021-05-03T00:00:00\", \"2021-05-04T00:00:00\", \"2021-05-05T00:00:00\", \"2021-05-06T00:00:00\", \"2021-05-07T00:00:00\", \"2021-05-10T00:00:00\", \"2021-05-11T00:00:00\", \"2021-05-12T00:00:00\", \"2021-05-13T00:00:00\", \"2021-05-14T00:00:00\", \"2021-05-17T00:00:00\", \"2021-05-18T00:00:00\", \"2021-05-19T00:00:00\", \"2021-05-20T00:00:00\", \"2021-05-21T00:00:00\", \"2021-05-24T00:00:00\", \"2021-05-25T00:00:00\", \"2021-05-26T00:00:00\", \"2021-05-27T00:00:00\", \"2021-05-28T00:00:00\", \"2021-06-01T00:00:00\", \"2021-06-02T00:00:00\", \"2021-06-03T00:00:00\", \"2021-06-04T00:00:00\", \"2021-06-07T00:00:00\", \"2021-06-08T00:00:00\", \"2021-06-09T00:00:00\", \"2021-06-10T00:00:00\", \"2021-06-11T00:00:00\", \"2021-06-14T00:00:00\"], \"xaxis\": \"x\", \"y\": [10000.0, 10000.0, 10081.240234375, 10015.5205078125, 10021.2001953125, 10025.640625, 10010.4404296875, 10007.3603515625, 9928.599609375, 10013.83984375, 9929.8798828125, 9944.7197265625, 9967.1201171875, 9953.0, 9861.83984375, 9800.1201171875, 9791.1201171875, 9783.5205078125, 9854.400390625, 9872.720703125, 9918.48046875, 9917.9599609375, 9923.8408203125, 9976.720703125, 9975.2001953125, 9971.560546875, 9961.0400390625, 9989.919921875, 9992.720703125, 9983.080078125, 9969.80078125, 9962.6005859375, 9986.80078125, 9991.6806640625, 9970.0400390625, 9988.720703125, 10005.0400390625, 9981.080078125, 9977.48046875, 10120.720703125, 10108.2802734375, 10206.3603515625, 10201.4404296875, 10215.4404296875, 10063.0400390625, 10045.2001953125, 10056.48046875, 10153.0400390625, 10119.16015625, 10098.8798828125, 10115.2001953125, 10119.9599609375, 10111.7197265625, 10156.8798828125, 10213.119140625, 10232.7197265625, 10157.919921875, 10188.6796875, 10186.919921875, 10172.2392578125, 10065.5185546875, 10098.998046875, 10106.55859375, 10095.55859375, 10113.998046875, 10128.55859375, 10139.677734375, 10146.0380859375, 10115.478515625, 10121.9580078125, 10099.7177734375, 10100.4384765625, 10141.23828125, 10153.318359375, 10190.5185546875, 10229.55859375, 10251.3984375, 10264.0380859375, 10321.3583984375, 10310.3984375, 10291.998046875, 10315.6376953125, 10334.0380859375, 10316.7978515625, 10325.7578125, 10343.6376953125, 10350.5185546875, 10358.1181640625, 10361.478515625, 10413.91796875, 10372.998046875, 10389.59765625, 10341.677734375, 10354.677734375, 10437.3583984375, 10404.3984375, 10414.1181640625, 10430.5185546875, 10432.478515625, 10464.4384765625, 10477.59765625, 10475.6376953125, 10490.59765625, 10451.55859375, 10442.2783203125, 10432.1181640625, 10408.23828125, 10351.23828125, 10435.3173828125, 10408.158203125, 10421.3173828125, 10488.8779296875, 10481.078125, 10502.4375, 10498.4375, 10600.91796875, 10597.0380859375, 10683.078125, 10661.6376953125, 10650.517578125, 10611.0380859375, 10609.837890625, 10640.677734375, 10698.7578125, 10732.7177734375, 10749.1982421875, 10744.158203125, 10700.7978515625, 10795.998046875, 10853.3974609375, 10818.6376953125, 10809.7578125, 10831.0380859375, 10826.1181640625, 10814.7578125, 10823.23828125, 10842.91796875, 10787.1982421875, 10875.998046875, 10944.998046875, 10988.7177734375, 11065.357421875, 11083.59765625, 11097.91796875, 11085.677734375, 11162.998046875, 11237.7177734375, 11198.4375, 11302.998046875, 11284.8779296875, 11333.9580078125, 11424.6376953125, 11449.27734375, 11442.91796875, 11449.7578125, 11584.23828125, 11506.8779296875, 11631.2783203125, 11636.798828125, 11629.478515625, 11868.87890625, 11416.119140625, 11600.9189453125, 11546.9990234375, 11144.359375, 11298.5595703125, 11444.359375, 11472.119140625, 11614.87890625, 11745.1591796875, 11749.2392578125, 11685.3994140625, 11745.119140625, 11755.638671875, 11582.2783203125, 11711.478515625, 11570.1982421875, 11691.998046875, 11547.357421875, 11493.037109375, 11611.796875, 11640.517578125, 11635.2373046875, 11683.9169921875, 11874.3173828125, 11860.1171875, 11789.27734375, 11852.59765625, 11861.197265625, 11842.4775390625, 11686.1171875, 11670.037109375, 11690.0771484375, 11963.037109375, 11741.3173828125, 12022.4775390625, 12206.197265625, 12175.716796875, 12319.197265625, 12083.236328125, 12213.5166015625, 12334.4765625, 12407.0771484375, 12173.5966796875, 12208.3564453125, 12383.197265625, 12324.4765625, 12411.6767578125, 12380.9169921875, 12467.0771484375, 12581.2763671875, 12590.2763671875, 12528.236328125, 12620.1962890625, 12620.796875, 12763.716796875, 12744.357421875, 12634.197265625, 12646.0771484375, 12558.6376953125, 12585.6376953125, 12509.1171875, 12485.357421875, 12620.1171875, 12656.95703125, 12654.1171875, 12757.59765625, 12858.7177734375, 12877.3173828125, 12886.95703125, 12812.2373046875, 12768.197265625, 12758.876953125, 12730.2373046875, 12810.3974609375, 12776.1171875, 12811.517578125, 12789.3974609375, 12763.677734375, 12637.7978515625, 12774.3974609375, 12699.4375, 12816.837890625, 12865.837890625, 12873.5576171875, 12967.7578125, 12959.837890625, 12994.4775390625, 13006.357421875, 13025.7578125, 12980.876953125, 13008.3173828125, 12996.9970703125, 13020.63671875, 12975.9970703125, 12957.0771484375, 12886.8369140625, 12866.357421875, 12715.1181640625, 12739.078125, 12832.798828125, 12899.51953125, 12907.759765625, 12941.119140625, 12887.1591796875, 12980.7197265625, 13073.5595703125, 13170.958984375, 13209.6396484375, 13130.359375, 13033.279296875, 13045.359375, 13033.8388671875, 13078.3193359375, 13102.599609375, 13058.0791015625, 13047.439453125, 13026.83984375, 13080.51953125, 13183.2001953125, 13148.6796875, 13074.2001953125, 13009.3203125, 13064.080078125, 13052.3603515625, 13107.7998046875, 13160.3203125, 13200.51953125, 13232.7197265625, 13229.7197265625, 13213.240234375, 13294.4404296875, 13249.0400390625, 13321.16015625, 13234.80078125, 13324.080078125, 13361.83984375, 13389.5205078125, 13413.16015625, 13408.6005859375, 13389.240234375, 13460.080078125, 13548.2802734375, 13551.400390625, 13617.48046875, 13565.83984375, 13564.279296875, 13583.4794921875, 13615.958984375, 13573.7587890625, 13548.2783203125, 13526.478515625, 13569.5185546875, 13573.638671875, 13634.6787109375, 13637.87890625, 13573.1591796875, 13635.19921875, 13649.7587890625, 13740.958984375, 13736.638671875, 13695.439453125, 13680.19921875, 13718.5595703125, 13750.6796875, 13750.759765625, 13793.2001953125, 13788.5595703125, 13796.8798828125, 13701.2802734375, 13765.4404296875, 13770.0009765625, 13786.3603515625, 14165.0009765625, 13878.451904296875, 14072.250732421875, 13990.551513671875, 13694.902099609375, 13692.062255859375, 13530.342529296875, 13534.342529296875, 13581.943115234375, 13642.702880859375, 13304.343505859375, 13502.223388671875, 13314.702880859375, 13401.902587890625, 13608.802001953125, 13754.3525390625, 13897.503173828125, 13810.952880859375, 13887.202880859375, 13972.90234375, 14265.10205078125, 14229.802978515625, 14100.70263671875, 13826.75244140625, 13806.55224609375, 13909.551513671875, 13766.4521484375, 13796.802490234375, 13569.102294921875, 13813.302734375, 13773.1025390625, 13859.951904296875, 14064.4013671875, 14108.001220703125, 14416.101806640625, 14386.15185546875, 14498.001220703125, 14649.002197265625, 14197.451904296875, 14176.9013671875, 14491.250732421875, 14514.4501953125, 14519.14990234375, 14447.69970703125, 14445.049560546875, 14192.09912109375, 13922.049072265625, 13923.14892578125, 14119.148681640625, 14316.848876953125, 14062.849853515625, 13735.249755859375, 13152.25048828125, 13046.599853515625, 13031.1494140625, 13136.699951171875, 13152.599609375, 12841.89892578125, 13262.149169921875, 13350.899169921875, 13474.4990234375, 13527.249267578125, 13585.64892578125, 13583.74951171875, 13515.5, 13653.94970703125, 13682.949951171875, 13782.249267578125, 13955.999267578125, 13766.948974609375, 13795.94921875, 13814.099853515625, 13926.24951171875, 13821.699951171875, 13802.449462890625, 13597.19921875, 13712.449462890625, 13724.59912109375, 13816.299560546875, 13880.44873046875, 13849.99951171875, 13722.19921875, 13713.050048828125, 13722.65087890625, 13897.300537109375, 13938.80078125, 13902.300537109375, 14051.651123046875, 14072.45068359375, 14097.150390625, 14048.050048828125, 13959.099853515625, 13972.860595703125, 13964.019775390625, 13957.939697265625, 13989.499267578125, 13912.699462890625, 13869.179931640625, 13881.820556640625, 13954.620361328125, 13842.020263671875, 13812.720947265625, 14013.870849609375, 14054.970703125, 14151.97021484375, 14139.7705078125, 14209.7705078125, 14262.0703125, 14260.220947265625, 14218.5205078125, 14065.27099609375, 13848.59130859375, 13839.19189453125, 13919.59130859375, 13867.23193359375, 13907.51123046875, 13983.35107421875, 14114.51123046875, 14114.71142578125, 14139.35107421875, 14163.31103515625, 14216.71142578125, 14228.83154296875, 14158.55126953125, 14118.51123046875, 14118.95166015625, 14042.59228515625, 14035.27294921875, 14041.19287109375, 14014.75244140625, 14033.07275390625, 14021.31298828125, 14124.15283203125, 14098.43310546875, 14094.11279296875, 14148.95263671875, 14161.55322265625, 14172.75341796875, 14084.35302734375, 14109.19287109375, 13996.71337890625, 13944.03369140625, 13750.35400390625, 13768.87451171875, 13734.07470703125, 13691.35498046875, 13969.47412109375, 14059.63330078125, 13993.43310546875, 14094.87353515625, 14027.71337890625, 13950.51318359375, 13853.99267578125, 13821.63232421875, 13685.51220703125, 13670.23193359375, 13764.91259765625, 13842.39208984375, 13865.75244140625, 13718.55224609375, 13688.11181640625, 13452.83154296875, 13544.35107421875, 13613.71142578125, 13733.11181640625, 13786.67138671875, 13782.63134765625, 13759.11181640625, 13806.31103515625, 13787.67138671875, 13798.43115234375, 13910.75146484375, 13945.59130859375, 14056.47119140625, 14041.59130859375, 14021.15185546875, 13909.27099609375, 13894.87158203125, 13939.43115234375, 14006.79150390625, 14097.07177734375, 14131.79150390625, 14223.03173828125, 14201.39111328125, 14143.55126953125, 14158.27099609375, 14212.03173828125, 14239.39111328125, 14294.83154296875, 14296.95166015625, 14255.91162109375, 14177.43115234375, 14220.19189453125, 14146.19189453125, 14179.87158203125, 14261.63134765625, 14317.99169921875, 14254.43115234375, 14165.67041015625, 14146.10986328125, 14114.94970703125, 14246.14990234375, 14353.42919921875, 14439.46923828125, 14788.70947265625, 14973.859619140625, 14984.909423828125, 14714.35888671875, 14617.157958984375, 14437.408203125, 14655.5087890625, 14226.908935546875, 14261.908935546875, 14056.509765625, 14196.3896484375, 14103.830078125, 14199.509765625, 14193.58984375, 13890.23046875, 14015.3095703125, 13978.4296875, 14053.5498046875, 14200.1103515625, 14207.6298828125, 14126.8701171875, 14252.91015625, 14405.7900390625, 14416.6298828125, 14415.509765625, 14419.349609375, 14505.509765625, 14540.0703125, 14531.349609375, 14493.6298828125, 14462.669921875, 14466.7900390625, 14467.0302734375, 14408.150390625, 14406.990234375, 14507.7099609375, 14434.6298828125, 14405.6298828125, 14468.9501953125, 14528.75, 14382.7900390625, 14593.349609375, 14501.2705078125, 14335.75, 14282.8701171875, 14099.9501953125, 14205.3095703125, 14280.2294921875, 14408.7900390625, 14392.3095703125, 14510.4296875, 14486.4697265625, 14519.509765625, 14472.509765625, 14554.58984375, 14511.669921875, 14545.7900390625, 14568.8701171875, 14617.91015625, 14685.3896484375, 14675.26953125, 14714.7900390625, 14677.9501953125, 14795.349609375, 14840.7900390625, 14826.1904296875, 14834.830078125, 14868.4296875, 14900.0302734375, 14875.75, 14895.0703125, 14903.8701171875, 14914.2294921875, 15009.5498046875, 15015.830078125, 15008.4296875, 14961.5498046875, 14941.8701171875, 14968.8701171875, 15062.26953125, 15089.7900390625, 15142.2294921875, 15091.6298828125, 14983.1904296875, 14900.509765625, 14978.75, 14997.4296875, 15111.349609375, 15071.5498046875, 15057.7900390625, 15021.3505859375, 15129.111328125, 15130.03125, 15220.630859375, 15224.9111328125, 15219.390625, 15276.3115234375, 15339.7109375, 15350.87109375, 15348.3505859375, 15414.470703125, 15414.9111328125, 15339.9912109375, 15302.03125, 15410.3115234375, 15318.3115234375, 15364.03125, 15400.431640625, 15463.912109375, 15550.51171875, 15513.1123046875, 15604.2314453125, 15584.3115234375, 15608.8720703125, 15718.9521484375, 15770.1923828125, 15734.8720703125, 15738.7119140625, 15723.5517578125, 15603.271484375, 15810.6318359375, 15941.072265625, 15929.7119140625, 15888.671875, 15656.1123046875, 15749.7119140625, 15944.392578125, 16092.7919921875, 16137.15234375, 16064.8720703125, 16162.392578125, 16185.0322265625, 16271.83203125, 16249.7919921875, 16274.671875, 16235.1923828125, 16171.7529296875, 16120.0732421875, 16261.9931640625, 15814.552734375, 15423.8330078125, 15376.552734375, 14826.033203125, 14727.873046875, 15271.9130859375, 15619.3525390625, 15112.3525390625, 15537.0732421875, 15279.22412109375, 14150.173828125, 14828.5234375, 15532.77392578125, 16836.473876953125, 18218.754638671875, 16269.413818359375, 17127.774169921875, 17914.313232421875, 17993.341796875, 17262.052001953125, 16789.411865234375, 18258.923095703125, 18456.532958984375, 17374.962890625, 16843.3623046875, 16332.28271484375, 16079.923828125, 15395.38330078125, 15733.78271484375, 15504.28271484375, 16554.462890625, 16580.0830078125, 17123.50341796875, 17362.5439453125, 17193.40283203125, 17699.98388671875, 17323.7841796875, 17420.923828125, 17870.98388671875, 17562.5830078125, 18082.18212890625, 18458.68212890625, 18449.6220703125, 18215.982421875, 18466.42236328125, 18375.8818359375, 18832.6025390625, 18670.1220703125, 19160.44189453125, 19232.6220703125, 19078.42236328125, 18958.30224609375, 18761.68212890625, 19053.3427734375, 19055.68212890625, 18695.26318359375, 18394.54248046875, 18589.54248046875, 18656.7421875, 19198.001953125, 19012.18212890625, 19304.203125, 19165.6025390625, 19207.2421875, 19425.16259765625, 19691.32177734375, 19652.92236328125, 19740.40283203125, 19808.92236328125, 19959.462890625, 20211.76318359375, 20148.64306640625, 20638.1220703125, 20868.8818359375, 20717.6220703125, 20615.3818359375, 19487.14306640625, 19722.40283203125, 19874.0830078125, 20222.982421875, 20155.482421875, 20166.5830078125, 20060.982421875, 20181.703125, 20262.28271484375, 19776.52294921875, 19977.1025390625, 19528.8427734375, 19793.982421875, 20076.28271484375, 20169.703125, 20254.6025390625, 20552.8623046875, 20346.462890625, 20494.18212890625, 20386.8427734375, 20584.78271484375, 20405.8623046875, 20659.66259765625, 20833.90283203125, 20767.962890625, 20822.92236328125, 20985.5830078125, 21018.3427734375, 21130.66259765625, 20888.501953125, 20768.32177734375, 20911.001953125, 20785.18212890625, 21025.18212890625, 20951.8623046875, 21101.26318359375, 21242.203125, 21313.6025390625, 21441.16259765625, 21569.501953125, 21582.22265625, 21637.3623046875, 21476.68212890625, 21756.64306640625, 21715.1220703125, 21711.64306640625, 21766.482421875, 21813.22265625, 21723.64306640625, 21787.6025390625, 21857.501953125, 22062.22265625, 22136.26318359375, 22346.92236328125, 22381.8427734375, 22522.6025390625, 22476.40283203125, 22634.44189453125, 22959.5830078125, 22204.90283203125, 22036.30224609375, 22607.021484375, 23009.74072265625, 23368.36083984375, 23379.041015625, 23123.62060546875, 23229.580078125, 23135.3203125, 22964.4404296875, 23189.6806640625, 23420.14013671875, 23661.710205078125, 24212.26123046875, 24279.952392578125, 24643.04150390625, 25015.0224609375, 24902.111572265625, 25094.82177734375, 25219.422119140625, 24992.76123046875, 25414.0224609375, 25080.611572265625, 25489.9013671875, 25681.63232421875, 25893.801025390625, 26293.361572265625, 26137.331298828125, 26300.1513671875, 26262.842529296875, 26259.552734375, 26657.78369140625, 26544.38232421875, 26491.4619140625, 26365.952392578125, 26449.251708984375, 25998.312255859375, 26070.342529296875, 25232.793212890625, 25506.353759765625, 25225.302734375, 24943.342529296875, 25355.781982421875, 25875.7421875, 26344.812255859375, 26337.7421875, 26625.16259765625, 26590.372802734375, 26780.281982421875, 26530.732666015625, 26867.7119140625, 27160.031982421875, 27038.372802734375, 26746.19287109375, 26844.75341796875, 26674.44287109375, 26814.793212890625, 27219.531982421875, 27179.2119140625, 27240.11328125, 27123.07177734375, 27408.812255859375, 27454.732666015625, 27438.702392578125, 27665.50341796875, 27615.38232421875, 27687.41259765625, 27481.403076171875, 27448.36328125, 27415.88232421875, 27304.092529296875, 27634.00341796875, 27679.85205078125, 27829.0224609375, 27737.531982421875, 27636.10205078125, 27582.482666015625, 27601.732666015625, 27693.0830078125, 27919.183349609375, 27860.94287109375, 27895.94287109375, 28064.153076171875, 27676.2119140625, 27859.683349609375, 28008.641845703125, 28398.19287109375, 28544.422119140625, 28368.933349609375, 28379.9921875, 28440.543212890625, 28340.44287109375, 28149.41259765625, 28364.031982421875, 28734.61328125, 28743.153076171875, 28661.952392578125, 28759.183349609375, 28719.00341796875, 29410.9541015625, 29667.22314453125, 29155.243896484375, 29572.584716796875, 29939.734375, 29966.75341796875, 30257.743896484375, 30363.37451171875, 30564.694580078125, 30534.173828125, 30524.72314453125, 30570.22314453125, 30699.37451171875, 30683.694580078125, 30674.87451171875, 30553.353759765625, 30604.173828125, 30815.6435546875, 30781.552734375, 30473.134033203125, 29800.505126953125, 29673.173828125, 30307.864990234375, 30528.5751953125, 30174.584716796875, 29815.834716796875, 30330.12451171875, 30474.25341796875, 30852.88232421875, 31016.47314453125, 31300.183349609375, 31328.183349609375, 31507.38232421875, 31463.7724609375, 31543.6435546875, 31133.0224609375, 31116.50341796875, 31308.933349609375, 31098.44287109375, 30948.781982421875, 31091.44287109375, 31546.5830078125, 31522.433349609375, 31434.653076171875, 31535.031982421875, 31863.8935546875, 32270.172119140625, 32242.38232421875, 32284.452392578125, 32404.9921875, 32626.4013671875, 32620.734375, 32715.931640625, 32597.423828125, 32917.7421875, 33023.09423828125, 32867.62109375, 33065.85986328125, 33335.2197265625, 33066.14013671875, 32749.810546875, 32801.9619140625, 32795.66259765625, 32770.88232421875, 32572.85205078125, 32360.75, 32441.181640625, 32245.181640625, 32224.673828125, 32462.8857421875, 32679.74560546875, 32370.55615234375, 32624.86669921875, 32001.4462890625, 32347.666015625, 32777.11669921875, 32851.037109375, 32602.8173828125, 32517.76806640625, 32821.84765625, 32799.02587890625, 33087.35546875, 33024.916015625, 33079.9384765625, 33114.166015625, 33136.77587890625, 33122.287109375, 33164.84765625, 33057.95751953125, 33317.23779296875, 33293.64697265625, 33298.8251953125, 33244.85546875, 33382.26806640625, 33440.08642578125, 33494.05615234375], \"yaxis\": \"y2\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Backtesting of the trading strategy\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.94], \"title\": {\"text\": \"Time\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"S&P500 closing price ($)\"}}, \"yaxis2\": {\"anchor\": \"x\", \"overlaying\": \"y\", \"side\": \"right\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b4d0b22c-ba74-4ff2-9e89-93f47b9f6454');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}